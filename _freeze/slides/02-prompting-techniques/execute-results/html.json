{
  "hash": "cfbc004f8a4423d6543b4b4198eb80ef",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Basic Prompting Techniques\"\nauthor: \"Andrew Ellis\"\ndate: last-modified\ndate-format: \"DD MMMM, YYYY\"\nbibliography: ../bibliography.bib\nnocite: |\n  @broschinskiGrafikenErklaertFunktioniert2023\nformat: \n    revealjs:\n        # theme: moon\n        theme: default\n        title-slide-attributes:\n          # data-background-image: ../assets/background-purple.png\n          # data-background-size: contain\n          data-background-opacity: \"1\"\n        # logo: ../assets/robot.png\n        footer: <a href=\"../index.html\">back to website ‚§¥Ô∏è</a>\n        navigation-mode: linear\n        progress: true\n        scrollable: false\n        slide-number: true\n        show-slide-number: all\n        controls-layout: bottom-right\n        controls-tutorial: true\n        preview-links: auto\n        chalkboard: true\n        from: markdown+emoji\n        code-fold: true\n        code-summary: \"Show code\"\n        code-tools: true\n        menu: \n          sticky: true\n          keyboard: true\n          autoOpen: true\n          width: normal\n          numbers: true\n          markers: true\n\n# slide-level: 3\n# number-sections: true\n---\n\n::: {.cell hash='02-prompting-techniques_cache/revealjs/unnamed-chunk-1_15b0b66503da51d364b491de6b0c4021'}\n\n:::\n\n\n\n\n\n\n\n<!-- # Basic Prompting Techniques {background-color=\"#b48ead\"}\n\n::: {.absolute top=\"0\" left=\"100%\"}\n::: {.sectionhead}\n\n:::\n::: -->\n\n## Unlocking knowledge?\n\n- LLMs (with enough parameters) show **emergent** behaviour; they learn to do things they were not explicitly trained to do. \n\n- They can e.g. solve logic puzzles and perform complex multistep reasoning. \n\n- Often, these capabilities, which are encoded in the models parameters, need to be \"unlocked\" by the right prompt.\n\n\n\n## OpenAI Best Practices\nOpenAI give a set of strategies for using their models [üëâ Six strategies](https://platform.openai.com/docs/guides/gpt-best-practices/six-strategies-for-getting-better-results) . \n\nThese include:\n\n- **writing clear instructions**\n- **providing reference texts**\n- **splitting tasks into subtasks**\n- **giving GPT 'time to think'**\n- **using external tools** (This is provided by e.g. plugins.)\n\n## Writing clear instructions\n\nGiven that might conceive of an LLM as a role-playing simulator of conversations, it is intuitively clear that instructions should be clear and unambiguous, and should indicate which role the model should adopt.\n\n- [Include details in your query to get more relevant answers](https://platform.openai.com/docs/guides/gpt-best-practices/tactic-include-details-in-your-query-to-get-more-relevant-answers)\n- [Ask the model to adopt a persona](https://platform.openai.com/docs/guides/gpt-best-practices/tactic-ask-the-model-to-adopt-a-persona)\n- [Use delimiters to clearly indicate distinct parts of the input](https://platform.openai.com/docs/guides/gpt-best-practices/tactic-ask-the-model-to-adopt-a-persona)\n- [Specify the steps required to complete a task](https://platform.openai.com/docs/guides/gpt-best-practices/tactic-specify-the-steps-required-to-complete-a-task)\n- [Provide examples](https://platform.openai.com/docs/guides/gpt-best-practices/tactic-provide-examples)\n- [Specify the desired length of the output](https://platform.openai.com/docs/guides/gpt-best-practices/tactic-specify-the-desired-length-of-the-output)\n\n\n## Providing reference texts\n\n- Provide a model with trusted and relevant information. \n- Then instruct the model to use the provided information to compose its answer.\n\nThis leads to an incredibly powerful technique, which is known as **retrieval-augmented generation**. By this, we mean that we first create a database of documents, and then retrieve the most relevant documents, based on a user's query. These are then included in the prompt to the model. The model is instructed to use the information in the documents to compose its answer.\n\n\n<!-- ## Splitting tasks into subtasks\nFOr complex queries, first split the task into subtasks, and then ask the model to complete each subtask in turn. -->\n\n## Giving GPT 'time to think'\n\n- LLMs generate text auto-regressively, and the model spends the same amount of computation on each token.\n- Rather that getting the answer straight away, it makes sense to give the model more context, and to give it more steps to \"think\". \n- By doing so, you are increasing the chances that the model will give a good answer.\n- This technique is known as **chain-of-thought** prompting, and can often be induced by simply instructing the model to `think step-by-step`.\n\n<!-- ![Chain-of-thought example](../assets/images/chain-of-thought.png) -->\n\n\n\n## Explore prompting techniques\nNow open the second activity to learn more about ChatGPT and OpenAI Playground: [üëâ Activity 2](../pages/activity-2-prompting-techniques.html).\n\n\n\n\n\n\n# References {background-color=\"#2e3440\"}\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}