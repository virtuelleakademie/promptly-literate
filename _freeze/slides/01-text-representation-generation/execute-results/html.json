{
  "hash": "9b3041cd2d70fe528c50a1bb2c3420ea",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"LLMs: Text representation, training, and text generation\"\nauthor: \"Andrew Ellis\"\ndate: last-modified\ndate-format: \"DD MMMM, YYYY\"\nbibliography: ../bibliography.bib\nnocite: |\n  @broschinskiGrafikenErklaertFunktioniert2023\nformat: \n    revealjs:\n        theme: simple                                                                                    \n        # theme: default\n        title-slide-attributes:\n          data-background-image: ../assets/background-purple.png\n          # data-background-size: contain\n          data-background-opacity: \"1\"\n        # logo: ../assets/robot.png\n        footer: <a href=\"../index.html\">back to website ‚§¥Ô∏è</a>\n        navigation-mode: vertical\n        progress: true\n        scrollable: true\n        slide-number: true\n        show-slide-number: all\n        controls-layout: bottom-right\n        controls-tutorial: true\n        preview-links: auto\n        chalkboard: true\n        from: markdown+emoji\n        code-fold: true\n        code-summary: \"Show code\"\n        code-tools: true\n        menu: \n          sticky: true\n          keyboard: true\n          autoOpen: true\n          width: normal\n          numbers: true\n          markers: true\n\n# slide-level: 3\n# number-sections: true\n---\n\n::: {.cell hash='01-text-representation-generation_cache/revealjs/unnamed-chunk-1_a714204590ec2c9bf0ad9b8cac674af3'}\n\n:::\n\n\n\n# Contents {background-color=\"#b48ead\"}\n\n1. What is natural language processing?\n2. How do LLMs represent text?\n3. What is ChatGPT?\n4. How was ChatGPT trained??\n5. How should we think about LLMs?\n6. ChatGPT and OpenAI Playground?\n\n<!-- ::: footer\n<a href=\"https://virtuelleakademie.github.io/gpt-nano\">üè† KI/GPT in der Hochschule</a>\n::: -->\n\n\n\n# What is natural language processing (NLP)? {background-color=\"#b48ead\"}\n\n::: {.absolute top=\"0\" left=\"100%\"}\n::: {.sectionhead}\n1 [2 3 4 5 6]{style=\"opacity:0.25\"}\n:::\n:::\n\n## What is NLP?\n\n- NLP is a subfield of [artificial intelligence (AI)]((https://www.derbund.ch/so-funktioniert-kuenstliche-intelligenz-599276436215)).\n- NLP is concerned with the interactions between computers and human (natural) languages.\n  \n### Brief timeline\n\n- 1950: Alan Turing proposed the Turing test to assess machine intelligence through conversation.\n- 1954: IBM introduced the first machine translation system, translating Russian to English using rules.\n- 1960s-1970s: Rule-based systems like SHRDLU and ELIZA used human-crafted rules for language interaction.\n- 1980s-1990s: Statistical methods employed probabilities and text data, using models like Hidden Markov Models and n-grams.\n- 2000s-present: NLP shifted to neural network methods with deep learning, employing recurrent neural networks for complex language tasks.\n- Transformers (first published in 2017) are the current state-of-the-art for NLP, and are the basis for large language models (LLMs), such as GPT-3.5, GPT-4, ChatGPT, Llama 2, and others.\n- LLMs are models with billions of parameters, trained on massive amounts of text data. Training consists of predicting the next word in a sequence of words. \n\n\n\n## Long-range dependencies\n\n:::{.callout-note}\n## Complicated sentence\n\"The boy who was throwing stones at the birds, despite being warned by his parents not to harm any creatures, was chased by the angry flock.\"\n:::\n\nWho was chased?\n\n- This type of long-range dependency is difficult for traditional NLP methods to handle.\n- The verb phrase (`the boy was chased`) is separated from the subject by a long distance - you can't just look at the previous few words to answer the question.\n- Transformers have a special feature that lets them easily connect words that are far apart in a sentence; `was chased` is linked directly to `The boy` without distraction by the words in between.\n\n\n\n## Key areas in NLP\n\n:::{.callout-note appearance=\"minimal\"}                                  \n`Sentiment Analysis`:             Identifying emotions and opinions in text.    \n`Machine Translation`:            Automatically translating between languages.  \n`Question Answering`:             Providing direct answers to user questions.   \n`Text Summarization`:             Generating concise summaries from long text.  \n`Speech Recognition`:             Converting spoken words to text.              \n`Speech Synthesis`:               Creating spoken words from text.              \n`Natural Language Generation`:    Generating human-like text.               \n`Natural Language Understanding`: Extracting meaning from text.          \n`Dialogue Systems`:               Conversing with humans using natural language.\n:::\n\n- Before LLMs, specialized models were trained for each task.\n- LLMs are general-purpose models that can perform a wide variety of tasks.\n  \n## Example: sentiment analysis (text classification)\nThe task of classifying text as positive, negative, or neutral.\n\n- `I love this movie!` ‚Üí positive üòä\n- `This movie is ok.` ‚Üí neutral üòê \n- `This movie is terrible!` ‚Üí negative üò†\n\n\n\n\n \n\n\n## Machine Learning primer\n\n- Earlier, rule-based systems had to be programmed.\n- Machine learning (ML) models learn implicitly, i.e. without rules being programmed in.\n\n\n- Important terms:\n  - __training data:__ Models are fed with data, and parameters of the model are adjusted so that the model is as \"good\" as possible.\n  - __supervised learning:__ Categories known, e.g. classify images of animals.\n  - __unsupervised learning:__ Categories are unknown, e.g. discover unknown patterns.\n  - __reinforcement learning:__ The goal is given, and the model learns through feedback (reward) how the goal can be achieved.\n\n> We have to learn the bitter lesson that building in how we think we think does not work in the long run. We should stop trying to find simple ways to think about space, objects, multiple agents, or symmetries... instead we should build in only the meta-methods that can find and capture this arbitrary complexity. We want AI agents that can discover like we can, not which contain what we have discovered [@suttonBitterLesson2019].\n\n\n## Supervised learning\n\n\n::: {.cell hash='01-text-representation-generation_cache/revealjs/unnamed-chunk-2_8b0c1d7410ef4e6c48b89fd015cf6c7a'}\n::: {.cell-output-display}\n![](../assets/images/cats-dogs.png){width=496}\n:::\n:::\n\n\n__Classifiy pictures of cats and dogs:__ The goal of a model could be to discover which features distinguish cats from dogs.\n\n\n\n\n\n## Reinforcement learning\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell hash='01-text-representation-generation_cache/revealjs/unnamed-chunk-3_c9fe821b68b17cc6cce1faad75dbc500'}\n::: {.cell-output-display}\n![](../assets/images/cartpole.gif)\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell hash='01-text-representation-generation_cache/revealjs/unnamed-chunk-4_922757a29585b6ae3ed7c0255be38203'}\n::: {.cell-output-display}\n![](../assets/images/RL-agent.png){width=803}\n:::\n:::\n\n:::\n::::\n\n\n\n\n\n\n# What is ChatGPT? {background-color=\"#b48ead\"}\n\n::: {.absolute top=\"0\" left=\"100%\"}\n::: {.sectionhead}\n[1]{style=\"opacity:0.25\"} 2 [3 4 5 6]{style=\"opacity:0.25\"}\n:::\n:::\n\n\n\n\n\n\n## ChatGPT\n\nChatGPT is a particular kind of LLM and consists of two models:\n\n__Base model:__ GPT-3.5 oder GPT-4 (generative pre-trained transformer). This model is trained \"simply\" to predict the next word in a sequence of words. A base model produces text, but not human-like conversations.\n\n:::{.callout-note}\n## Example\nGive the input `Once upon a time there was a`, the model will predict which word is likely to follow.\n:::\n\n__Assistant model:__ This model is trained using reinforcement learning from human feedback to have human-like conversations.\n\n:::{.callout-note}\n## Example\nüë©‚Äçüíº: `Tell me a story!`\n\nüí¨: `Once upon a time there was a ....`\n:::\n\n\n## Text generation\n\n- LLMs produce text by predicting the next word, one word at a time: \n- This is known as \"auto-regressive next token prediction\" (we'll discover what tokens are in the next section).\n\n- The model predicts which token is likely to follow, given a sequence of tokens (words, punctuation, emojis, etc.). \n- Key idea: this simple procedure is followed over and over again, with <mark style=\"background: #FFF3A3A6;\">each new token being added to the sequence of tokens</mark> that the model uses to predict the next token.\n$$ P(w_{w+1} | w_1, w_2, ..., w_t) $$\n\n- The sequence of words is called the <mark style=\"background: #FFF3A3A6;\">context</mark>; the text generated by the model is dependent on the context.\n- The output of the model is a probability distribution over all possible tokens. The model then chooses one token from this distribution.\n\n\n## Text generation examples\n\n\n::: {.cell hash='01-text-representation-generation_cache/revealjs/unnamed-chunk-5_995ff785dbb436de607ac7a1b30e1b25'}\n::: {.cell-output-display}\n![](../assets/images/autoregressive.png){width=538}\n:::\n:::\n\n\n- The new context is used to generate the next token, etc. \n- <mark style=\"background: #FFF3A3A6;\">Every token is given an equal amount time (computation per token is constant)</mark>. The model has no concept of more or less important tokens. This is crucial for understanding how LLMs work.\n\n\n\n## Tokenization\nSo far we have been talking about words, but LLMs operate with tokens. These are sub-words, and make working with text much easier for the model. A rule of thumb is that one token generally corresponds to ~4 characters of English text. This translates to roughly $\\frac{3}{4}$ of a word (so 100 tokens is about 75 words).\n\n\n::: {.cell hash='01-text-representation-generation_cache/revealjs/unnamed-chunk-6_934e43dcd7fbab1315d5dd22c8825d37'}\n::: {.cell-output-display}\n![](../assets/images/tokenization.png){width=675}\n:::\n:::\n\n\nFeel free to try out the [OpenAI tokenizer](https://platform.openai.com/tokenizer). \n\n## Embeddings\n- The next step is to represent tokens as vectors. \n- This is called \"embedding\" the tokens. The vectors are high-dimensional, and the **distance** between vectors measures the similarity between tokens.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n\n::: {.cell hash='01-text-representation-generation_cache/revealjs/unnamed-chunk-7_773fdf1c5ace00a8975835bd5e5182ce'}\n::: {.cell-output-display}\n![](../assets/images/embedding.png){width=751}\n:::\n:::\n\n:::\n::: {.column width=\"50%\"}\n- In this 2-dimensional representation, concepts that are \"related\" lie close together.\n\n:::\n\n::::\n\nYou can read more about embeddings [in this tutorial](../pages/text-representation.qmd).\n\n\n\n# How was ChatGPT trained? {background-color=\"#b48ead\"}\n\n::: {.absolute top=\"0\" left=\"100%\"}\n::: {.sectionhead}\n[1 2]{style=\"opacity:0.25\"} 3 [4 5 6]{style=\"opacity:0.25\"}\n:::\n:::\n\n## Summary\nModern LLMs, such as ChatGPT, are trained in 3 steps:\n\n1) Pre-training: the model absorbs knowledge from text datasets.\n2) Supervised finetuning: model is refined to better adhere to specific instructions.\n3) Alignment: hones the LLM to respond more helpfully and safely to user prompts. This step is known as \"reinforcement learning from human feedback\" (RLHF).\n\n\n\n## Pre-training Data\n\n\n::: {.cell hash='01-text-representation-generation_cache/revealjs/unnamed-chunk-8_285dd13cc37421d58da5a76b4f8483ff'}\n::: {.cell-output-display}\n![](../assets/images/karpathy-training-data.png){width=723}\n:::\n:::\n\n\n\n## Pre-training\n\n\n::: {.cell hash='01-text-representation-generation_cache/revealjs/unnamed-chunk-9_9424f3590b9133dcb7b98b981e7a2ba8'}\n::: {.cell-output-display}\n![](../assets/images/pretraining.png){width=576}\n:::\n\n::: {.cell-output-display}\n![](../assets/images/karpathy-training-process.png){width=723}\n:::\n:::\n\n\n## Supervised fine-tuning\n\n::: {.cell hash='01-text-representation-generation_cache/revealjs/unnamed-chunk-10_2792bdc8d2e4f47ae999c1399a092d9d'}\n::: {.cell-output-display}\n![](../assets/images/finetuning.png){width=501}\n:::\n:::\n\n\n## Reinforcement learning from human feedback (RLHF)\n\nUses human feedback to rank the model's responses. The goal is for the model to learn human preferences for responses.\n\n\n::: {.cell hash='01-text-representation-generation_cache/revealjs/unnamed-chunk-11_5bb4cc523e09a4441368c22d04d44bfb'}\n\n:::\n::: {.cell hash='01-text-representation-generation_cache/revealjs/unnamed-chunk-12_7b8005054a7922d39fc640cedcf02c2d'}\n::: {.cell-output-display}\n![Source: [openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)](../assets/images/RLHF.png){width=2053}\n:::\n:::\n\n\n \n\n# How should we think about LLMs? {background-color=\"#b48ead\"}\n\n::: {.absolute top=\"0\" left=\"100%\"}\n::: {.sectionhead}\n[1 2 3 4]{style=\"opacity:0.25\"} 5 [6]{style=\"opacity:0.25\"}\n:::\n:::\n\n\n## Useful analogy: Role-playing simulator\n\n> We can think of an LLM as a non-deterministic simulator capable of role-playing an infinity of characters, or, to put it another way, capable of stochastically generating an infinity of simulacra. \n\nSource: @shanahanRolePlayLargeLanguage2023\n\n- A large language model (LLM) trained as an assistant is a simulator of possible human conversation.\n- An assistant model does not have any intentions. It is not an entity with its own goals. It is merely trained to respond to user prompts in a human-like way.\n- An assistant model does not have a \"personality\" or \"character\" in the traditional sense. It is a simulation of a conversation, and can be thought of as a role-playing simulator.\n- There is no concept of \"truth\" or \"lying\" in a role-playing simulator. The model is not trying to deceive the user, it is simply trying to respond in a human-like way.\n\nThis is very important when we try to understand why LLMs hallucinate, i.e. generate text that is not factually true.\n\n\n## Role-Playing Simulator\n\n![](../assets/images/simulator.png){width=800}\n\n\n\n# ChatGPT and OpenAI Playground {background-color=\"#b48ead\"}\n\n::: {.absolute top=\"0\" left=\"100%\"}\n::: {.sectionhead}\n[1 2 3 4 5]{style=\"opacity:0.25\"} 6\n:::\n:::\n\n\n\n## ChatGPT vs OpenAI Playground\n \nOpenAI offer two ways to interact with their assistant model:\n\n1) ChatGPT: A web interface where you can chat with the model.\n2) OpenAI Playground: A web interface that gives users more control over the model.\n\nNow open the first activity to learn more about ChatGPT and OpenAI Playground: [üëâ Activity 1](../pages/activity-1-explore-llms.html).\n\n\n\n# References {background-color=\"#2e3440\"}\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}