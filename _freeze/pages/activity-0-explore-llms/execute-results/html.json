{
  "hash": "82a8c069d5def06c3c3a632810a4f44b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Optional Activity: Exploring LLMs'\ndescription: |\n  Programming a GPT model.\ndate: last-modified\ndate-format: 'DD MMM, YYYY'\nauthor:\n  - name: Andrew Ellis\n    url: 'https://github.com/awellis'\n    affiliation: 'Virtuelle Akademie, Berner Fachhochschule'\n    affiliation-url: 'https://virtuelleakademie.ch'\n    orcid: 0000-0002-2788-936X\nlicense: CC BY\ncitation: true\nbibliography: ../bibliography.bib\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-link: true\nexecute:\n  cache: false\n  keep-ipynb: true\ncode-annotations: select\n---\n\n## Tasks\n\nUse both ChatGPT and the Playground to perform the following tasks:\n[Feel free to use the other models (Bing, Bard, Llama2, GooseAI) as well.]{.aside}\n\n:::{.callout-tip}\n## Prompts\n\n1) Generate fiction: `Tell me a short story about a monk and a tortoise going on a road trip.`\n2)\tLet the models write a poem. Give it a topic and a style (e.g. `a haiku about an exciting day at the office`).\n3) Let the models explain a concept from your field of study in a short text passage.\n4) Use the models to do some maths (e.g. `What is 89322/1313?`).\n5) Use the models to solve some common sense reasoning tasks. For example, `We have a book, 9 eggs (without the egg carton), a laptop, a bottle, and a nail. Please tell me how I can stack them on top of each other in a stable way.`\n:::\n\nIn all of these examples, use the **temperature** parameter in the playground to control the randomness of the model's output. Try different settings, and see how the output changes.\n\n\n## Models\n- [ChatGPT](https://chat.openai.com/)\n- [OpenAI Playground](https://platform.openai.com/playground)\n- [Bing Chat](https://www.bing.com/)\n- [Google Bard](https://bard.google.com/)\n- [Llama2](https://www.llama2.ai/)\n- [Goose AI](https://goose.ai/playground)\n\nNow we will explore two different interfaces to the same underlying OpenAI language models. These are **GPT-3.5-turbo** and **GPT-4**. The first is a smaller model (fewer parameters), whereas the second is the most advanced model (more parameters). \n\n[GPT-4 is only accessible to paid customers.]{.aside}\n\nBoth of these models are trained on the same data, but the second is larger and more powerful. Both are optimized for conversations, and are capable of a wide variety of tasks. However, GPT-4 generally performs better, especially at tasks requiring more complex reasoning, and at following instructions. The differences between models are described in this [article](https://openai.com/research/gpt-4).\n\nOne of the most important differences is the context length that the models can handle. GPT-4 can process much more context than GPT-3.5-turbo. \n\n\n- GPT-3.5-turbo can process a context of 4097 tokens (~3073 words) or 16'000 tokens (~12'000 words).\n\n- GPT-4 comes in two varieties: 8192 tokens (~6144 words ) or 32'768 tokens (~25'000 words).\n\n[However: ChatGPT only allows shorter context lengths; to get the full context length, you have to use the API (or playground).]{.aside}\n\n\nGeneral capabilities of the models include:\n\n- LLMs are *few-shot* learners, meaning they can learn from a small number of examples.\n- LLMs are *zero-shot* learners, meaning they can perform tasks without any examples, given appropriate instructions.\n- reasoning\n- writing code in common programming languages\n- translating between languages\n- basic mathematical abilities\n\n@bubeckSparksArtificialGeneral2023 give a fascinating summary of tasks that GPT-4 is claimed to be capable of.\n\n\nBoth models function in the same basic way (in a conversation): the entire previous conversation is fed into the model as context (prompt), and the model generates a response (token by token). \n\nIf you feel that the conversation has taken a wrong turn, you can **edit** your message, and the conversation will be re-generated from that point.\n\n\n\n\n### ChatGPT\n\nThis is a simple interface to the GPT-3.5-turbo and GPT-4 models. It does not offer any possibility of adjusting the parameters of the model, but it does allow you to enter a prompt, and then to interact with the model.\n\nNotable features:\n- In the paid version, you can choose between GPT-3.5-turbo and GPT-4.\n- GPT-4 offers plugins. These can give the assistant access to a wide variety of sources of information, including databases, APIs, and web scraping. A very useful plugin is the *Wolfram Alpha* plugin, which allows the assistant to compute answers based on facts and mathematical knowledge.\n- GPT-4 and **Advanced Data Analysis** plugin; this gives the model the ability to run python code and display the results.\n\n\n\n### Playground\n\nThis is a more advanced interface to the GPT-3.5-turbo and GPT-4 models. It allows you to adjust the parameters of the model, and to enter different types of prompts, and then to interact with the model. It also allows you to use the full context lengths (8k, 16k or 32k tokens), meaning that you can process much longer texts.\n\nFurthermore, it allows you to save your prompts as **presets** to reuse them or to share them with others.\n\n#### Parameters\nThe playground offers the following parameters:\n\n- Mode: Currently only `Chat`\n- Model: GPT-3.5-turbo or GPT-4 with varying context lengths.\n- <mark style=\"background: #FFF3A3A6;\">Temperature: This is the most interesting parameter - it controls the level of _randomness_. A setting of 0 means that the model will sample text deterministically (it will always choose the most probable next token), higher settings make the model's output increasingly more random.</mark>\n- Maximum length: controls the length of the output text.\n- Stop sequences: characters telling the model to stop generating text.\n- Top P: tells the model to consider only subset of most probable tokens when generating. Use **temperature** instead.\n- Frequency penalty: penalizes the model based on number of times that token has appeared.\n- Presence penalty: penalizes the model for based on whether they have already appeared. Encourages diversity of tokens.\n\nIn general, the only parameters that you need to adjust are **temperature** and (possibly) **maximum length**. \n\n\nIf you want to read more about the temperature parameter, see the following article\n[ðŸ‘‰ Temperature](https://docs.cohere.com/docs/temperature).\n\n\n[[1](https://www.prompthub.us/blog/understanding-openai-parameters-how-to-optimize-your-prompts-for-better-outputs) and \n[2](https://shivammehta25.github.io/posts/temperature-in-language-models-open-ai-whisper-probabilistic-machine-learning/) are also interesting.]{.aside}\n\n\n\n\n#### System and user messages\nThe playground offers three types of messages: **system**, **user** and **assistant** messages. \n\nThese system and user messages are both fed into the model as context, but they are treated differently; the system message is not part of the conversation. The idea is that the system message is a prompt that is not visible to the user, i.e. it can be hidden when building a chatbot.\n\nThe user and assistant messages are displayed in the conversation. The assistant messages are generated by the model, and the user messages are entered by the user.\n\n\n\n\n:::{.callout-caution}\n## Discussion ðŸ’¬ \n\n- What do you think of the models' performance? What are their strengths and weaknesses? What are the limitations of the models? \n- If you are unhappy, how can you improve the model's performance?\n\n:::\n\n",
    "supporting": [
      "activity-0-explore-llms_files"
    ],
    "filters": [],
    "includes": {}
  }
}