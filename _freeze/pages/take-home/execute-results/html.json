{
  "hash": "418d703da9f6310f46765624e19f060e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Take-home messages\ndescription: |\n  Programming a GPT model.\ndate: last-modified\ndate-format: 'DD MMM, YYYY'\nauthor:\n  - name: Andrew Ellis\n    url: 'https://github.com/awellis'\n    affiliation: 'Virtuelle Akademie, Berner Fachhochschule'\n    affiliation-url: 'https://virtuelleakademie.ch'\n    orcid: 0000-0002-2788-936X\nlicense: CC BY\ncitation: true\nbibliography: ../bibliography.bib\nformat:\n  html:\n    toc: true\n    code-fold: false\n    code-link: true\nexecute:\n  cache: false\n  keep-ipynb: true\ncode-annotations: select\nlightbox: auto\n---\n\n1) An LLM is not a knowledge base, instead it's a statistical model of a knowledge base. An LLM is trained to be a language model.\nAn LLM is a probabilistic model of its training corpus. It can be used to predict text auto-regressively.\n\n\n {{< fa chess-pawn >}}\n {{< fa thumbs-up >}} \n{{< fa calendar >}}\n\n![](../assets/robot.png)\n\n{{< youtube YDiSFS-yHwk >}}\n\n\n\n## Embed a presentation\n```{=html}\n<iframe width=\"800\" height=\"600\" src=\"../slides/demo.html\" title=\"Presentation example\"></iframe>\n```\n\n## Embed Miro board\n\n```{=html}\n<iframe width=\"768\" height=\"432\" src=\"https://miro.com/app/live-embed/uXjVNW_AhHc=/?moveToViewport=-777,-200,3208,2084&embedId=260483093652\" frameborder=\"0\" scrolling=\"no\" allow=\"fullscreen; clipboard-read; clipboard-write\" allowfullscreen></iframe>\n```\n\n",
    "supporting": [
      "take-home_files"
    ],
    "filters": [],
    "includes": {}
  }
}