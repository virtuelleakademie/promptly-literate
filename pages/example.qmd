---
title: "Take-home messages"
description: |
  Programming a GPT model.
date: last-modified
date-format: "DD MMM, YYYY"
author: 
  - name: Andrew Ellis
    url: https://github.com/awellis
    affiliation: Virtuelle Akademie, Berner Fachhochschule
    affiliation-url: https://virtuelleakademie.ch
    orcid: 0000-0002-2788-936X
license: CC BY
citation: true
bibliography: ../bibliography.bib
format:
    html:
        toc: true
        code-fold: false
        code-link: true
jupyter: python3
execute: 
  cache: false
  keep-ipynb: true
code-annotations: select

# filters:
#    - lightbox
lightbox: auto
---

1) An LLM is not a knowledge base, instead it's a statistical model of a knowledge base. An LLM is trained to be a language model.
An LLM is a probabilistic model of its training corpus. It can be used to predict text auto-regressively.


 {{< fa chess-pawn >}}
 {{< fa thumbs-up >}} 
{{< fa calendar >}}

![](../assets/robot.png)

{{< youtube YDiSFS-yHwk >}}



## Embed a presentation
```{=html}
<iframe width="800" height="600" src="../slides/demo.html" title="Presentation example"></iframe>
```

## Embed Miro board

```{=html}
<iframe width="768" height="432" src="https://miro.com/app/live-embed/uXjVNW_AhHc=/?moveToViewport=-777,-200,3208,2084&embedId=260483093652" frameborder="0" scrolling="no" allow="fullscreen; clipboard-read; clipboard-write" allowfullscreen></iframe>
```