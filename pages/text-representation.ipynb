{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Text representation\n",
        "description: |\n",
        "  Programming a GPT model.\n",
        "date: last-modified\n",
        "date-format: 'DD MMM, YYYY'\n",
        "author:\n",
        "  - name: Andrew Ellis\n",
        "    url: 'https://github.com/awellis'\n",
        "    affiliation: 'Virtuelle Akademie, Berner Fachhochschule'\n",
        "    affiliation-url: 'https://virtuelleakademie.ch'\n",
        "    orcid: 0000-0002-2788-936X\n",
        "license: CC BY\n",
        "citation: true\n",
        "bibliography: ../bibliography.bib\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    code-fold: true\n",
        "    code-link: true\n",
        "execute:\n",
        "  cache: false\n",
        "  keep-ipynb: true\n",
        "code-annotations: select\n",
        "---"
      ],
      "id": "82ce4f17"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embeddings\n",
        "\n",
        "Text embeddings are numerical representations of words, sentences or documents. They are used in many NLP tasks, such as sentiment analysis, machine translation, and question answering.\n",
        "\n",
        "- Embeddings should capture features of words or concepts, and relationships between these.\n",
        "- A sentence embedding is just like a word embedding, except it associates every sentence with a vector full of numbers, capturing similarities between sentences. \n",
        "- The key idea is that words/sentences with similar meanings will have similar vectors. This is useful for many tasks in natural language processing, such as sentiment analysis, machine translation, and question answering.\n",
        "\n",
        "You can read more about text embeddings in this [ðŸ‘‰ post](https://docs.cohere.com/docs/text-embeddings).\n",
        "\n",
        "The following is an example of sentence embeddings, showing the distance between sentences. The distance is calculated using the dot product (cosine similarity) of the embeddings.\n",
        "\n",
        "<!-- #TODO: use Hugging Face sentence transformers-->\n",
        "<!-- #TODO: use UMAP -->\n"
      ],
      "id": "b94a426b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "# embedding = OpenAIEmbeddings()\n",
        "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pyobsplot import Plot, d3, Math, js"
      ],
      "id": "3427d009",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "sentences = [\n",
        "    \"Good morning, how are you?\",\n",
        "    \"I am doing well, how about you?\",\n",
        "    \"Hi, how are you doing today?\",\n",
        "    \"Hey, what's up?\",\n",
        "    \"I like apples.\",\n",
        "    \"One of my daughters doesn't like kiwis.\",\n",
        "    \"The other doesn't like bananas.\",\n",
        "    \"The earth is the third planet from the sun.\",\n",
        "    \"The moon is a natural satellite of the earth.\",\n",
        "    \"Jupiter is the fifth planet from the Sun and the largest in the Solar System.\",\n",
        "    \"The humpback whale is renowned for its enchanting songs, which are believed to serve various purposes, including communication, mating, and navigation during migration.\",\n",
        "    \"Dolphins, highly intelligent marine mammals, communicate with each other using a complex system of clicks, whistles, and body language, enabling them to work together in hunting and navigation.\",\n",
        "    \"The honeybee, through its pollination efforts, plays a vital role in agriculture, contributing to the growth of many of the fruits and vegetables humans rely on for sustenance.\",\n",
        "    \"The Large Plane Trees, also known as Road Menders at Saint-RÃ©my, is an oil-on-canvas painting by Vincent van Gogh.\",\n",
        "    \"Pablo Picasso's Guernica, an iconic mural-sized oil painting, stands as a poignant representation of the horrors of war.\",\n",
        "    \"This powerful artwork was created in response to the bombing of the town of Guernica during the Spanish Civil War.\"\n",
        "]"
      ],
      "id": "7ec82864",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "embeddings = np.array([embedding.embed_query(sentence) for sentence in sentences])\n",
        "\n",
        "dot_product_matrix = np.dot(embeddings, embeddings.T)\n",
        "\n",
        "df = pd.DataFrame(dot_product_matrix, columns=range(1, len(embeddings)+1))\n",
        "\n",
        "df['embedding_index'] = range(1, len(embeddings)+1)\n",
        "\n",
        "df = df.melt(id_vars=['embedding_index'], var_name='embedding_index_2', value_name='similarity')"
      ],
      "id": "6c86d951",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "pd.set_option(\"display.max_colwidth\", 600)\n",
        "pd.set_option(\"display.max_rows\", None)  # To display all rows\n",
        "pd.set_option(\"display.width\", 1000)  # Adjust the width as needed\n",
        "\n",
        "sentences_df = pd.DataFrame(sentences, columns=['Sentences'])\n",
        "sentences_df = sentences_df.reset_index(drop=True)\n",
        "sentences_df.index += 1\n",
        "\n",
        "display(sentences_df)"
      ],
      "id": "67e15e25",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false\n",
        "Plot.plot(\n",
        "    {\n",
        "        \"height\": 640,\n",
        "        \"padding\": 0.05,\n",
        "        \"grid\": True,\n",
        "        \"x\": {\"axis\": \"top\", \"label\": \"Embedding Index\"},\n",
        "        \"y\": {\"label\": \"Embedding Index\"},\n",
        "        \"color\": {\"type\": \"linear\", \"scheme\": \"PiYG\"},\n",
        "        \"marks\": [\n",
        "            Plot.cell(\n",
        "                df,\n",
        "                {\"x\": \"embedding_index\", \"y\": \"embedding_index_2\", \"fill\": \"similarity\", \"tip\": True},\n",
        "            ),\n",
        "            Plot.text(\n",
        "                df,\n",
        "                {\n",
        "                    \"x\": \"embedding_index\",\n",
        "                    \"y\": \"embedding_index_2\",\n",
        "                    \"text\": js(\"d => d.similarity.toFixed(2)\"),\n",
        "                    \"title\": \"title\"\n",
        "                },\n",
        "            ),\n",
        "        ],\n",
        "    }\n",
        ")"
      ],
      "id": "828a4540",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that the sentences are grouped together by semantic similarity. For example, the sentences about fruit are grouped together, and the sentences about planets are grouped together, in the sense that they are similar to each other.\n",
        "\n",
        "\n",
        "This is important: embeddings capture the meaning of words and sentences, and are the basis of LLMs' ability to \"understand\" language. \n",
        "\n",
        "Here is an alternative similarity plot, showing the general topic of each sentence. \n"
      ],
      "id": "3eaa1359"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "topics = {\"Greeting\": 4, \"Fruit\": 3,\n",
        "          \"Planets\": 3, \"Animals\": 3,\n",
        "          \"Art\": 3}\n",
        "topics_repeated = [key for key, value in topics.items() for i in range(value)]\n",
        "\n",
        "\n",
        "# Create the similarity matrix\n",
        "embeddings = np.array([embedding.embed_query(sentence) for sentence in sentences])\n",
        "dot_product_matrix = np.dot(embeddings, embeddings.T)\n",
        "\n",
        "# Create the heatmap\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(dot_product_matrix)\n",
        "\n",
        "# Set the x and y axis labels\n",
        "ax.set_xticks(np.arange(len(sentences)))\n",
        "ax.set_yticks(np.arange(len(sentences)))\n",
        "ax.set_xticklabels(topics_repeated)\n",
        "ax.set_yticklabels(topics_repeated)\n",
        "\n",
        "# Rotate the x axis labels\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# Add a colorbar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "7b201dec",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}