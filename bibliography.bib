@online{190407272Introduction2021,
  title = {[1904.07272] {{Introduction}} to {{Multi-Armed Bandits}}},
  date = {2021-01-31},
  url = {https://web.archive.org/web/20210131110323/https://arxiv.org/abs/1904.07272},
  urldate = {2023-04-21},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/W6I62VZC/1904.html}
}

@online{adamsBayesianOnlineChangepoint2007,
  title = {Bayesian {{Online Changepoint Detection}}},
  author = {Adams, Ryan Prescott and MacKay, David J. C.},
  date = {2007-10-19},
  eprint = {0710.3742},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.0710.3742},
  url = {http://arxiv.org/abs/0710.3742},
  urldate = {2023-03-17},
  abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
  pubstate = {preprint},
  keywords = {/unread,Statistics - Machine Learning},
  file = {/Users/andrew/Zotero/storage/B9844L4J/Adams_MacKay_2007_Bayesian Online Changepoint Detection.pdf;/Users/andrew/Zotero/storage/36XZ5FQB/0710.html}
}

@online{adamsSparseDenseGPT42023,
  title = {From {{Sparse}} to {{Dense}}: {{GPT-4 Summarization}} with {{Chain}} of {{Density Prompting}}},
  shorttitle = {From {{Sparse}} to {{Dense}}},
  author = {Adams, Griffin and Fabbri, Alexander and Ladhak, Faisal and Lehman, Eric and Elhadad, Noémie},
  date = {2023-09-08},
  eprint = {2309.04269},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.04269},
  urldate = {2023-09-17},
  abstract = {Selecting the ``right'' amount of information to include in a summary is a difficult task. A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a ``Chain of Density'' (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries generated by CoD are more abstractive, exhibit more fusion, and have less of a lead bias than GPT-4 summaries generated by a vanilla prompt. We conduct a human preference study on 100 CNN DailyMail articles and find that that humans prefer GPT-4 summaries that are more dense than those generated by a vanilla prompt and almost as dense as human written summaries. Qualitative analysis supports the notion that there exists a tradeoff between informativeness and readability. 500 annotated CoD summaries, as well as an extra 5,000 unannotated summaries, are freely available on HuggingFace (https://huggingface.co/datasets/griffin/chain\_of\_density).},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/K7HZ73IP/Adams et al_2023_From Sparse to Dense.pdf;/Users/andrew/Zotero/storage/H8GLKSD9/2309.html}
}

@online{agarwalTransformersReinforcementLearning2023,
  title = {Transformers in {{Reinforcement Learning}}: {{A Survey}}},
  shorttitle = {Transformers in {{Reinforcement Learning}}},
  author = {Agarwal, Pranav and Rahman, Aamer Abdul and St-Charles, Pierre-Luc and Prince, Simon J. D. and Kahou, Samira Ebrahimi},
  date = {2023-07-12},
  eprint = {2307.05979},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.05979},
  urldate = {2023-08-02},
  abstract = {Transformers have significantly impacted domains like natural language processing, computer vision, and robotics, where they improve performance compared to other neural networks. This survey explores how transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability. We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical RL algorithms. Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL. We examine the application of transformers to various aspects of RL, including representation learning, transition and reward function modeling, and policy optimization. We also discuss recent research that aims to enhance the interpretability and efficiency of transformers in RL, using visualization techniques and efficient training strategies. Often, the transformer architecture must be tailored to the specific needs of a given application. We present a broad overview of how transformers have been adapted for several applications, including robotics, medicine, language modeling, cloud computing, and combinatorial optimization. We conclude by discussing the limitations of using transformers in RL and assess their potential for catalyzing future breakthroughs in this field. CCS Concepts: • Computing methodologies → Reinforcement learning; Neural networks; • General and reference → Surveys and overviews.},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/W85GDSUV/Agarwal et al. - 2023 - Transformers in Reinforcement Learning A Survey.pdf}
}

@online{amatriainTransformerModelsIntroduction2023,
  title = {Transformer Models: An Introduction and Catalog},
  shorttitle = {Transformer Models},
  author = {Amatriain, Xavier},
  date = {2023-02-11},
  eprint = {2302.07730},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.07730},
  url = {http://arxiv.org/abs/2302.07730},
  urldate = {2023-02-16},
  abstract = {In the past few years we have seen the meteoric appearance of dozens of models of the Transformer family, all of which have funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovation in Transformer models.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/94ENBB72/Amatriain_2023_Transformer models.pdf;/Users/andrew/Zotero/storage/XCL2MGKM/2302.html}
}

@online{angelopoulosGentleIntroductionConformal2022,
  title = {A {{Gentle Introduction}} to {{Conformal Prediction}} and {{Distribution-Free Uncertainty Quantification}}},
  author = {Angelopoulos, Anastasios N. and Bates, Stephen},
  date = {2022-12-07},
  eprint = {2107.07511},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2107.07511},
  urldate = {2023-07-28},
  abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on.},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/andrew/Zotero/storage/N9EX8LDW/Angelopoulos and Bates - 2022 - A Gentle Introduction to Conformal Prediction and .pdf}
}

@online{arkoudasGPT4CanReason2023,
  title = {{{GPT-4 Can}}'t {{Reason}}},
  author = {Arkoudas, Konstantine},
  date = {2023-08-10},
  eprint = {2308.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.03762},
  url = {http://arxiv.org/abs/2308.03762},
  urldate = {2023-09-25},
  abstract = {GPT-4 was released in March 2023 to wide acclaim, marking a very substantial improvement across the board over GPT-3.5 (OpenAI's previously best model, which had powered the initial release of ChatGPT). However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason. This position paper discusses the nature of reasoning; criticizes the current formulation of reasoning problems in the NLP community, as well as the way in which LLM reasoning performance is currently evaluated; introduces a small collection of 21 diverse reasoning problems; and performs a detailed qualitative evaluation of GPT-4's performance on those problems. Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/3TSKKKLN/Arkoudas_2023_GPT-4 Can't Reason.pdf;/Users/andrew/Zotero/storage/SLBD9E4Z/2308.html}
}

@article{asadiRippleConceptBasedInterpretation,
  title = {Ripple: {{Concept-Based Interpretation}} for {{Raw Time Series Models}} in {{Education}}},
  author = {Asadi, Mohammad and Swamy, Vinitra and Frej, Jibril and Vignoud, Julien and Marras, Mirko and Kaser, Tanja},
  abstract = {Time series is the most prevalent form of input data for educational prediction tasks. The vast majority of research using time series data focuses on hand-crafted features, designed by experts for predictive performance and interpretability. However, extracting these features is labor-intensive for humans and computers. In this paper, we propose an approach that utilizes irregular multivariate time series modeling with graph neural networks to achieve comparable or better accuracy with raw time series clickstreams in comparison to handcrafted features. Furthermore, we extend concept activation vectors for interpretability in raw time series models. We analyze these advances in the education domain, addressing the task of early student performance prediction for downstream targeted interventions and instructional support. Our experimental analysis on 23 MOOCs with millions of combined interactions over six behavioral dimensions show that models designed with our approach can (i) beat state-of-the-art educational time series baselines with no feature extraction and (ii) provide interpretable insights for personalized interventions. Source code: https://github.com/epfl-ml4ed/ripple/.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/E4TQA4KF/Asadi et al. - Ripple Concept-Based Interpretation for Raw Time .pdf}
}

@article{badrinathPyBKTAccessiblePython,
  title = {{{pyBKT}}: {{An Accessible Python Library}} of {{Bayesian Knowledge Tracing Models}}},
  author = {Badrinath, Anirudhan and Wang, Frederic and Pardos, Zachary},
  abstract = {Bayesian Knowledge Tracing, a model used for cognitive mastery estimation, has been a hallmark of adaptive learning research and an integral component of deployed intelligent tutoring systems (ITS). In this paper, we provide a brief history of knowledge tracing model research and introduce pyBKT, an accessible and computationally efficient library of model extensions from the literature. The library provides data generation, fitting, prediction, and cross-validation routines, as well as a simple to use data helper interface to ingest typical tutor log dataset formats. We evaluate the runtime with various dataset sizes and compare to past implementations. Additionally, we conduct sanity checks of the model using experiments with simulated data to evaluate the accuracy of its EM parameter learning and use real-world data to validate its predictions, comparing pyBKT’s supported model variants with results from the papers in which they were originally introduced. The library is open source and open license for the purpose of making knowledge tracing more accessible to communities of research and practice and to facilitate progress in the field through easier replication of past approaches.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/EX6WHJBD/Badrinath et al. - pyBKT An Accessible Python Library of Bayesian Kn.pdf}
}

@online{bellerCounterfactualSimulationModel2023,
  title = {A Counterfactual Simulation Model of Causal Language},
  author = {Beller, Ari and Gerstenberg, Tobias},
  date = {2023-07-04T20:30:01},
  doi = {10.31234/osf.io/xv8hf},
  url = {https://psyarxiv.com/xv8hf/},
  urldate = {2023-07-05},
  abstract = {The words we use to describe what happened shape the story a listener imagines. How do speakers choose what causal expression to use? How does that impact what listeners infer about what happened? In this paper, we develop a computational model of how people use the causal expressions "caused", "enabled", "affected", and "made no difference". The model first builds a causal representation of what happened. By running counterfactual simulations, the model computes causal aspects that capture the different ways in which a candidate cause made a difference to the outcome. Logical combinations of these aspects define a semantics for the different causal expressions. The model then uses pragmatic inference favoring informative utterances to decide what word to use in context. We test our model in a series of experiments. In a set of psycholinguistic studies, we verify semantic and pragmatic assumptions of our model. We show that the causal expressions exist on a hierarchy of informativeness, and that participants draw informative pragmatic inferences in line with this scale. In the next two studies, we demonstrate that our model quantitatively fits participant behavior in a speaker task and a listener task involving dynamic physical scenarios. We compare our model to two lesioned alternatives, one which removes the pragmatic inference component, and another which additionally removes the semantics of the causal expressions. Our full model better accounts for participants' behavior than both alternatives, suggesting that causal knowledge, semantics, and pragmatics are all important for understanding how people produce and comprehend causal language.},
  langid = {american},
  pubstate = {preprint},
  keywords = {/unread,causality,Cognitive Psychology,Concepts and Categories,counterfactuals,intuitive physics,Judgment and Decision Making,Language,mental simulation,pragmatics,Reasoning,semantics,Social and Behavioral Sciences},
  file = {/Users/andrew/Zotero/storage/ZYMRB99E/Beller_Gerstenberg_2023_A counterfactual simulation model of causal language.pdf}
}

@online{bestaGraphThoughtsSolving2023,
  title = {Graph of {{Thoughts}}: {{Solving Elaborate Problems}} with {{Large Language Models}}},
  shorttitle = {Graph of {{Thoughts}}},
  author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
  date = {2023-08-21},
  eprint = {2308.09687},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.09687},
  url = {http://arxiv.org/abs/2308.09687},
  urldate = {2023-08-29},
  abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {$>$}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/8Y9XLWYG/Besta et al_2023_Graph of Thoughts.pdf;/Users/andrew/Zotero/storage/LLTA6BCG/2308.html}
}

@article{binzUsingCognitivePsychology2023,
  title = {Using Cognitive Psychology to Understand {{GPT-3}}},
  author = {Binz, Marcel and Schulz, Eric},
  date = {2023-02-07},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {6},
  pages = {e2218523120},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2218523120},
  url = {https://www.pnas.org/doi/10.1073/pnas.2218523120},
  urldate = {2023-02-11},
  abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3’s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3’s behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/2DE7KRJ8/Binz_Schulz_2023_Using cognitive psychology to understand GPT-3.pdf}
}

@article{binzUsingCognitivePsychology2023a,
  title = {Using Cognitive Psychology to Understand {{GPT-3}}},
  author = {Binz, Marcel and Schulz, Eric},
  date = {2023-02-07},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {6},
  pages = {e2218523120},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2218523120},
  url = {https://www.pnas.org/doi/10.1073/pnas.2218523120},
  urldate = {2023-03-25},
  abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3’s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3’s behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/J823HK9W/Binz_Schulz_2023_Using cognitive psychology to understand GPT-3.pdf}
}

@online{bommasaniOpportunitiesRisksFoundation2022,
  title = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and family=Arx, given=Sydney, prefix=von, useprefix=true and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  date = {2022-07-12},
  eprint = {2108.07258},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2108.07258},
  url = {http://arxiv.org/abs/2108.07258},
  urldate = {2023-11-01},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/ER6APDWA/Bommasani et al_2022_On the Opportunities and Risks of Foundation Models.pdf;/Users/andrew/Zotero/storage/ZF3UG93I/2108.html}
}

@article{bozkurta.SpeculativeFuturesChatGPT2023,
  title = {Speculative Futures on {{ChatGPT}} and Generative Artificial Intelligence ({{AI}}): {{A}} Collective Reflection from the Educational Landscape},
  shorttitle = {Speculative Futures on {{ChatGPT}} and Generative Artificial Intelligence ({{AI}})},
  author = {Bozkurt, A. and Xiao, J. and Lambert, S. and Pazurek, A. and Crompton, H. and Koseoglu, S. and Farrow, R. and Bond, M. and Nerantzi, C. and Honeychurch, S. and Bali, M. and Dron, J. and Mir, K. and Stewart, B. and Costello, E. and Mason, J. and Stracke, C. M. and Romero-Hall, E. and Koutropoulos, A. and Toquero, C. M. and Singh, L. and Tlili, A. and Lee, K. and Nichols, M. and Ossiannilsson, E. and Brown, M. and Irvine, V. and Raffaghelli, J. E. and Santos-Hermosa, G. and Farrell, O. and Adam, T. and Thong, Y. L. and Sani-Bozkurt, S. and Sharma, R. C. and Hrastinski, S. and Jandrić, P.},
  date = {2023-02-13},
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.7636568},
  url = {https://zenodo.org/record/7636568},
  urldate = {2023-03-23},
  abstract = {While ChatGPT has recently become very popular, AI has a long history and philosophy. This paper intends to explore the promises and pitfalls of the Generative Pre-trained Transformer (GPT) AI and potentially future technologies by adopting a speculative methodology. Speculative future narratives with a specific focus on educational contexts are provided in an attempt to identify emerging themes and discuss their implications for education in the 21st century. Affordances of (using) AI in Education (AIEd) and possible adverse effects are identified and discussed which emerge from the narratives. It is argued that now is the best of times to define human vs AI contribution to education because AI can accomplish more and more educational activities that used to be the prerogative of human educators. Therefore, it is imperative to rethink the respective roles of technology and human educators in education with a future-oriented mindset.},
  langid = {english},
  keywords = {/unread,artificial intelligence (AI),artificial intelligence in education (AIEd),future educational perspectives,generative pre-trained transformer (GPT),natural language processing,speculative methodology},
  file = {/Users/andrew/Zotero/storage/B7H35EFB/Bozkurt, A. et al. - 2023 - Speculative futures on ChatGPT and generative arti.pdf}
}

@article{branwenScalingHypothesis2020,
  title = {The {{Scaling Hypothesis}}},
  author = {Branwen, Gwern},
  date = {2020-05-28},
  url = {https://gwern.net/scaling-hypothesis},
  urldate = {2023-06-30},
  abstract = {On GPT-3: meta-learning, scaling, implications, and deep theory. The scaling hypothesis: neural nets absorb data \& compute, generalizing and becoming more Bayesian as problems get harder, manifesting new abilities even at trivial-by-global-standards-scale. The deep learning revolution has begun as foretold.},
  langid = {american},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/QHRZYSBL/scaling-hypothesis.html}
}

@online{broschinskiGrafikenErklaertFunktioniert2023,
  title = {In 9 Grafiken erklärt – So funktioniert künstliche Intelligenz},
  author = {Broschinski, Sebastian and Plattner, Titus and Meier, Patrick and Vögeli, Patrick},
  date = {2023-06-10},
  url = {https://www.derbund.ch/so-funktioniert-kuenstliche-intelligenz-599276436215},
  urldate = {2023-06-13},
  abstract = {Kann künstliche Intelligenz mehr als Äpfel und Birnen sortieren? Und warum lassen sich Computer immer noch leicht übertölpeln? Hier finden Sie alle Antworten.},
  langid = {ngerman},
  organization = {{Der Bund}},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/3G74I43I/so-funktioniert-kuenstliche-intelligenz-599276436215.html}
}

@online{brownLanguageModelsAre2020a,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2023-03-01},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/SUCZVXXP/Brown et al_2020_Language Models are Few-Shot Learners.pdf;/Users/andrew/Zotero/storage/JW9ITXGI/2005.html}
}

@online{brownLanguageModelsAre2020b,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2023-06-08},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/HSTN9QGG/Brown et al_2020_Language Models are Few-Shot Learners.pdf;/Users/andrew/Zotero/storage/VHPJRB6M/2005.html}
}

@online{bubeckSparksArtificialGeneral2023,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  date = {2023-04-13},
  eprint = {2303.12712},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.12712},
  urldate = {2023-05-29},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4 [Ope23], was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT4 is part of a new cohort of LLMs (along with ChatGPT and Google’s PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4’s performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/2QTBXBKP/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf}
}

@article{buckHochschulbildungVorHintergrund2023,
  title = {Hochschulbildung vor dem Hintergrund von Natural Language Processing (KI-Schreibtools)},
  author = {Buck, Isabella and Limburg, Anika},
  date = {2023},
  langid = {ngerman},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/ZYDESPAY/Buck and Limburg - 2023 - Hochschulbildung vor dem Hintergrund von Natural L.pdf}
}

@article{cardonaArtificialIntelligenceFuture,
  title = {Artificial {{Intelligence}} and the {{Future}} of {{Teaching}} and {{Learning}}},
  author = {Cardona, Miguel A and Rodríguez, Roberto J and Ishmael, Kristina},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/GZIMYF79/Cardona et al. - Artificial Intelligence and the Future of Teaching.pdf}
}

@article{chiLearningHumanTutoring2001,
  title = {Learning from Human Tutoring},
  author = {Chi, Michelene and Siler, Stephanie and Jeong, Heisawn and Yamauchi, Takashi and Hausmann, Robert},
  date = {2001-07-01},
  journaltitle = {Cognitive Science},
  shortjournal = {Cognitive Science},
  volume = {25},
  pages = {471--533},
  doi = {10.1016/S0364-0213(01)00044-1},
  abstract = {Human one-to-one tutoring has been shown to be a very effective form of instruction. Three contrasting hypotheses, a tutor-centered one, a student-centered one, and an interactive one could all potentially explain the effectiveness of tutoring. To test these hypotheses, analyses focused not only on the effectiveness of the tutors’ moves, but also on the effectiveness of the students’ construction on learning, as well as their interaction. The interaction hypothesis is further tested in the second study by manipulating the kind of tutoring tactics tutors were permitted to use. In order to promote a more interactive style of dialogue, rather than a didactic style, tutors were suppressed from giving explanations and feedback. Instead, tutors were encouraged to prompt the students. Surprisingly, students learned just as effectively even when tutors were suppressed from giving explanations and feedback. Their learning in the interactive style of tutoring is attributed to construction from deeper and a greater amount of scaffolding episodes, as well as their greater effort to take control of their own learning by reading more. What they learned from reading was limited, however, by their reading abilities.},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/BWWCXV84/Chi et al_2001_Learning from human tutoring.pdf}
}

@book{clarkSupersizingMindEmbodiment2008,
  title = {Supersizing the {{Mind}}: {{Embodiment}}, {{Action}}, and {{Cognitive Extension}}},
  shorttitle = {Supersizing the {{Mind}}},
  author = {Clark, Andy},
  date = {2008},
  publisher = {{New York: Oxford University Press}},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/UX96EM6A/CLASTM.html}
}

@article{corbettKnowledgeTracingModeling1994,
  title = {Knowledge Tracing: {{Modeling}} the Acquisition of Procedural Knowledge},
  shorttitle = {Knowledge Tracing},
  author = {Corbett, Albert T. and Anderson, John R.},
  date = {1994-12-01},
  journaltitle = {User Modeling and User-Adapted Interaction},
  shortjournal = {User Model User-Adap Inter},
  volume = {4},
  number = {4},
  pages = {253--278},
  issn = {1573-1391},
  doi = {10.1007/BF01099821},
  url = {https://doi.org/10.1007/BF01099821},
  urldate = {2023-03-31},
  abstract = {This paper describes an effort to model students' changing knowledge state during skill acquisition. Students in this research are learning to write short programs with the ACT Programming Tutor (APT). APT is constructed around a production rule cognitive model of programming knowledge, called theideal student model. This model allows the tutor to solve exercises along with the student and provide assistance as necessary. As the student works, the tutor also maintains an estimate of the probability that the student has learned each of the rules in the ideal model, in a process calledknowledge tracing. The tutor presents an individualized sequence of exercises to the student based on these probability estimates until the student has ‘mastered’ each rule. The programming tutor, cognitive model and learning and performance assumptions are described. A series of studies is reviewed that examine the empirical validity of knowledge tracing and has led to modifications in the process. Currently the model is quite successful in predicting test performance. Further modifications in the modeling process are discussed that may improve performance levels.},
  langid = {english},
  keywords = {/unread,empirical validity,individual differences,intelligent tutoring systems,learning,mastery learning,procedural knowledge,Student modeling},
  file = {/Users/andrew/Zotero/storage/BCTFGDNV/Corbett_Anderson_1994_Knowledge tracing.pdf}
}

@online{creswellFaithfulReasoningUsing2022,
  title = {Faithful {{Reasoning Using Large Language Models}}},
  author = {Creswell, Antonia and Shanahan, Murray},
  date = {2022-08-30},
  eprint = {2208.14271},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.14271},
  url = {http://arxiv.org/abs/2208.14271},
  urldate = {2023-04-21},
  abstract = {Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/JHWPXKW7/Creswell_Shanahan_2022_Faithful Reasoning Using Large Language Models.pdf;/Users/andrew/Zotero/storage/C9TB3MXG/2208.html}
}

@online{daiCanLargeLanguage2023,
  title = {Can {{Large Language Models Provide Feedback}} to {{Students}}? {{A Case Study}} on {{ChatGPT}}},
  shorttitle = {Can {{Large Language Models Provide Feedback}} to {{Students}}?},
  author = {Dai, Wei and Lin, Jionghao and Jin, Flora and Li, Tongguang and Tsai, Yi-Shan and Gasevic, Dragan and Chen, Guanliang},
  date = {2023-04-13T06:49:18},
  doi = {10.35542/osf.io/hcgzj},
  url = {https://edarxiv.org/hcgzj/},
  urldate = {2023-04-15},
  abstract = {Educational feedback has been widely acknowledged as an effective approach to improving student learning. However, scaling effective practices can be laborious and costly, which motivated researchers to work on automated feedback systems (AFS). Inspired by the recent advancements in the pre-trained language models (e.g., ChatGPT), we posit that such models might advance the existing knowledge of textual feedback generation in AFS because of their capability to offer natural-sounding and detailed responses. Therefore, we aimed to investigate the feasibility of using ChatGPT to provide students with feedback to help them learn better. Specifically, we first examined the readability of ChatGPT-generated feedback. Then, we measured the agreement between ChatGPT and the instructor when assessing students' assignments according to the marking rubric. Finally, we used a well-known theoretical feedback framework to further investigate the effectiveness of the feedback generated by ChatGPT. Our results show that i) ChatGPT is capable of generating more detailed feedback that fluently and coherently summarizes students' performance than human instructors; ii) ChatGPT achieved high agreement with the instructor when assessing the topic of students' assignments; and iii) ChatGPT could provide feedback on the process of students completing the task, which benefits students developing learning skills.},
  langid = {american},
  pubstate = {preprint},
  keywords = {/unread,and Research,Automated Feedback,Education,Educational Assessment,Educational Methods,Evaluation,Feedback Effectiveness,Feedback Generation,Higher Education,Large Language Model},
  file = {/Users/andrew/Zotero/storage/ICYKBJ87/Dai et al_2023_Can Large Language Models Provide Feedback to Students.pdf}
}

@article{degallier-rochatHumanAugmentationNot2022,
  title = {Human Augmentation, Not Replacement: {{A}} Research Agenda for {{AI}} and Robotics in the Industry},
  shorttitle = {Human Augmentation, Not Replacement},
  author = {Dégallier-Rochat, Sarah and Kurpicz-Briki, Mascha and Endrissat, Nada and Yatsenko, Olena},
  date = {2022},
  journaltitle = {Frontiers in Robotics and AI},
  volume = {9},
  issn = {2296-9144},
  doi = {10.3389/frobt.2022.997386},
  url = {https://www.frontiersin.org/articles/10.3389/frobt.2022.997386},
  urldate = {2023-03-03},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/9QEXUZ9N/Dégallier-Rochat et al_2022_Human augmentation, not replacement.pdf}
}

@article{demszkyUsingLargeLanguage2023,
  title = {Using Large Language Models in Psychology},
  author = {Demszky, Dorottya and Yang, Diyi and Yeager, David S. and Bryan, Christopher J. and Clapper, Margarett and Chandhok, Susannah and Eichstaedt, Johannes C. and Hecht, Cameron and Jamieson, Jeremy and Johnson, Meghann and Jones, Michaela and Krettek-Cobb, Danielle and Lai, Leslie and JonesMitchell, Nirel and Ong, Desmond C. and Dweck, Carol S. and Gross, James J. and Pennebaker, James W.},
  date = {2023-10-13},
  journaltitle = {Nature Reviews Psychology},
  shortjournal = {Nat Rev Psychol},
  pages = {1--14},
  publisher = {{Nature Publishing Group}},
  issn = {2731-0574},
  doi = {10.1038/s44159-023-00241-5},
  url = {https://www.nature.com/articles/s44159-023-00241-5},
  urldate = {2023-10-21},
  abstract = {Large language models (LLMs), such as OpenAI’s GPT-4, Google’s Bard or Meta’s LLaMa, have created unprecedented opportunities for analysing and generating language data on a massive scale. Because language data have a central role in all areas of psychology, this new technology has the potential to transform the field. In this Perspective, we review the foundations of LLMs. We then explain how the way that LLMs are constructed enables them to effectively generate human-like linguistic output without the ability to think or feel like a human. We argue that although LLMs have the potential to advance psychological measurement, experimentation and practice, they are not yet ready for many of the most transformative psychological applications — but further research and development may enable such use. Next, we examine four major concerns about the application of LLMs to psychology, and how each might be overcome. Finally, we conclude with recommendations for investments that could help to address these concerns: field-initiated ‘keystone’ datasets; increased standardization of performance benchmarks; and shared computing and analysis infrastructure to ensure that the future of LLM-powered research is equitable.},
  langid = {english},
  keywords = {Human behaviour,Language and linguistics,Psychology,Science,technology and society},
  file = {/Users/andrew/Zotero/storage/7BZWJTSP/Demszky et al_2023_Using large language models in psychology.pdf}
}

@article{desaireDistinguishingAcademicScience2023,
  title = {Distinguishing Academic Science Writing from Humans or {{ChatGPT}} with over 99\% Accuracy Using Off-the-Shelf Machine Learning Tools},
  author = {Desaire, Heather and Chua, Aleesa E. and Isom, Madeline and Jarosova, Romana and Hua, David},
  date = {2023-06},
  journaltitle = {Cell Reports Physical Science},
  shortjournal = {Cell Reports Physical Science},
  pages = {101426},
  issn = {26663864},
  doi = {10.1016/j.xcrp.2023.101426},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S266638642300200X},
  urldate = {2023-06-08},
  abstract = {ChatGPT has enabled access to artificial intelligence (AI)-generated writing for the masses, initiating a culture shift in the way people work, learn, and write. The need to discriminate human writing from AI is now both critical and urgent. Addressing this need, we report a method for discriminating text generated by ChatGPT from (human) academic scientists, relying on prevalent and accessible supervised classification methods. The approach uses new features for discriminating (these) humans from AI; as examples, scientists write long paragraphs and have a penchant for equivocal language, frequently using words like ‘‘but,’’ ‘‘however,’’ and ‘‘although.’’ With a set of 20 features, we built a model that assigns the author, as human or AI, at over 99\% accuracy. This strategy could be further adapted and developed by others with basic skills in supervised classification, enabling access to many highly accurate and targeted models for detecting AI usage in academic writing and beyond.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/VFDLC92L/Desaire et al. - 2023 - Distinguishing academic science writing from human.pdf}
}

@article{desaireDistinguishingAcademicScience2023a,
  title = {Distinguishing Academic Science Writing from Humans or {{ChatGPT}} with over 99\% Accuracy Using Off-the-Shelf Machine Learning Tools},
  author = {Desaire, Heather and Chua, Aleesa E. and Isom, Madeline and Jarosova, Romana and Hua, David},
  date = {2023-06-07},
  journaltitle = {Cell Reports Physical Science},
  shortjournal = {Cell Reports Physical Science},
  pages = {101426},
  issn = {2666-3864},
  doi = {10.1016/j.xcrp.2023.101426},
  url = {https://www.sciencedirect.com/science/article/pii/S266638642300200X},
  urldate = {2023-06-11},
  abstract = {ChatGPT has enabled access to artificial intelligence (AI)-generated writing for the masses, initiating a culture shift in the way people work, learn, and write. The need to discriminate human writing from AI is now both critical and urgent. Addressing this need, we report a method for discriminating text generated by ChatGPT from (human) academic scientists, relying on prevalent and accessible supervised classification methods. The approach uses new features for discriminating (these) humans from AI; as examples, scientists write long paragraphs and have a penchant for equivocal language, frequently using words like “but,” “however,” and “although.” With a set of 20 features, we built a model that assigns the author, as human or AI, at over 99\% accuracy. This strategy could be further adapted and developed by others with basic skills in supervised classification, enabling access to many highly accurate and targeted models for detecting AI usage in academic writing and beyond.},
  langid = {english},
  keywords = {/unread,AI,ChatGPT,machine learing,plagiarism,text analysis,XGBoost},
  file = {/Users/andrew/Zotero/storage/QT575F6U/Desaire et al_2023_Distinguishing academic science writing from humans or ChatGPT with over 99%.pdf;/Users/andrew/Zotero/storage/Q3KICH9D/S266638642300200X.html}
}

@article{DidaktischeUndRechtliche2023,
  title = {Didaktische Und Rechtliche {{Perspektiven}} Auf {{KI-gestütztes Schreiben}} in Der {{Hochschulbildung}}},
  date = {2023-03-07},
  url = {https://hss-opus.ub.ruhr-uni-bochum.de/opus4/frontdoor/index/index/docId/9734},
  urldate = {2023-03-15},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/4IZVMT7F/2023_Didaktische und rechtliche Perspektiven auf KI-gestütztes Schreiben in der.pdf}
}

@online{dohanLanguageModelCascades2022,
  title = {Language {{Model Cascades}}},
  author = {Dohan, David and Xu, Winnie and Lewkowycz, Aitor and Austin, Jacob and Bieber, David and Lopes, Raphael Gontijo and Wu, Yuhuai and Michalewski, Henryk and Saurous, Rif A. and Sohl-dickstein, Jascha and Murphy, Kevin and Sutton, Charles},
  date = {2022-07-28},
  eprint = {2207.10342},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.10342},
  url = {http://arxiv.org/abs/2207.10342},
  urldate = {2023-04-21},
  abstract = {Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/V8KLV6ZR/Dohan et al_2022_Language Model Cascades.pdf;/Users/andrew/Zotero/storage/Y74R2PJV/2207.html}
}

@online{dohanLanguageModelCascades2022a,
  title = {Language {{Model Cascades}}},
  author = {Dohan, David and Xu, Winnie and Lewkowycz, Aitor and Austin, Jacob and Bieber, David and Lopes, Raphael Gontijo and Wu, Yuhuai and Michalewski, Henryk and Saurous, Rif A. and Sohl-dickstein, Jascha and Murphy, Kevin and Sutton, Charles},
  date = {2022-07-28},
  eprint = {2207.10342},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.10342},
  url = {http://arxiv.org/abs/2207.10342},
  urldate = {2023-10-24},
  abstract = {Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/KVLYHVNZ/Dohan et al_2022_Language Model Cascades.pdf;/Users/andrew/Zotero/storage/7CHTB98K/2207.html}
}

@online{dziriFaithFateLimits2023,
  title = {Faith and {{Fate}}: {{Limits}} of {{Transformers}} on {{Compositionality}}},
  shorttitle = {Faith and {{Fate}}},
  author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Welleck, Sean and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
  date = {2023-06-01},
  eprint = {2305.18654},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.18654},
  url = {http://arxiv.org/abs/2305.18654},
  urldate = {2023-08-29},
  abstract = {Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how Transformers' performance will rapidly decay with increased task complexity.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/K8JPBESX/Dziri et al_2023_Faith and Fate.pdf;/Users/andrew/Zotero/storage/CCHSFQ65/2305.html}
}

@unpublished{fleckPruefungsrechtlicheFragenChatGPT2023,
  title = {Prüfungsrechtliche Fragen zu ChatGPT},
  author = {Fleck, Tilmann},
  date = {2023},
  langid = {ngerman},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/72DBD84Q/Fleck - Prüfungsrechtliche Fragen zu ChatGPT.pdf}
}

@online{gandhiUnderstandingSocialReasoning2023,
  title = {Understanding {{Social Reasoning}} in {{Language Models}} with {{Language Models}}},
  author = {Gandhi, Kanishk and Fränken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah D.},
  date = {2023-06-21},
  eprint = {2306.15448},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.15448},
  url = {http://arxiv.org/abs/2306.15448},
  urldate = {2023-07-01},
  abstract = {As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/andrew/Zotero/storage/X4HKY53H/Gandhi et al_2023_Understanding Social Reasoning in Language Models with Language Models.pdf;/Users/andrew/Zotero/storage/6HKAPS44/2306.html}
}

@online{gandhiUnderstandingSocialReasoning2023a,
  title = {Understanding {{Social Reasoning}} in {{Language Models}} with {{Language Models}}},
  author = {Gandhi, Kanishk and Fränken, Jan-Philipp and Gerstenberg, Tobias and Goodman, Noah D.},
  date = {2023-06-21},
  eprint = {2306.15448},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.15448},
  url = {http://arxiv.org/abs/2306.15448},
  urldate = {2023-10-15},
  abstract = {As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/andrew/Zotero/storage/CPH23HUV/Gandhi et al_2023_Understanding Social Reasoning in Language Models with Language Models.pdf;/Users/andrew/Zotero/storage/YGMU6RWE/2306.html}
}

@article{ghoshOptionTracingBinary,
  title = {Option {{Tracing}}: {{Beyond Binary Knowledge Tracing}}},
  author = {Ghosh, Aritra and Lan, Andrew S},
  abstract = {This paper details our solutions to Tasks 1\&2 of the NeurIPS 2020 Education Challenge.1 Knowledge tracing, a family of methods to estimate each student’s mastery levels on skills/knowledge components from their past responses to assessment questions, is useful for progress monitoring, personalization, and helping teachers to deliver personalized and targeted feedback to students to improve their learning outcomes. One key limitation of current knowledge tracing methods is that they can only estimate an overall knowledge level of a student since they analyze only the binary-valued correctness of student responses. We adapt a series of popular knowledge tracing methods to the task of option prediction in multiple choice questions. Experimental results show that our method performs well on both option prediction and correctness prediction.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/IHK5ULH6/Ghosh and Lan - Option Tracing Beyond Binary Knowledge Tracing.pdf}
}

@online{gurneeLanguageModelsRepresent2023,
  title = {Language {{Models Represent Space}} and {{Time}}},
  author = {Gurnee, Wes and Tegmark, Max},
  date = {2023-10-03},
  eprint = {2310.02207},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.02207},
  url = {http://arxiv.org/abs/2310.02207},
  urldate = {2023-10-04},
  abstract = {The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process -- a world model. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual ``space neurons'' and ``time neurons'' that reliably encode spatial and temporal coordinates. Our analysis demonstrates that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn not merely superficial statistics, but literal world models.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/SDS6AINH/Gurnee_Tegmark_2023_Language Models Represent Space and Time.pdf;/Users/andrew/Zotero/storage/NT4RMCNI/2310.html}
}

@online{hagendorffMachinePsychologyInvestigating2023,
  title = {Machine {{Psychology}}: {{Investigating Emergent Capabilities}} and {{Behavior}} in {{Large Language Models Using Psychological Methods}}},
  shorttitle = {Machine {{Psychology}}},
  author = {Hagendorff, Thilo},
  date = {2023-04-11},
  eprint = {2303.13988},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.13988},
  url = {http://arxiv.org/abs/2303.13988},
  urldate = {2023-04-20},
  abstract = {Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behavioral patterns discovered in LLMs are to be interpreted. In sum, machine psychology aims to discover emergent abilities in LLMs that cannot be detected by most traditional natural language processing benchmarks.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/GGMKIIXK/Hagendorff_2023_Machine Psychology.pdf;/Users/andrew/Zotero/storage/GNLYS9D7/2303.html}
}

@online{Home,
  title = {Home},
  url = {https://sites.google.com/view/social-reasoning-lms},
  urldate = {2023-07-01},
  abstract = {ABSTRACT As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/8W93VA9V/social-reasoning-lms.html}
}

@online{hrefHowDoesIncontext,
  title = {How Does In-Context Learning Work? {{A}} Framework for Understanding the Differences from Traditional Supervised Learning},
  shorttitle = {How Does In-Context Learning Work?},
  author = {{href=},},
  url = {http://ai.stanford.edu/blog/understanding-incontext/},
  urldate = {2023-03-28},
  abstract = {The official Stanford AI Lab blog},
  keywords = {/unread}
}

@online{huangLanguageNotAll2023,
  title = {Language {{Is Not All You Need}}: {{Aligning Perception}} with {{Language Models}}},
  shorttitle = {Language {{Is Not All You Need}}},
  author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Patra, Barun and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu},
  date = {2023-03-01},
  eprint = {2302.14045},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.14045},
  urldate = {2023-03-02},
  abstract = {A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/andrew/Zotero/storage/U6D9JARC/Huang et al_2023_Language Is Not All You Need.pdf;/Users/andrew/Zotero/storage/MEX5ATVT/2302.html}
}

@online{huangReasoningLargeLanguage2022,
  title = {Towards {{Reasoning}} in {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Towards {{Reasoning}} in {{Large Language Models}}},
  author = {Huang, Jie and Chang, Kevin Chen-Chuan},
  date = {2022-12-20},
  eprint = {2212.10403},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.10403},
  url = {http://arxiv.org/abs/2212.10403},
  urldate = {2023-04-20},
  abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/XL6CTLJE/Huang_Chang_2022_Towards Reasoning in Large Language Models.pdf;/Users/andrew/Zotero/storage/D4H273EY/2212.html}
}

@inproceedings{hunzikerTeachingMultipleConcepts2019a,
  title = {Teaching {{Multiple Concepts}} to a {{Forgetful Learner}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hunziker, Anette and Chen, Yuxin and Mac Aodha, Oisin and Gomez Rodriguez, Manuel and Krause, Andreas and Perona, Pietro and Yue, Yisong and Singla, Adish},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/2952351097998ac1240cb2ab7333a3d2-Abstract.html},
  urldate = {2023-02-13},
  abstract = {How can we help a forgetful learner learn multiple concepts within a limited time frame? While there have been extensive studies in designing optimal schedules for teaching a single concept given a learner's memory model, existing approaches for teaching multiple concepts are typically based on heuristic scheduling techniques without theoretical guarantees. In this paper, we look at the problem from the perspective of discrete optimization and introduce a novel algorithmic framework for teaching multiple concepts with strong performance guarantees.  Our framework is both generic, allowing the design of teaching schedules for different memory models, and also interactive, allowing the teacher to adapt the schedule to the underlying forgetting mechanisms of the learner. Furthermore, for a well-known memory model, we are able to identify a regime of model parameters where our framework is guaranteed to achieve high performance. We perform extensive evaluations using simulations along with real user studies in two concrete applications: (i) an educational app for online vocabulary teaching; and (ii) an app for teaching novices how to recognize animal species from images.  Our results demonstrate the effectiveness of our algorithm compared to popular heuristic approaches.},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/W2EDAAKP/Hunziker et al_2019_Teaching Multiple Concepts to a Forgetful Learner.pdf}
}

@online{huThoughtCloningLearning2023,
  title = {Thought {{Cloning}}: {{Learning}} to {{Think}} While {{Acting}} by {{Imitating Human Thinking}}},
  shorttitle = {Thought {{Cloning}}},
  author = {Hu, Shengran and Clune, Jeff},
  date = {2023-05-31},
  eprint = {2306.00323},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.00323},
  url = {http://arxiv.org/abs/2306.00323},
  urldate = {2023-06-09},
  abstract = {Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than Behavioral Cloning and its performance advantage grows the further out of distribution test tasks are, highlighting its ability to better handle novel situations. Thought Cloning also provides important benefits for AI Safety and Interpretability, and makes it easier to debug and improve AI. Because we can observe the agent's thoughts, we can (1) more easily diagnose why things are going wrong, making it easier to fix the problem, (2) steer the agent by correcting its thinking, or (3) prevent it from doing unsafe things it plans to do. Overall, by training agents how to think as well as behave, Thought Cloning creates safer, more powerful agents.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/XGKRWFQ6/Hu_Clune_2023_Thought Cloning.pdf;/Users/andrew/Zotero/storage/7DUCYMMG/2306.html}
}

@online{imaniMathPrompterMathematicalReasoning2023,
  title = {{{MathPrompter}}: {{Mathematical Reasoning}} Using {{Large Language Models}}},
  shorttitle = {{{MathPrompter}}},
  author = {Imani, Shima and Du, Liang and Shrivastava, Harsh},
  date = {2023-03-03},
  eprint = {2303.05398},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.05398},
  urldate = {2023-03-23},
  abstract = {Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose `MathPrompter', a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple Algebraic expressions or Python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the MultiArith dataset (\$78.7\textbackslash\%\textbackslash rightarrow92.5\textbackslash\%\$) evaluated using 175B parameter GPT-based LLM.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/7A7HVRIY/Imani et al_2023_MathPrompter.pdf;/Users/andrew/Zotero/storage/PGSEZPZP/2303.html}
}

@online{jinCanLargeLanguage2023,
  title = {Can {{Large Language Models Infer Causation}} from {{Correlation}}?},
  author = {Jin, Zhijing and Liu, Jiarui and Lyu, Zhiheng and Poff, Spencer and Sachan, Mrinmaya and Mihalcea, Rada and Diab, Mona and Schölkopf, Bernhard},
  date = {2023-06-09},
  eprint = {2306.05836},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.05836},
  url = {http://arxiv.org/abs/2306.05836},
  urldate = {2023-09-21},
  abstract = {Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/4GZHHJS6/Jin et al_2023_Can Large Language Models Infer Causation from Correlation.pdf;/Users/andrew/Zotero/storage/98ST4W58/2306.html}
}

@online{kaddourChallengesApplicationsLarge2023,
  title = {Challenges and {{Applications}} of {{Large Language Models}}},
  author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  date = {2023-07-19},
  eprint = {2307.10169},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.10169},
  url = {http://arxiv.org/abs/2307.10169},
  urldate = {2023-07-25},
  abstract = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/THLCX6BU/Kaddour et al_2023_Challenges and Applications of Large Language Models.pdf;/Users/andrew/Zotero/storage/LY9873T3/2307.html}
}

@incollection{kaserKnowledgeTracingModeling2014,
  title = {Beyond {{Knowledge Tracing}}: {{Modeling Skill Topologies}} with {{Bayesian Networks}}},
  shorttitle = {Beyond {{Knowledge Tracing}}},
  booktitle = {Intelligent {{Tutoring Systems}}},
  author = {Käser, Tanja and Klingler, Severin and Schwing, Alexander Gerhard and Gross, Markus},
  editor = {Trausan-Matu, Stefan and Boyer, Kristy Elizabeth and Crosby, Martha and Panourgia, Kitty},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard},
  editorbtype = {redactor},
  date = {2014},
  volume = {8474},
  pages = {188--198},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-07221-0_23},
  url = {http://link.springer.com/10.1007/978-3-319-07221-0_23},
  urldate = {2023-02-23},
  abstract = {Modeling and predicting student knowledge is a fundamental task of an intelligent tutoring system. A popular approach for student modeling is Bayesian Knowledge Tracing (BKT). BKT models, however, lack the ability to describe the hierarchy and relationships between the different skills of a learning domain. In this work, we therefore aim at increasing the representational power of the student model by employing dynamic Bayesian networks that are able to represent such skill topologies. To ensure model interpretability, we constrain the parameter space. We evaluate the performance of our models on five large-scale data sets of different learning domains such as mathematics, spelling learning and physics, and demonstrate that our approach outperforms BKT in prediction accuracy on unseen data across all learning domains.},
  isbn = {978-3-319-07220-3 978-3-319-07221-0},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/PVTWQBMU/Käser et al. - 2014 - Beyond Knowledge Tracing Modeling Skill Topologie.pdf}
}

@incollection{kaserKnowledgeTracingModeling2014a,
  title = {Beyond {{Knowledge Tracing}}: {{Modeling Skill Topologies}} with {{Bayesian Networks}}},
  shorttitle = {Beyond {{Knowledge Tracing}}},
  booktitle = {Intelligent {{Tutoring Systems}}},
  author = {Käser, Tanja and Klingler, Severin and Schwing, Alexander Gerhard and Gross, Markus},
  editor = {Trausan-Matu, Stefan and Boyer, Kristy Elizabeth and Crosby, Martha and Panourgia, Kitty},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard},
  editorbtype = {redactor},
  date = {2014},
  volume = {8474},
  pages = {188--198},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-07221-0_23},
  url = {http://link.springer.com/10.1007/978-3-319-07221-0_23},
  urldate = {2023-03-08},
  abstract = {Modeling and predicting student knowledge is a fundamental task of an intelligent tutoring system. A popular approach for student modeling is Bayesian Knowledge Tracing (BKT). BKT models, however, lack the ability to describe the hierarchy and relationships between the different skills of a learning domain. In this work, we therefore aim at increasing the representational power of the student model by employing dynamic Bayesian networks that are able to represent such skill topologies. To ensure model interpretability, we constrain the parameter space. We evaluate the performance of our models on five large-scale data sets of different learning domains such as mathematics, spelling learning and physics, and demonstrate that our approach outperforms BKT in prediction accuracy on unseen data across all learning domains.},
  isbn = {978-3-319-07220-3 978-3-319-07221-0},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/96L8MSFW/Käser et al. - 2014 - Beyond Knowledge Tracing Modeling Skill Topologie.pdf}
}

@online{kirchenbauerWatermarkLargeLanguage2023,
  title = {A {{Watermark}} for {{Large Language Models}}},
  author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  date = {2023-01-27},
  eprint = {2301.10226},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.10226},
  url = {http://arxiv.org/abs/2301.10226},
  urldate = {2023-02-15},
  abstract = {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/QIQVSX4J/Kirchenbauer et al_2023_A Watermark for Large Language Models.pdf;/Users/andrew/Zotero/storage/IUF8M4VC/2301.html}
}

@article{kojimaLargeLanguageModels,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-bystep answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs’ ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding “Let’s think step by step” before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large-scale InstructGPT model (text-davinci002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/CJ2G7HTR/Kojima et al. - Large Language Models are Zero-Shot Reasoners.pdf}
}

@online{kosinskiTheoryMindMay2023,
  title = {Theory of {{Mind May Have Spontaneously Emerged}} in {{Large Language Models}}},
  author = {Kosinski, Michal},
  date = {2023-03-14},
  eprint = {2302.02083},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.02083},
  url = {http://arxiv.org/abs/2302.02083},
  urldate = {2023-04-20},
  abstract = {Theory of mind (ToM), or the ability to impute unobservable mental states to others, is central to human social interactions, communication, empathy, self-consciousness, and morality. We tested several language models using 40 classic false-belief tasks widely used to test ToM in humans. The models published before 2020 showed virtually no ability to solve ToM tasks. Yet, the first version of GPT-3 ("davinci-001"), published in May 2020, solved about 40\% of false-belief tasks-performance comparable with 3.5-year-old children. Its second version ("davinci-002"; January 2022) solved 70\% of false-belief tasks, performance comparable with six-year-olds. Its most recent version, GPT-3.5 ("davinci-003"; November 2022), solved 90\% of false-belief tasks, at the level of seven-year-olds. GPT-4 published in March 2023 solved nearly all the tasks (95\%). These findings suggest that ToM-like ability (thus far considered to be uniquely human) may have spontaneously emerged as a byproduct of language models' improving language skills.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction},
  file = {/Users/andrew/Zotero/storage/PSVQIFRD/Kosinski_2023_Theory of Mind May Have Spontaneously Emerged in Large Language Models.pdf;/Users/andrew/Zotero/storage/IGGWBMA9/2302.html}
}

@online{kungPerformanceChatGPTUSMLE2022,
  title = {Performance of {{ChatGPT}} on {{USMLE}}: {{Potential}} for {{AI-Assisted Medical Education Using Large Language Models}}},
  shorttitle = {Performance of {{ChatGPT}} on {{USMLE}}},
  author = {Kung, Tiffany H. and Cheatham, Morgan and ChatGPT and Medenilla, Arielle and Sillos, Czarina and Leon, Lorie De and Elepaño, Camille and Madriaga, Maria and Aggabao, Rimel and Diaz-Candido, Giezel and Maningo, James and Tseng, Victor},
  date = {2022-12-21},
  eprinttype = {medRxiv},
  pages = {2022.12.19.22283643},
  doi = {10.1101/2022.12.19.22283643},
  url = {https://www.medrxiv.org/content/10.1101/2022.12.19.22283643v2},
  urldate = {2023-02-11},
  abstract = {We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/5SATZSAZ/Kung et al_2022_Performance of ChatGPT on USMLE.pdf}
}

@article{kurpicz-brikiCulturalDifferencesBias,
  title = {Cultural {{Differences}} in {{Bias}}? {{Origin}} and {{Gender Bias}} in {{Pre-Trained German}} and {{French Word Embeddings}}},
  author = {Kurpicz-Briki, Mascha},
  abstract = {Smart applications often rely on training data in form of text. If there is a bias in that training data, the decision of the applications might not be fair. Common training data has been shown to be biased towards different groups of minorities. However, there is no generic algorithm to determine the fairness of training data. One existing approach is to measure gender bias using word embeddings. Most research in this field has been dedicated to the English language. In this work, we identified that there is a bias towards gender and origin in both German and French word embeddings. In particular, we found that real-world bias and stereotypes from the 18th century are still included in today’s word embeddings. Furthermore, we show that the gender bias in German has a different form from English and there is indication that bias has cultural differences that need to be considered when analyzing texts and word embeddings in different languages.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/X4NJQJNZ/Kurpicz-Briki - Cultural Differences in Bias Origin and Gender Bi.pdf}
}

@article{kurpicz-brikiWorldFullStereotypes2021,
  title = {A {{World Full}} of {{Stereotypes}}? {{Further Investigation}} on {{Origin}} and {{Gender Bias}} in {{Multi-Lingual Word Embeddings}}},
  shorttitle = {A {{World Full}} of {{Stereotypes}}?},
  author = {Kurpicz-Briki, Mascha and Leoni, Tomaso},
  date = {2021},
  journaltitle = {Frontiers in Big Data},
  volume = {4},
  issn = {2624-909X},
  doi = {10.3389/fdata.2021.625290},
  url = {https://www.frontiersin.org/articles/10.3389/fdata.2021.625290},
  urldate = {2023-03-03},
  abstract = {Publicly available off-the-shelf word embeddings that are often used in productive applications for natural language processing have been proven to be biased. We have previously shown that this bias can come in different forms, depending on the language and the cultural context. In this work, we extend our previous work and further investigate how bias varies in different languages. We examine Italian and Swedish word embeddings for gender and origin bias, and demonstrate how an origin bias concerning local migration groups in Switzerland is included in German word embeddings. We propose BiasWords, a method to automatically detect new forms of bias. Finally, we discuss how cultural and language aspects are relevant to the impact of bias on the application and to potential mitigation measures.},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/6GGF3AFM/Kurpicz-Briki_Leoni_2021_A World Full of Stereotypes.pdf}
}

@online{kwonRewardDesignLanguage2023,
  title = {Reward {{Design}} with {{Language Models}}},
  author = {Kwon, Minae and Xie, Sang Michael and Bullard, Kalesha and Sadigh, Dorsa},
  date = {2023-02-27},
  eprint = {2303.00001},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.00001},
  urldate = {2023-03-10},
  abstract = {Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by prompting a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of the desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperform RL agents trained with reward functions learned via supervised learning},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/AIXZC8BS/Kwon et al_2023_Reward Design with Language Models.pdf;/Users/andrew/Zotero/storage/PI3A34ML/2303.html}
}

@article{lakeBuildingMachinesThat2017,
  title = {Building Machines That Learn and Think like People},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  year = {2017/ed},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {40},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X16001837},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993#},
  urldate = {2021-02-08},
  abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/2TZWZ4KG/Lake et al_2017_Building machines that learn and think like people.pdf;/Users/andrew/Zotero/storage/2LBPILKD/A9535B1D745A0377E16C590E14B94993.html}
}

@article{lakeHumanFewshotLearning,
  title = {Human Few-Shot Learning of Compositional Instructions},
  author = {Lake, Brenden M and Linzen, Tal and Baroni, Marco},
  abstract = {People learn in fast and flexible ways that have not been emulated by machines. Once a person learns a new verb “dax,” he or she can effortlessly understand how to “dax twice,” “walk and dax,” or “dax vigorously.” There have been striking recent improvements in machine learning for natural language processing, yet the best algorithms require vast amounts of experience and struggle to generalize new concepts in compositional ways. To better understand these distinctively human abilities, we study the compositional skills of people through languagelike instruction learning tasks. Our results show that people can learn and use novel functional concepts from very few examples (few-shot learning), successfully applying familiar functions to novel inputs. People can also compose concepts in complex ways that go beyond the provided demonstrations. Two additional experiments examined the assumptions and inductive biases that people make when solving these tasks, revealing three biases: mutual exclusivity, one-to-one mappings, and iconic concatenation. We discuss the implications for cognitive modeling and the potential for building machines with more human-like language learning capabilities.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/W8H4AGKW/Lake et al. - Human few-shot learning of compositional instructi.pdf}
}

@article{lakeHumanlikeSystematicGeneralization2023,
  title = {Human-like Systematic Generalization through a Meta-Learning Neural Network},
  author = {Lake, Brenden M. and Baroni, Marco},
  date = {2023-10-25},
  journaltitle = {Nature},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06668-3},
  url = {https://www.nature.com/articles/s41586-023-06668-3},
  urldate = {2023-10-26},
  abstract = {The power of human language and thought arises from systematic compositionality—the algebraic ability to understand and produce novel combinations from known components. Fodor and Pylyshyn1 famously argued that artificial neural networks lack this capacity and are therefore not viable models of the mind. Neural networks have advanced considerably in the years since, yet the systematicity challenge persists. Here we successfully address Fodor and Pylyshyn’s challenge by providing evidence that neural networks can achieve human-like systematicity when optimized for their compositional skills. To do so, we introduce the meta-learning for compositionality (MLC) approach for guiding training through a dynamic stream of compositional tasks. To compare humans and machines, we conducted human behavioural experiments using an~instruction learning paradigm. After considering seven different models, we found that, in contrast to perfectly systematic but rigid probabilistic symbolic models, and perfectly flexible but unsystematic neural networks, only MLC achieves both the systematicity and flexibility needed for human-like generalization. MLC also advances the compositional skills of machine learning systems in several systematic generalization benchmarks. Our results show how a standard neural network architecture, optimized for its compositional skills, can mimic human systematic generalization in a head-to-head comparison.},
  langid = {english},
  keywords = {Computer science,Human behaviour},
  file = {/Users/andrew/Zotero/storage/QGPS8CIX/Lake_Baroni_2023_Human-like systematic generalization through a meta-learning neural network.pdf}
}

@article{lakePeopleInferRecursive2020,
  title = {People {{Infer Recursive Visual Concepts}} from {{Just}} a {{Few Examples}}},
  author = {Lake, Brenden M. and Piantadosi, Steven T.},
  date = {2020-03-01},
  journaltitle = {Computational Brain \& Behavior},
  shortjournal = {Comput Brain Behav},
  volume = {3},
  number = {1},
  pages = {54--65},
  issn = {2522-087X},
  doi = {10.1007/s42113-019-00053-y},
  url = {https://doi.org/10.1007/s42113-019-00053-y},
  urldate = {2022-02-03},
  abstract = {Machine learning has made major advances in categorizing objects in images, yet the best algorithms miss important aspects of how people learn and think about categories. People can learn richer concepts from fewer examples, including causal models that explain how members of a category are formed. Here, we explore the limits of this human ability to infer causal “programs”—latent generating processes with nontrivial algorithmic properties—from one, two, or three visual examples. People were asked to extrapolate the programs in several ways, for both classifying and generating new examples. As a theory of these inductive abilities, we present a Bayesian program learning model that searches the space of programs for the best explanation of the observations. Although variable, people’s judgments are broadly consistent with the model and inconsistent with several alternatives, including a pretrained deep neural network for object recognition, indicating that people can learn and reason with rich algorithmic abstractions from sparse input data.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/GMQQPPDV/Lake and Piantadosi - 2020 - People Infer Recursive Visual Concepts from Just a.pdf}
}

@online{lakeWordMeaningMinds2021,
  title = {Word Meaning in Minds and Machines},
  author = {Lake, Brenden M. and Murphy, Gregory L.},
  date = {2021-04-17},
  eprint = {2008.01766},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2008.01766},
  url = {http://arxiv.org/abs/2008.01766},
  urldate = {2023-02-11},
  abstract = {Machines have achieved a broad and growing set of linguistic competencies, thanks to recent progress in Natural Language Processing (NLP). Psychologists have shown increasing interest in such models, comparing their output to psychological judgments such as similarity, association, priming, and comprehension, raising the question of whether the models could serve as psychological theories. In this article, we compare how humans and machines represent the meaning of words. We argue that contemporary NLP systems are fairly successful models of human word similarity, but they fall short in many other respects. Current models are too strongly linked to the text-based patterns in large corpora, and too weakly linked to the desires, goals, and beliefs that people express through words. Word meanings must also be grounded in perception and action and be capable of flexible combinations in ways that current systems are not. We discuss more promising approaches to grounding NLP systems and argue that they will be more successful with a more human-like, conceptual basis for word meaning.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/UM2SEPQH/Lake_Murphy_2021_Word meaning in minds and machines.pdf;/Users/andrew/Zotero/storage/R7MFY27P/2008.html}
}

@article{lakeWordMeaningMinds2021a,
  title = {Word Meaning in Minds and Machines.},
  author = {Lake, Brenden M. and Murphy, Gregory L.},
  date = {2021-07-22},
  journaltitle = {Psychological Review},
  shortjournal = {Psychological Review},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/rev0000297},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/rev0000297},
  urldate = {2023-03-04},
  abstract = {Machines have achieved a broad and growing set of linguistic competencies, thanks to recent progress in Natural Language Processing (NLP). Psychologists have shown increasing interest in such models, comparing their output to psychological judgments such as similarity, association, priming, and comprehension, raising the question of whether the models could serve as psychological theories. In this article, we compare how humans and machines represent the meaning of words. We argue that contemporary NLP systems are fairly successful models of human word similarity, but they fall short in many other respects. Current models are too strongly linked to the text-based patterns in large corpora, and too weakly linked to the desires, goals, and beliefs that people express through words. Word meanings must also be grounded in perception and action and be capable of flexible combinations in ways that current systems are not. We discuss promising approaches to grounding NLP systems and argue that they will be more successful, with a more human-like, conceptual basis for word meaning.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/QN5IJG5Q/Lake and Murphy - 2021 - Word meaning in minds and machines..pdf}
}

@online{lampinenPassiveLearningActive2023,
  title = {Passive Learning of Active Causal Strategies in Agents and Language Models},
  author = {Lampinen, Andrew Kyle and Chan, Stephanie C. Y. and Dasgupta, Ishita and Nam, Andrew J. and Wang, Jane X.},
  date = {2023-05-25},
  eprint = {2305.16183},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.16183},
  urldate = {2023-05-29},
  abstract = {What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from perfectlyconfounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing examples of experimentation, together with explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and may help to understand the behaviors and capabilities of language models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/7LP2SCKD/Lampinen et al. - 2023 - Passive learning of active causal strategies in ag.pdf}
}

@online{LearningDoingGuide,
  title = {Learning by {{Doing}}: A Guide to Teaching and Learning Methods by {{Graham Gibbs}} - {{eBook}} | {{Oxford Brookes University Online Shop}}},
  url = {https://shop.brookes.ac.uk/product-catalogue/oxford-centre-for-staff-learning-development/books-publications/ebooks/learning-by-doing-a-guide-to-teaching-and-learning-methods-by-graham-gibbs-ebook},
  urldate = {2023-10-03},
  file = {/Users/andrew/Zotero/storage/TZ6I9HEA/Learning by Doing a guide to teaching and learnin.pdf;/Users/andrew/Zotero/storage/EMD24KJV/learning-by-doing-a-guide-to-teaching-and-learning-methods-by-graham-gibbs-ebook.html}
}

@online{lehnertAIInsightsTheoretical2023,
  title = {{{AI Insights}} into {{Theoretical Physics}} and the {{Swampland Program}}: {{A Journey Through}} the {{Cosmos}} with {{ChatGPT}}},
  shorttitle = {{{AI Insights}} into {{Theoretical Physics}} and the {{Swampland Program}}},
  author = {Lehnert, Kay},
  date = {2023-01-10},
  eprint = {2301.08155},
  eprinttype = {arxiv},
  eprintclass = {physics},
  doi = {10.48550/arXiv.2301.08155},
  url = {http://arxiv.org/abs/2301.08155},
  urldate = {2023-02-15},
  abstract = {In this case study, we explore the capabilities and limitations of ChatGPT, a natural language processing model developed by OpenAI, in the field of string theoretical swampland conjectures. We find that it is effective at paraphrasing and explaining concepts in a variety of styles, but not at genuinely connecting concepts. It will provide false information with full confidence and make up statements when necessary. However, its ingenious use of language can be fruitful for identifying analogies and describing visual representations of abstract concepts.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Physics - Popular Physics},
  file = {/Users/andrew/Zotero/storage/JYCW4CH3/Lehnert_2023_AI Insights into Theoretical Physics and the Swampland Program.pdf;/Users/andrew/Zotero/storage/BD79CY89/2301.html}
}

@online{lewSequentialMonteCarlo2023,
  title = {Sequential {{Monte Carlo Steering}} of {{Large Language Models}} Using {{Probabilistic Programs}}},
  author = {Lew, Alexander K. and Zhi-Xuan, Tan and Grand, Gabriel and Mansinghka, Vikash K.},
  date = {2023-06-05},
  eprint = {2306.03081},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2306.03081},
  url = {http://arxiv.org/abs/2306.03081},
  urldate = {2023-06-25},
  abstract = {Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone. We propose a new inference-time approach to enforcing syntactic and semantic constraints on the outputs of LLMs, called sequential Monte Carlo (SMC) steering. The key idea is to specify language generation tasks as posterior inference problems in a class of discrete probabilistic sequence models, and replace standard decoding with sequential Monte Carlo inference. For a computational cost similar to that of beam search, SMC can steer LLMs to solve diverse tasks, including infilling, generation under syntactic constraints, and prompt intersection. To facilitate experimentation with SMC steering, we present a probabilistic programming library, LLaMPPL (https://github.com/probcomp/LLaMPPL), for concisely specifying new generation tasks as language model probabilistic programs, and automating steering of LLaMA-family Transformers.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Programming Languages,Statistics - Computation},
  file = {/Users/andrew/Zotero/storage/YARLLDCM/Lew et al_2023_Sequential Monte Carlo Steering of Large Language Models using Probabilistic.pdf;/Users/andrew/Zotero/storage/NIJGMUCW/2306.html}
}

@online{liEmergentWorldRepresentations2023,
  title = {Emergent {{World Representations}}: {{Exploring}} a {{Sequence Model Trained}} on a {{Synthetic Task}}},
  shorttitle = {Emergent {{World Representations}}},
  author = {Li, Kenneth and Hopkins, Aspen K. and Bau, David and Viégas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  date = {2023-01-25},
  eprint = {2210.13382},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.13382},
  urldate = {2023-02-15},
  abstract = {Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create "latent saliency maps" that can help explain predictions in human terms.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/5ZTLQG5D/Li et al_2023_Emergent World Representations.pdf;/Users/andrew/Zotero/storage/K77V6GLW/2210.html}
}

@article{limburgDiesemDiskussionspapierWerden,
  title = {In diesem Diskussionspapier werden zehn Thesen zur Zukunft des wissenschaftlichen Schreibens vorgestellt, mit denen für die Tragweite der KI-induzierten Disruption und der damit einherge- henden Veränderungen des Schreibens sensibilisiert werden soll. Die Thesen sind das Resultat eines kollaborativen Denk- und Schreibprozesses aller Autor:innen.},
  author = {Limburg, Anika and Knorr, Dagmar},
  langid = {ngerman},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/ACNU2RM5/Limburg and Knorr - In diesem Diskussionspapier werden zehn Thesen zur.pdf}
}

@online{linTeachingModelsExpress2022,
  title = {Teaching {{Models}} to {{Express Their Uncertainty}} in {{Words}}},
  author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  date = {2022-06-13},
  eprint = {2205.14334},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.14334},
  url = {http://arxiv.org/abs/2205.14334},
  urldate = {2023-07-24},
  abstract = {We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. "90\% confidence" or "high confidence"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words ("verbalized probability") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/Q654QAMU/Lin et al_2022_Teaching Models to Express Their Uncertainty in Words.pdf;/Users/andrew/Zotero/storage/AVZC9KYD/2205.html}
}

@online{liTeachLLMsPersonalize2023,
  title = {Teach {{LLMs}} to {{Personalize}} -- {{An Approach}} Inspired by {{Writing Education}}},
  author = {Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Wang, Yaqing and Hombaiah, Spurthi Amba and Liang, Yi and Bendersky, Michael},
  date = {2023-08-15},
  eprint = {2308.07968},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.07968},
  url = {http://arxiv.org/abs/2308.07968},
  urldate = {2023-09-07},
  abstract = {Personalized text generation is an emerging research area that has attracted much attention in recent years. Most studies in this direction focus on a particular domain by designing bespoke features or models. In this work, we propose a general approach for personalized text generation using large language models (LLMs). Inspired by the practice of writing education, we develop a multistage and multitask framework to teach LLMs for personalized generation. In writing instruction, the task of writing from sources is often decomposed into multiple steps that involve finding, evaluating, summarizing, synthesizing, and integrating information. Analogously, our approach to personalized text generation consists of multiple stages: retrieval, ranking, summarization, synthesis, and generation. In addition, we introduce a multitask setting that helps the model improve its generation ability further, which is inspired by the observation in education that a student's reading proficiency and writing ability are often correlated. We evaluate our approach on three public datasets, each of which covers a different and representative domain. Our results show significant improvements over a variety of baselines.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/NWF4XDHE/Li et al_2023_Teach LLMs to Personalize -- An Approach inspired by Writing Education.pdf;/Users/andrew/Zotero/storage/MUH6VAXN/2308.html}
}

@online{liuLLMEmpoweringLarge2023,
  title = {{{LLM}}+{{P}}: {{Empowering Large Language Models}} with {{Optimal Planning Proficiency}}},
  shorttitle = {{{LLM}}+{{P}}},
  author = {Liu, Bo and Jiang, Yuqian and Zhang, Xiaohan and Liu, Qiang and Zhang, Shiqi and Biswas, Joydeep and Stone, Peter},
  date = {2023-04-22},
  eprint = {2304.11477},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.11477},
  url = {http://arxiv.org/abs/2304.11477},
  urldate = {2023-04-28},
  abstract = {Large language models (LLMs) have demonstrated remarkable zero-shot generalization abilities: state-of-the-art chatbots can provide plausible answers to many common questions that arise in daily life. However, so far, LLMs cannot reliably solve long-horizon planning problems. By contrast, classical planners, once a problem is given in a formatted way, can use efficient search algorithms to quickly identify correct, or even optimal, plans. In an effort to get the best of both worlds, this paper introduces LLM+P, the first framework that incorporates the strengths of classical planners into LLMs. LLM+P takes in a natural language description of a planning problem, then returns a correct (or optimal) plan for solving that problem in natural language. LLM+P does so by first converting the language description into a file written in the planning domain definition language (PDDL), then leveraging classical planners to quickly find a solution, and then translating the found solution back into natural language. Along with LLM+P, we define a diverse set of different benchmark problems taken from common planning scenarios. Via a comprehensive set of experiments on these benchmark problems, we find that LLM+P is able to provide optimal solutions for most problems, while LLMs fail to provide even feasible plans for most problems.\textbackslash footnote\{The code and results are publicly available at https://github.com/Cranial-XIX/llm-pddl.git.\vphantom\}},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/andrew/Zotero/storage/6JKMPU69/Liu et al_2023_LLM+P.pdf;/Users/andrew/Zotero/storage/HYEFMIMG/2304.html}
}

@online{liuLostMiddleHow2023,
  title = {Lost in the {{Middle}}: {{How Language Models Use Long Contexts}}},
  shorttitle = {Lost in the {{Middle}}},
  author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  date = {2023-07-31},
  eprint = {2307.03172},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.03172},
  url = {http://arxiv.org/abs/2307.03172},
  urldate = {2023-09-03},
  abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/JFI3IEWY/Liu et al_2023_Lost in the Middle.pdf;/Users/andrew/Zotero/storage/MLT2T66L/2307.html}
}

@online{liuTrustworthyLLMsSurvey2023,
  title = {Trustworthy {{LLMs}}: A {{Survey}} and {{Guideline}} for {{Evaluating Large Language Models}}' {{Alignment}}},
  shorttitle = {Trustworthy {{LLMs}}},
  author = {Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and Zhang, Xiaoying and Guo, Ruocheng and Cheng, Hao and Klochkov, Yegor and Taufiq, Muhammad Faaiz and Li, Hang},
  date = {2023-08-10},
  eprint = {2308.05374},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.05374},
  url = {http://arxiv.org/abs/2308.05374},
  urldate = {2023-10-15},
  abstract = {Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/QN7ZQNND/Liu et al_2023_Trustworthy LLMs.pdf;/Users/andrew/Zotero/storage/AZ2KCG7W/2308.html}
}

@online{liuWeReAfraid2023,
  title = {We're {{Afraid Language Models Aren}}'t {{Modeling Ambiguity}}},
  author = {Liu, Alisa and Wu, Zhaofeng and Michael, Julian and Suhr, Alane and West, Peter and Koller, Alexander and Swayamdipta, Swabha and Smith, Noah A. and Choi, Yejin},
  date = {2023-04-27},
  eprint = {2304.14399},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.14399},
  url = {http://arxiv.org/abs/2304.14399},
  urldate = {2023-04-28},
  abstract = {Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models (LMs) are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We characterize ambiguity in a sentence by its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for the recent GPT-4, whose generated disambiguations are considered correct only 32\% of the time in human evaluation, compared to 90\% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/CKKEQ77I/Liu et al_2023_We're Afraid Language Models Aren't Modeling Ambiguity.pdf;/Users/andrew/Zotero/storage/442HT3FD/2304.html}
}

@article{luckinEmpoweringEducatorsBe2022,
  title = {Empowering Educators to Be {{AI-ready}}},
  author = {Luckin, Rosemary and Cukurova, Mutlu and Kent, Carmel and family=Boulay, given=Benedict, prefix=du, useprefix=true},
  date = {2022-01-01},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {3},
  pages = {100076},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2022.100076},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000315},
  urldate = {2023-04-26},
  abstract = {In this paper, we present the concept of AI Readiness, along with a framework for developing AI Readiness training. ‘AI Readiness’ can be framed as a contextualised way of helping people to understand AI, in particular, data-driven AI. The nature of AI Readiness training is not the same as merely learning about AI. Rather, AI Readiness recognises the diversity of the professions, workplaces and sectors for whom AI has a potential impact. For example, AI Readiness for lawyers may be based on the same principles as AI Readiness for Educators. However, the details will be contextualised differently. AI Readiness recognises that such contextualisation is not an option: it is essential due to the multiple intricacies, sensitivities and variations between different sectors and their settings, which all impact the application of AI. To embrace such contextualisation, AI Readiness needs to be an active, participatory training process and aims to empower people to be more able to leverage AI to meet their needs. The text that follows focuses on AI Readiness within the Education and Training sector and starts with a discussion of the current state of AI within education and training, and the need for AI Readiness. We then problematize the concept of AI Readiness, why AI Readiness is needed, and what it means. We expand upon the nature of AI Readiness through a discussion of the difference between human and Artificial Intelligence, before presenting a 7-step framework for helping people to become AI Ready. Finally, we use an example of AI Readiness in action within Higher Education to exemplify AI Readiness.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/L2RNJKXY/Luckin et al. - 2022 - Empowering educators to be AI-ready.pdf}
}

@online{mahowaldDissociatingLanguageThought2023a,
  title = {Dissociating Language and Thought in Large Language Models: A Cognitive Perspective},
  shorttitle = {Dissociating Language and Thought in Large Language Models},
  author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
  date = {2023-01-16},
  eprint = {2301.06627},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.06627},
  url = {http://arxiv.org/abs/2301.06627},
  urldate = {2023-02-12},
  abstract = {Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- "thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/BGLP7GQM/Mahowald et al_2023_Dissociating language and thought in large language models.pdf;/Users/andrew/Zotero/storage/Q5CBAEVF/2301.html}
}

@article{markauskaiteRethinkingEntwinementArtificial2022,
  title = {Rethinking the Entwinement between Artificial Intelligence and Human Learning: {{What}} Capabilities Do Learners Need for a World with {{AI}}?},
  shorttitle = {Rethinking the Entwinement between Artificial Intelligence and Human Learning},
  author = {Markauskaite, Lina and Marrone, Rebecca and Poquet, Oleksandra and Knight, Simon and Martinez-Maldonado, Roberto and Howard, Sarah and Tondeur, Jo and De Laat, Maarten and Buckingham Shum, Simon and Gašević, Dragan and Siemens, George},
  date = {2022},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {3},
  pages = {100056},
  issn = {2666920X},
  doi = {10.1016/j.caeai.2022.100056},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2666920X2200011X},
  urldate = {2023-04-13},
  abstract = {The proliferation of AI in many aspects of human life—from personal leisure, to collaborative professional work, to global policy decisions—poses a sharp question about how to prepare people for an interconnected, fastchanging world which is increasingly becoming saturated with technological devices and agentic machines. What kinds of capabilities do people need in a world infused with AI? How can we conceptualise these capa­ bilities? How can we help learners develop them? How can we empirically study and assess their development? With this paper, we open the discussion by adopting a dialogical knowledge-making approach. Our team of 11 co-authors participated in an orchestrated written discussion. Engaging in a semi-independent and semi-joint written polylogue, we assembled a pool of ideas of what these capabilities are and how learners could be hel­ ped to develop them. Simultaneously, we discussed conceptual and methodological ideas that would enable us to test and refine our hypothetical views. In synthesising these ideas, we propose that there is a need to move beyond AI-centred views of capabilities and consider the ecology of technology, cognition, social interaction, and values.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/HMPQNLW7/Markauskaite et al. - 2022 - Rethinking the entwinement between artificial inte.pdf}
}

@article{markauskaiteRethinkingEntwinementArtificial2022a,
  title = {Rethinking the Entwinement between Artificial Intelligence and Human Learning: {{What}} Capabilities Do Learners Need for a World with {{AI}}?},
  shorttitle = {Rethinking the Entwinement between Artificial Intelligence and Human Learning},
  author = {Markauskaite, Lina and Marrone, Rebecca and Poquet, Oleksandra and Knight, Simon and Martinez-Maldonado, Roberto and Howard, Sarah and Tondeur, Jo and De Laat, Maarten and Buckingham Shum, Simon and Gašević, Dragan and Siemens, George},
  date = {2022-01-01},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {3},
  pages = {100056},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2022.100056},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X2200011X},
  urldate = {2023-04-26},
  abstract = {The proliferation of AI in many aspects of human life—from personal leisure, to collaborative professional work, to global policy decisions—poses a sharp question about how to prepare people for an interconnected, fast-changing world which is increasingly becoming saturated with technological devices and agentic machines. What kinds of capabilities do people need in a world infused with AI? How can we conceptualise these capabilities? How can we help learners develop them? How can we empirically study and assess their development? With this paper, we open the discussion by adopting a dialogical knowledge-making approach. Our team of 11 co-authors participated in an orchestrated written discussion. Engaging in a semi-independent and semi-joint written polylogue, we assembled a pool of ideas of what these capabilities are and how learners could be helped to develop them. Simultaneously, we discussed conceptual and methodological ideas that would enable us to test and refine our hypothetical views. In synthesising these ideas, we propose that there is a need to move beyond AI-centred views of capabilities and consider the ecology of technology, cognition, social interaction, and values.},
  langid = {english},
  keywords = {/unread,AI in education,Capabilities for AI,Ecological approach,Postdigital dialogue},
  file = {/Users/andrew/Zotero/storage/MJH9X3KD/Markauskaite et al_2022_Rethinking the entwinement between artificial intelligence and human learning.pdf;/Users/andrew/Zotero/storage/EX4MSQMH/S2666920X2200011X.html}
}

@online{mccoyEmbersAutoregressionUnderstanding2023,
  title = {Embers of {{Autoregression}}: {{Understanding Large Language Models Through}} the {{Problem They}} Are {{Trained}} to {{Solve}}},
  shorttitle = {Embers of {{Autoregression}}},
  author = {McCoy, R. Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Matthew and Griffiths, Thomas L.},
  date = {2023-09-24},
  eprint = {2309.13638},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.13638},
  url = {http://arxiv.org/abs/2309.13638},
  urldate = {2023-10-02},
  abstract = {The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations. We argue that in order to develop a holistic understanding of these systems we need to consider the problem that they were trained to solve: next-word prediction over Internet text. By recognizing the pressures that this task exerts we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail. This approach - which we call the teleological approach - leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input. We predict that LLMs will achieve higher accuracy when these probabilities are high than when they are low - even in deterministic settings where probability should not matter. To test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we find robust evidence that LLMs are influenced by probability in the ways that we have hypothesized. In many cases, the experiments reveal surprising failure modes. For instance, GPT-4's accuracy at decoding a simple cipher is 51\% when the output is a high-probability word sequence but only 13\% when it is low-probability. These results show that AI practitioners should be careful about using LLMs in low-probability situations. More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system - one that has been shaped by its own particular set of pressures.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/XEUXLQ5P/McCoy et al_2023_Embers of Autoregression.pdf;/Users/andrew/Zotero/storage/XQFYTKF2/2309.html}
}

@online{mitchellDebateUnderstandingAI2023,
  title = {The {{Debate Over Understanding}} in {{AI}}'s {{Large Language Models}}},
  author = {Mitchell, Melanie and Krakauer, David C.},
  date = {2023-02-10},
  eprint = {2210.13966},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.13966},
  url = {http://arxiv.org/abs/2210.13966},
  urldate = {2023-03-29},
  abstract = {We survey a current, heated debate in the AI research community on whether large pre-trained language models can be said to "understand" language -- and the physical and social situations language encodes -- in any important sense. We describe arguments that have been made for and against such understanding, and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that a new science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/WAVN8MYY/Mitchell_Krakauer_2023_The Debate Over Understanding in AI's Large Language Models.pdf;/Users/andrew/Zotero/storage/XAYUWIJI/2210.html}
}

@online{mitchellDetectGPTZeroShotMachineGenerated2023,
  title = {{{DetectGPT}}: {{Zero-Shot Machine-Generated Text Detection}} Using {{Probability Curvature}}},
  shorttitle = {{{DetectGPT}}},
  author = {Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D. and Finn, Chelsea},
  date = {2023-01-26},
  eprint = {2301.11305},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2301.11305},
  urldate = {2023-02-15},
  abstract = {The fluency and factual knowledge of large language models (LLMs) heightens the need for corresponding systems to detect whether a piece of text is machine-written. For example, students may use LLMs to complete written assignments, leaving instructors unable to accurately assess student learning. In this paper, we first demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g, T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/VFZ4ZU2A/Mitchell et al_2023_DetectGPT.pdf;/Users/andrew/Zotero/storage/HJEJ9TSC/2301.html}
}

@online{modrakSimulationBasedCalibrationChecking2022,
  title = {Simulation-{{Based Calibration Checking}} for {{Bayesian Computation}}: {{The Choice}} of {{Test Quantities Shapes Sensitivity}}},
  shorttitle = {Simulation-{{Based Calibration Checking}} for {{Bayesian Computation}}},
  author = {Modrák, Martin and Moon, Angie H. and Kim, Shinyoung and Bürkner, Paul and Huurre, Niko and Faltejsková, Kateřina and Gelman, Andrew and Vehtari, Aki},
  date = {2022-11-04},
  eprint = {2211.02383},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2211.02383},
  url = {http://arxiv.org/abs/2211.02383},
  urldate = {2023-10-09},
  abstract = {Simulation-based calibration checking (SBC) is a practical method to validate computationally-derived posterior distributions or their approximations. In this paper, we introduce a new variant of SBC to alleviate several known problems. Our variant allows the user to in principle detect any possible issue with the posterior, while previously reported implementations could never detect large classes of problems including when the posterior is equal to the prior. This is made possible by including additional data-dependent test quantities when running SBC. We argue and demonstrate that the joint likelihood of the data is an especially useful test quantity. Some other types of test quantities and their theoretical and practical benefits are also investigated. We provide theoretical analysis of SBC, thereby providing a more complete understanding of the underlying statistical mechanisms. We also bring attention to a relatively common mistake in the literature and clarify the difference between SBC and checks based on the data-averaged posterior. We support our recommendations with numerical case studies on a multivariate normal example and a case study in implementing an ordered simplex data type for use with Hamiltonian Monte Carlo. The SBC variant introduced in this paper is implemented in the \$\textbackslash mathtt\{SBC\}\$ R package.},
  pubstate = {preprint},
  version = {1},
  keywords = {Statistics - Methodology},
  file = {/Users/andrew/Zotero/storage/RUTFS9ZE/Modrák et al_2022_Simulation-Based Calibration Checking for Bayesian Computation.pdf;/Users/andrew/Zotero/storage/YEXYXH9P/2211.html}
}

@online{mollickAssigningAISeven2023,
  type = {SSRN Scholarly Paper},
  title = {Assigning {{AI}}: {{Seven Approaches}} for {{Students}}, with {{Prompts}}},
  shorttitle = {Assigning {{AI}}},
  author = {Mollick, Ethan R. and Mollick, Lilach},
  date = {2023-06-12},
  number = {4475995},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.4475995},
  url = {https://papers.ssrn.com/abstract=4475995},
  urldate = {2023-09-14},
  abstract = {This paper examines the transformative role of Large Language Models (LLMs) in education and their potential as learning tools, despite their inherent risks and limitations. The authors propose seven approaches for utilizing AI in classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator, and AI-student, each with distinct pedagogical benefits and risks. The aim is to help students learn with and about AI, with practical strategies designed to mitigate risks such as complacency about the AI’s output, errors, and biases. These strategies promote active oversight, critical assessment of AI outputs, and complementarity of AI's capabilities with the students' unique insights. By challenging students to remain the "human in the loop", the authors aim to enhance learning outcomes while ensuring that AI serves as a supportive tool rather than a replacement. The proposed framework offers a guide for educators navigating the integration of AI-assisted learning in classrooms.},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread,AI,Education,LLM,Prompts},
  file = {/Users/andrew/Zotero/storage/86XYJQ2F/Mollick_Mollick_2023_Assigning AI.pdf}
}

@online{mollickAssigningAISeven2023a,
  title = {Assigning {{AI}}: {{Seven Approaches}} for {{Students}}, with {{Prompts}}},
  shorttitle = {Assigning {{AI}}},
  author = {Mollick, Ethan and Mollick, Lilach},
  date = {2023-06-12},
  eprint = {2306.10052},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2306.10052},
  urldate = {2023-09-14},
  abstract = {This paper examines the transformative role of Large Language Models (LLMs) in education and their potential as learning tools, despite their inherent risks and limitations. The authors propose seven approaches for utilizing AI in classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator, and AI-student, each with distinct pedagogical benefits and risks. The aim is to help students learn with and about AI, with practical strategies designed to mitigate risks such as complacency about the AI's output, errors, and biases. These strategies promote active oversight, critical assessment of AI outputs, and complementarity of AI's capabilities with the students' unique insights. By challenging students to remain the "human in the loop," the authors aim to enhance learning outcomes while ensuring that AI serves as a supportive tool rather than a replacement. The proposed framework offers a guide for educators navigating the integration of AI-assisted learning in classrooms},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {/Users/andrew/Zotero/storage/BNRWUZBX/Mollick_Mollick_2023_Assigning AI.pdf;/Users/andrew/Zotero/storage/ENQN2XLJ/2306.html}
}

@online{mollickNewModesLearning2022a,
  type = {SSRN Scholarly Paper},
  title = {New {{Modes}} of {{Learning Enabled}} by {{AI Chatbots}}: {{Three Methods}} and {{Assignments}}},
  shorttitle = {New {{Modes}} of {{Learning Enabled}} by {{AI Chatbots}}},
  author = {Mollick, Ethan R. and Mollick, Lilach},
  date = {2022-12-13},
  number = {4300783},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.4300783},
  url = {https://papers.ssrn.com/abstract=4300783},
  urldate = {2023-02-16},
  abstract = {Chatbots are able to produce high-quality, sophisticated text in natural language. The authors of this paper believe that AI can be used to overcome three barriers to learning in the classroom: improving transfer, breaking the illusion of explanatory depth, and training students to critically evaluate explanations. The paper provides background information and techniques on how AI can be used to overcome these barriers and includes prompts and assignments that teachers can incorporate into their teaching. The goal is to help teachers use the capabilities and drawbacks of AI to improve learning},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread,AI,chatbot,education,learning,transfer},
  file = {/Users/andrew/Zotero/storage/TI6A8P95/Mollick_Mollick_2022_New Modes of Learning Enabled by AI Chatbots.pdf}
}

@article{mollickUsingAIImplement2023,
  title = {Using {{AI}} to {{Implement Effective Teaching Strategies}} in {{Classrooms}}: {{Five Strategies}}, {{Including Prompts}}},
  shorttitle = {Using {{AI}} to {{Implement Effective Teaching Strategies}} in {{Classrooms}}},
  author = {Mollick, Ethan R. and Mollick, Lilach},
  date = {2023},
  journaltitle = {SSRN Electronic Journal},
  shortjournal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4391243},
  url = {https://www.ssrn.com/abstract=4391243},
  urldate = {2023-05-03},
  abstract = {This paper provides guidance for using AI to quickly and easily implement evidencebased teaching strategies that instructors can integrate into their teaching. We discuss five teaching strategies that have proven value but are hard to implement in practice due to time and effort constraints. We show how AI can help instructors create material that supports these strategies and improve student learning. The strategies include providing multiple examples and explanations; uncovering and addressing student misconceptions; frequent low-stakes testing; assessing student learning; and distributed practice. The paper provides guidelines for how AI can support each strategy, and discusses both the promises and perils of this approach, arguing that AI may act as a “force multiplier” for instructors if implemented cautiously and thoughtfully in service of evidence-based teaching practices.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/ZWU886LV/Mollick and Mollick - 2023 - Using AI to Implement Effective Teaching Strategie.pdf}
}

@online{mollickUsingAIImplement2023a,
  type = {SSRN Scholarly Paper},
  title = {Using {{AI}} to {{Implement Effective Teaching Strategies}} in {{Classrooms}}: {{Five Strategies}}, {{Including Prompts}}},
  shorttitle = {Using {{AI}} to {{Implement Effective Teaching Strategies}} in {{Classrooms}}},
  author = {Mollick, Ethan R. and Mollick, Lilach},
  date = {2023-03-17},
  number = {4391243},
  location = {{Rochester, NY}},
  doi = {10.2139/ssrn.4391243},
  url = {https://papers.ssrn.com/abstract=4391243},
  urldate = {2023-09-07},
  abstract = {This paper provides guidance for using AI to quickly and easily implement evidence-based teaching strategies that instructors can integrate into their teaching. We discuss five teaching strategies that have proven value but are hard to implement in practice due to time and effort constraints. We show how AI can help instructors create material that supports these strategies and improve student learning. The strategies include providing multiple examples and explanations; uncovering and addressing student misconceptions; frequent low-stakes testing; assessing student learning; and distributed practice. The paper provides guidelines for how AI can support each strategy, and discusses both the promises and perils of this approach, arguing that AI may act as a “force multiplier” for instructors if implemented cautiously and thoughtfully in service of evidence-based teaching practices.},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread,AI,ChatGPT,GPT4,Learning},
  file = {/Users/andrew/Zotero/storage/92ZG94XT/Mollick_Mollick_2023_Using AI to Implement Effective Teaching Strategies in Classrooms.pdf}
}

@online{neelakantanTextCodeEmbeddings2022,
  title = {Text and {{Code Embeddings}} by {{Contrastive Pre-Training}}},
  author = {Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris and Heidecke, Johannes and Shyam, Pranav and Power, Boris and Nekoul, Tyna Eloundou and Sastry, Girish and Krueger, Gretchen and Schnurr, David and Such, Felipe Petroski and Hsu, Kenny and Thompson, Madeleine and Khan, Tabarak and Sherbakov, Toki and Jang, Joanne and Welinder, Peter and Weng, Lilian},
  date = {2022-01-24},
  eprint = {2201.10005},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.10005},
  url = {http://arxiv.org/abs/2201.10005},
  urldate = {2023-06-13},
  abstract = {Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4\% and 1.8\% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4\%, 14.7\%, and 10.6\% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8\% relative improvement over prior best work on code search.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/Q6YJLLU4/Neelakantan et al_2022_Text and Code Embeddings by Contrastive Pre-Training.pdf;/Users/andrew/Zotero/storage/Q28HLSDN/2201.html}
}

@book{nerantzi101CreativeIdeas2023,
  title = {101 Creative Ideas to Use {{AI}} in Education, {{A}} Crowdsourced Collection},
  author = {Nerantzi, Chrissi and Abegglen, Sandra and Karatsiori, Marianna and Martínez-Arboleda (Eds.), Antonio},
  date = {2023-07-31},
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.8355454},
  url = {https://zenodo.org/record/8355454},
  urldate = {2023-10-11},
  abstract = {This open crowdsourced collection by \#creativeHE presents a rich tapestry of our collective thinking in the first months of 2023 stitching together potential alternative uses and applications of Artificial Intelligence (AI) that could make a difference and create new learning, development, teaching and assessment opportunities. ~ Experimentation is at the heart of learning, teaching and scholarship. Being open to diverse ideas will help us make novel connections that can lead to new discoveries and insights to make a positive contribution to our world. Ideas shared may be in its embryonic stage, but worth exploring further through active and creative inquiry.~~ We would like to illuminate the importance of responsible, critical and ethical use of AI in education settings and more generally.~ We are grateful for all 101 contributions from 19 countries: Australia, Canada, China, Egypt, Germany, Greece, India, Israel, Italy, Ireland, Jordan, Liberia, Mexico, South Africa, Spain, Thailand, Turkey, United Kingdom and the US. A special thank you to Bushra Hashim for the beautiful design. Suggested citation: Nerantzi, C., Abegglen, S., Karatsiori, M. and Martinez-Arboleda, A. (Eds.) (2023). 101 Creative ideas to use AI in education. A collection curated by \#creativeHE. Graphic Design by Bushra Hashim. CC-BY-NC-SA 4.0. As the collection is made available under the Creative Commons License CC-BY-NC-SA license, anybody can use the collection as open data to further interrogate the use of AI in Education. Please share any resulting outcomes with the editorial team and the wider community.~ ~ The editors ~ “This collection represents vision; it embodies creativity. The importance of perspective and community of practice comes to life here in the breadth of examples demonstrating creative ideas to use AI in education. As we explore how we design new experiences for our learners and differentiate opportunities to engage in new ways, we have an opportunity to push our own boundaries and explore. We can collaborate, radically. This is a collection that will only grow as we shift our own practice and as we allow ourselves to experiment and iterate for a transformational student experience.” Dr Margaret Korosec, Dean of Online and Digital Education, University of Leeds.},
  version = {2023 1.2},
  keywords = {\#creativeHE,AI in Education,Artificial intelligence,Creativity,crowdsourced collection,Education,open book},
  file = {/Users/andrew/Zotero/storage/MG8RRPP8/Nerantzi et al. - 2023 - 101 creative ideas to use AI in education, A crowd.pdf}
}

@online{ouyangTrainingLanguageModels2022b,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  date = {2022-03-04},
  eprint = {2203.02155},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.02155},
  urldate = {2023-02-13},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/FAADUUEG/Ouyang et al_2022_Training language models to follow instructions with human feedback.pdf;/Users/andrew/Zotero/storage/WMCGN5Y8/2203.html}
}

@online{panUnifyingLargeLanguage2023,
  title = {Unifying {{Large Language Models}} and {{Knowledge Graphs}}: {{A Roadmap}}},
  shorttitle = {Unifying {{Large Language Models}} and {{Knowledge Graphs}}},
  author = {Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
  date = {2023-06-20},
  eprint = {2306.08302},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.08302},
  url = {http://arxiv.org/abs/2306.08302},
  urldate = {2023-06-21},
  abstract = {Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/QDSPAGAT/Pan et al_2023_Unifying Large Language Models and Knowledge Graphs.pdf;/Users/andrew/Zotero/storage/APPDNP8E/2306.html}
}

@inproceedings{pardosOATutorOpensourceAdaptive2023,
  title = {{{OATutor}}: {{An Open-source Adaptive Tutoring System}} and {{Curated Content Library}} for {{Learning Sciences Research}}},
  shorttitle = {{{OATutor}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Pardos, Zachary A. and Tang, Matthew and Anastasopoulos, Ioannis and Sheel, Shreya K. and Zhang, Ethan},
  date = {2023-04-19},
  series = {{{CHI}} '23},
  pages = {1--17},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581574},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581574},
  urldate = {2023-04-27},
  abstract = {Despite decades long establishment of effective tutoring principles, no adaptive tutoring system has been developed and open-sourced to the research community. The absence of such a system inhibits researchers from replicating adaptive learning studies and extending and experimenting with various tutoring system design directions. For this reason, adaptive learning research is primarily conducted on a small number of proprietary platforms. In this work, we aim to democratize adaptive learning research with the introduction of the first open-source adaptive tutoring system based on Intelligent Tutoring System principles. The system, we call Open Adaptive Tutor (OATutor), has been iteratively developed over three years with field trials in classrooms drawing feedback from students, teachers, and researchers. The MIT-licensed source code includes three creative commons (CC BY) textbooks worth of algebra problems, with tutoring supports authored by the OATutor project. Knowledge Tracing, an A/B testing framework, and LTI support are included.},
  isbn = {978-1-4503-9421-5},
  keywords = {/unread,Adaptive learning,content authoring,intelligent tutoring systems,OER,open source,replicable research,research through design},
  file = {/Users/andrew/Zotero/storage/CXRWYD97/Pardos et al_2023_OATutor.pdf}
}

@article{pattersonCarbonEmissionsLarge2022,
  title = {Carbon {{Emissions}} and {{Large Neural Network Training}}},
  author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  date = {2022},
  abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models—T5, Meena, GShard, Switch Transformer, and GPT-3—and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): ● Large but sparsely activated DNNs can consume {$<$}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. ● Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary \textasciitilde 5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. ● Specific datacenter infrastructure matters, as Cloud datacenters can be \textasciitilde 1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be \textasciitilde 2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to \textasciitilde 100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/9HE5ZBU8/Patterson et al. - Carbon Emissions and Large Neural Network Training.pdf}
}

@article{peachUnderstandingLearnerBehaviour2021,
  title = {Understanding Learner Behaviour in Online Courses with {{Bayesian}} Modelling and Time Series Characterisation},
  author = {Peach, Robert L. and Greenbury, Sam F. and Johnston, Iain G. and Yaliraki, Sophia N. and Lefevre, David J. and Barahona, Mauricio},
  date = {2021-02-02},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {2823},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-81709-3},
  url = {https://www.nature.com/articles/s41598-021-81709-3},
  urldate = {2023-05-24},
  abstract = {The intrinsic temporality of learning demands the adoption of methodologies capable of exploiting time-series information. In this study we leverage the sequence data framework and show how data-driven analysis of temporal sequences of task completion in online courses can be used to characterise personal and group learners’ behaviors, and to identify critical tasks and course sessions in a given course design. We also introduce a recently developed probabilistic Bayesian model to learn sequential behaviours of students and predict student performance. The application of our data-driven sequence-based analyses to data from learners undertaking an on-line Business Management course reveals distinct behaviors within the cohort of learners, identifying learners or groups of learners that deviate from the nominal order expected in the course. Using course grades a posteriori, we explore differences in behavior between high and low performing learners. We find that high performing learners follow the progression between weekly sessions more regularly than low performing learners, yet within each weekly session high performing learners are less tied to the nominal task order. We then model the sequences of high and low performance students using the probablistic Bayesian model and show that we can learn engagement behaviors associated with performance. We also show that the data sequence framework can be used for task-centric analysis; we identify critical junctures and differences among types of tasks within the course design. We find that non-rote learning tasks, such as interactive tasks or discussion posts, are correlated with higher performance. We discuss the application of such analytical techniques as an aid to course design, intervention, and student supervision.},
  issue = {1},
  langid = {english},
  keywords = {/unread,Human behaviour,Information technology},
  file = {/Users/andrew/Zotero/storage/T3ECNUX2/Peach et al_2021_Understanding learner behaviour in online courses with Bayesian modelling and.pdf}
}

@article{piantadosiModernLanguageModels,
  title = {Modern Language Models Refute {{Chomsky}}’s Approach to Language},
  author = {Piantadosi, Steven T},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/ER5CQ64P/Piantadosi - Modern language models refute Chomsky’s approach t.pdf}
}

@online{piantasodiMeaningReferenceLarge2022,
  title = {Meaning without Reference in Large Language Models},
  author = {Piantasodi, Steven T. and Hill, Felix},
  date = {2022-08-04},
  eprint = {2208.02957},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.02957},
  url = {http://arxiv.org/abs/2208.02957},
  urldate = {2022-08-09},
  abstract = {The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/ZWYCJ33Q/Piantasodi and Hill - 2022 - Meaning without reference in large language models.pdf;/Users/andrew/Zotero/storage/F9JAXQ23/2208.html}
}

@online{piechDeepKnowledgeTracing2015b,
  title = {Deep {{Knowledge Tracing}}},
  author = {Piech, Chris and Spencer, Jonathan and Huang, Jonathan and Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas and Sohl-Dickstein, Jascha},
  date = {2015-06-19},
  eprint = {1506.05908},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1506.05908},
  url = {http://arxiv.org/abs/1506.05908},
  urldate = {2023-02-16},
  abstract = {Knowledge tracing---where a machine models the knowledge of a student as they interact with coursework---is a well established problem in computer supported education. Though effectively modeling student knowledge would have high educational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neural networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and discovery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,K.3.1},
  file = {/Users/andrew/Zotero/storage/76MLAPTN/Piech et al_2015_Deep Knowledge Tracing.pdf;/Users/andrew/Zotero/storage/8LGWIZUL/1506.html}
}

@article{pintrichRoleMetacognitiveKnowledge2002,
  title = {The {{Role}} of {{Metacognitive Knowledge}} in {{Learning}}, {{Teaching}}, and {{Assessing}}},
  author = {Pintrich, Paul R.},
  date = {2002-11-01},
  journaltitle = {Theory Into Practice},
  shortjournal = {Theory Into Practice},
  volume = {41},
  number = {4},
  pages = {219--225},
  issn = {0040-5841, 1543-0421},
  doi = {10.1207/s15430421tip4104_3},
  url = {https://www.tandfonline.com/doi/full/10.1207/s15430421tip4104_3},
  urldate = {2023-05-05},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/LJXIFAG5/Pintrich - 2002 - The Role of Metacognitive Knowledge in Learning, T.pdf;/Users/andrew/Zotero/storage/PQIEADTF/Pintrich_2002_The Role of Metacognitive Knowledge in Learning, Teaching, and Assessing.pdf}
}

@online{princeUnderstandingDeepLearning,
  title = {Understanding {{Deep Learning}}},
  author = {Prince, Simon},
  url = {https://udlbook.github.io/udlbook/},
  urldate = {2023-08-02},
  organization = {{Understanding Deep Learning}},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/KSUS8QU3/udlbook.html}
}

@online{prystawskiPsychologicallyinformedChainofthoughtPrompts2023,
  title = {Psychologically-Informed Chain-of-Thought Prompts for Metaphor Understanding in Large Language Models},
  author = {Prystawski, Ben and Thibodeau, Paul and Potts, Christopher and Goodman, Noah D.},
  date = {2023-05-19},
  eprint = {2209.08141},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.08141},
  url = {http://arxiv.org/abs/2209.08141},
  urldate = {2023-10-15},
  abstract = {Probabilistic models of language understanding are valuable tools for investigating human language use. However, they need to be hand-designed for a particular domain. In contrast, large language models (LLMs) are trained on text that spans a wide array of domains, but they lack the structure and interpretability of probabilistic models. In this paper, we use chain-of-thought prompts to introduce structures from probabilistic models into LLMs. We explore this approach in the case of metaphor understanding. Our chain-of-thought prompts lead language models to infer latent variables and reason about their relationships in order to choose appropriate paraphrases for metaphors. The latent variables and relationships chosen are informed by theories of metaphor understanding from cognitive psychology. We apply these prompts to the two largest versions of GPT-3 and show that they can improve performance in a paraphrase selection task.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/S72N4BVP/Prystawski et al_2023_Psychologically-informed chain-of-thought prompts for metaphor understanding in.pdf;/Users/andrew/Zotero/storage/PA6WE8WY/2209.html}
}

@online{qinChatGPTGeneralPurposeNatural2023,
  title = {Is {{ChatGPT}} a {{General-Purpose Natural Language Processing Task Solver}}?},
  author = {Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  date = {2023-02-15},
  eprint = {2302.06476},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.06476},
  urldate = {2023-02-22},
  abstract = {Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/QJE4EGUH/Qin et al_2023_Is ChatGPT a General-Purpose Natural Language Processing Task Solver.pdf;/Users/andrew/Zotero/storage/BQF7N8SL/2302.html}
}

@article{radfordImprovingLanguageUnderstanding,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/ZBTH9HXC/Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@online{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  date = {2021-02-26},
  eprint = {2102.12092},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2102.12092},
  url = {http://arxiv.org/abs/2102.12092},
  urldate = {2023-06-13},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/WTFS2ZWC/Ramesh et al_2021_Zero-Shot Text-to-Image Generation.pdf;/Users/andrew/Zotero/storage/Z75SNWHC/2102.html}
}

@online{ramInContextRetrievalAugmentedLanguage2023,
  title = {In-{{Context Retrieval-Augmented Language Models}}},
  author = {Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
  date = {2023-01-31},
  eprint = {2302.00083},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.00083},
  url = {http://arxiv.org/abs/2302.00083},
  urldate = {2023-06-17},
  abstract = {Retrieval-Augmented Language Modeling (RALM) methods, that condition a language model (LM) on relevant documents from a grounding corpus during generation, have been shown to significantly improve language modeling while also providing a natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper proposes an under-explored alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input. We show that in-context RALM which uses off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that in-context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access. To that end, we make our code publicly available.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/andrew/Zotero/storage/SZGZJ74F/Ram et al_2023_In-Context Retrieval-Augmented Language Models.pdf;/Users/andrew/Zotero/storage/K9TGTTYD/2302.html}
}

@online{razeghiImpactPretrainingTerm2022,
  title = {Impact of {{Pretraining Term Frequencies}} on {{Few-Shot Reasoning}}},
  author = {Razeghi, Yasaman and Logan IV, Robert L. and Gardner, Matt and Singh, Sameer},
  date = {2022-05-23},
  eprint = {2202.07206},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.07206},
  urldate = {2023-02-11},
  abstract = {Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above \$70\textbackslash\%\$ (absolute) more accurate on the top 10\textbackslash\% frequent terms in comparison to the bottom 10\textbackslash\%. Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/HH5T9WLG/Razeghi et al_2022_Impact of Pretraining Term Frequencies on Few-Shot Reasoning.pdf;/Users/andrew/Zotero/storage/UQQ78JRW/2202.html}
}

@online{razeghiImpactPretrainingTerm2022a,
  title = {Impact of {{Pretraining Term Frequencies}} on {{Few-Shot Reasoning}}},
  author = {Razeghi, Yasaman and Logan IV, Robert L. and Gardner, Matt and Singh, Sameer},
  date = {2022-05-23},
  eprint = {2202.07206},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.07206},
  url = {http://arxiv.org/abs/2202.07206},
  urldate = {2023-10-02},
  abstract = {Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above \$70\textbackslash\%\$ (absolute) more accurate on the top 10\textbackslash\% frequent terms in comparison to the bottom 10\textbackslash\%. Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/NHK92KRE/Razeghi et al_2022_Impact of Pretraining Term Frequencies on Few-Shot Reasoning.pdf;/Users/andrew/Zotero/storage/TTCMRAA4/2202.html}
}

@article{rogersPrimerBERTologyWhat2020,
  title = {A {{Primer}} in {{BERTology}}: {{What We Know About How BERT Works}}},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  date = {2020},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {842--866},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA}},
  doi = {10.1162/tacl_a_00349},
  url = {https://aclanthology.org/2020.tacl-1.54},
  urldate = {2023-03-15},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/2IFRFAD2/Rogers et al_2020_A Primer in BERTology.pdf}
}

@article{rohrerThatLotProcess2022,
  title = {That’s a {{Lot}} to {{Process}}! {{Pitfalls}} of {{Popular Path Models}}},
  author = {Rohrer, Julia M. and Hünermund, Paul and Arslan, Ruben C. and Elson, Malte},
  date = {2022-04-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {5},
  number = {2},
  pages = {25152459221095827},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/25152459221095827},
  url = {https://doi.org/10.1177/25152459221095827},
  urldate = {2023-05-29},
  abstract = {Path models to test claims about mediation and moderation are a staple of psychology. But applied researchers may sometimes not understand the underlying causal inference problems and thus endorse conclusions that rest on unrealistic assumptions. In this article, we aim to provide a clear explanation for the limited conditions under which standard procedures for mediation and moderation analysis can succeed. We discuss why reversing arrows or comparing model fit indices cannot tell us which model is the right one and how tests of conditional independence can at least tell us where our model goes wrong. Causal modeling practices in psychology are far from optimal but may be kept alive by domain norms that demand every article makes some novel claim about processes and boundary conditions. We end with a vision for a different research culture in which causal inference is pursued in a much slower, more deliberate, and collaborative manner.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/XPHMPG4C/Rohrer et al_2022_That’s a Lot to Process.pdf}
}

@online{rooijReclaimingAITheoretical2023,
  title = {Reclaiming {{AI}} as a Theoretical Tool for Cognitive Science},
  author = {family=Rooij, given=Iris, prefix=van, useprefix=false and Guest, Olivia and Adolfi, Federico G. and family=Haan, given=Ronald, prefix=de, useprefix=false and Kolokolova, Antonina and Rich, Patricia},
  date = {2023-08-01T20:26:54},
  doi = {10.31234/osf.io/4cbuv},
  url = {https://psyarxiv.com/4cbuv/},
  urldate = {2023-09-02},
  abstract = {The idea that human cognition is, or can be understood as, a form of computation is a useful conceptual tool for cognitive science. It was a foundational assumption during the birth of cognitive science as a multidisciplinary field, with Artificial Intelligence (AI) as one of its contributing fields. One conception of AI in this context is as a provider of computational tools (frameworks, concepts, formalisms, models, proofs, simulations, etc.) that support theory building in cognitive science. The contemporary field of AI, however, has taken the theoretical possibility of explaining human cognition as a form of computation to imply the practical feasibility of realising human(-like or -level) cognition in factual computational systems; and, the field frames this realisation as a short-term inevitability. Yet, as we formally prove herein, creating systems with human(-like or -level) cognition is intrinsically computationally intractable. This means that any factual AI systems created in the short-run are at best decoys. When we think these systems capture something deep about ourselves and our thinking, we induce distorted and impoverished images of ourselves and our cognition. In other words, AI in current practice is deteriorating our theoretical understanding of cognition rather than advancing and enhancing it. The situation could be remediated by releasing the grip of the currently dominant view on AI and by returning to the idea of AI as a theoretical tool for cognitive science. In reclaiming this older idea of AI, however, it is important not to repeat conceptual mistakes of the past (and present) that brought us to where we are today.},
  langid = {american},
  pubstate = {preprint},
  keywords = {/unread,artificial intelligence (AI),Cognitive Psychology,cognitive science,computational complexity,engineering,explanation,Meta-science,Social and Behavioral Sciences,theory,Theory and Philosophy of Science},
  file = {/Users/andrew/Zotero/storage/9D3ZEXVK/Rooij et al_2023_Reclaiming AI as a theoretical tool for cognitive science.pdf}
}

@report{saldenDidaktischeUndRechtliche2023,
  title = {Didaktische und rechtliche Perspektiven auf KI-gestütztes Schreiben in der Hochschulbildung},
  author = {Salden, Peter},
  editora = {Leschke, Jonas},
  editoratype = {collaborator},
  date = {2023},
  pages = {1119 KB, 41 pages},
  institution = {{Ruhr-Universität Bochum}},
  doi = {10.13154/294-9734},
  url = {https://hss-opus.ub.ruhr-uni-bochum.de/opus4/9734},
  urldate = {2023-06-14},
  langid = {ngerman},
  keywords = {/unread,{370 Erziehung, Schul- und Bildungswesen}},
  file = {/Users/andrew/Zotero/storage/TBSUFMBP/Salden, Peter - 2023 - Didaktische und rechtliche Perspektiven auf KI-ges.pdf}
}

@online{schaefferAreEmergentAbilities2023,
  title = {Are {{Emergent Abilities}} of {{Large Language Models}} a {{Mirage}}?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  date = {2023-04-28},
  eprint = {2304.15004},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.15004},
  url = {http://arxiv.org/abs/2304.15004},
  urldate = {2023-05-01},
  abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not. Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale. We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how similar metric decisions suggest apparent emergent abilities on vision tasks in diverse deep network architectures (convolutional, autoencoder, transformers). In all three analyses, we find strong supporting evidence that emergent abilities may not be a fundamental property of scaling AI models.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence},
  file = {/Users/andrew/Zotero/storage/YI3D3JAV/Schaeffer et al_2023_Are Emergent Abilities of Large Language Models a Mirage.pdf;/Users/andrew/Zotero/storage/C9ACQRBT/2304.html}
}

@article{shahExplainableKnowledgeTracing,
  title = {Explainable {{Knowledge Tracing Models}} for {{Big Data}}: {{Is Ensembling}} an {{Answer}}?},
  author = {Shah, Tirth and Sharma, Aditya and Olson, Lukas and Patel, Nirmal},
  abstract = {In this paper, we describe our Knowledge Tracing model for the 2020 NeurIPS Education Challenge. We used a combination of 22 models to predict whether the students will answer a given question correctly or not. Our combination of different approaches allowed us to get an accuracy higher than any of the individual models, and the variation of our model types gave our solution better explainability, more alignment with learning science theories, and high predictive power.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/MZ3WIFI2/Shah et al. - Explainable Knowledge Tracing Models for Big Data.pdf}
}

@online{shanahanRolePlayLargeLanguage2023,
  title = {Role-{{Play}} with {{Large Language Models}}},
  author = {Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
  date = {2023-05-25},
  eprint = {2305.16367},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.16367},
  url = {http://arxiv.org/abs/2305.16367},
  urldate = {2023-05-29},
  abstract = {As dialogue agents become increasingly human-like in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. In this paper, we foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. Two important cases of dialogue agent behaviour are addressed this way, namely (apparent) deception and (apparent) self-awareness.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/PN6NU6L7/Shanahan et al_2023_Role-Play with Large Language Models.pdf;/Users/andrew/Zotero/storage/YTHKAW75/2305.html}
}

@online{shanahanTalkingLargeLanguage2023,
  title = {Talking {{About Large Language Models}}},
  author = {Shanahan, Murray},
  date = {2023-01-25},
  eprint = {2212.03551},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.03551},
  url = {http://arxiv.org/abs/2212.03551},
  urldate = {2023-02-11},
  abstract = {Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as "knows", "believes", and "thinks", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/EK8WVTU2/Shanahan_2023_Talking About Large Language Models.pdf;/Users/andrew/Zotero/storage/TDJQ7QT9/2212.html}
}

@unpublished{spannagelRulesTools2023,
  title = {Rules for {{Tools}}},
  author = {Spannagel, Christian},
  date = {2023-03},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/SXNMPYKA/Rules for Tools.pdf}
}

@book{steinsMythenFehlvorstellungenFehlkonzepte2022,
  title = {Mythen, Fehlvorstellungen, Fehlkonzepte und Irrtümer in Schule und Unterricht},
  editor = {Steins, Gisela and Spinath, Birgit and Dutke, Stephan and Roth, Marcus and Limbourg, Maria},
  date = {2022},
  series = {Psychologie in Bildung und Erziehung: Vom Wissen zum Handeln},
  publisher = {{Springer Fachmedien}},
  location = {{Wiesbaden}},
  doi = {10.1007/978-3-658-36260-7},
  url = {https://link.springer.com/10.1007/978-3-658-36260-7},
  urldate = {2023-08-11},
  isbn = {978-3-658-36259-1 978-3-658-36260-7},
  langid = {ngerman},
  keywords = {/unread,Bugs,Fehlvorstellungen,Irrtümer,Lehrberuf,Lerntypen,Mythen,Neuromythen},
  file = {/Users/andrew/Zotero/storage/HS25SIGG/Steins et al_2022_Mythen, Fehlvorstellungen, Fehlkonzepte und Irrtümer in Schule und Unterricht.pdf}
}

@inproceedings{strubellEnergyPolicyConsiderations2019,
  title = {Energy and {{Policy Considerations}} for {{Deep Learning}} in {{NLP}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  date = {2019-07},
  pages = {3645--3650},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1355},
  url = {https://aclanthology.org/P19-1355},
  urldate = {2023-10-25},
  abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
  eventtitle = {{{ACL}} 2019},
  file = {/Users/andrew/Zotero/storage/V5IR3G4D/Strubell et al_2019_Energy and Policy Considerations for Deep Learning in NLP.pdf}
}

@report{sumersReconcilingTruthfulnessRelevance2022,
  type = {preprint},
  title = {Reconciling Truthfulness and Relevance as Epistemic and Decision-Theoretic Utility},
  author = {Sumers, Theodore and Ho, Mark K and Griffiths, Thomas L. and Hawkins, Robert},
  date = {2022-10-02},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/e9m3j},
  url = {https://osf.io/e9m3j},
  urldate = {2023-10-15},
  abstract = {People use language to influence others’ beliefs and actions. Yet models of communication have diverged along these lines, formalizing the speaker’s objective in terms of either the listener’s beliefs or actions. We argue that this divergence lies at the root of a longstanding controversy over the Gricean maxims of truthfulness and relevance. We first bridge the divide by introducing a speaker model which considers both the listener’s beliefs (epistemic utility) and their actions (decision-theoretic utility). We show that formalizing truthfulness as an epistemic utility and relevance as a decision-theoretic utility reconciles the tension between them, readily explaining puzzles such as context-dependent standards of truthfulness. We then test a set of novel predictions generated by our combined model. We introduce a new signaling game which decouples utterances’ truthfulness and relevance, then use it to conduct a pair of experiments. Our first experiment demonstrates that participants jointly maximize epistemic and decision-theoretic utility, rather than either alone. Our second experiment shows that when the two conflict, participants make a graded tradeo rather than prioritizing one over the other. These results demonstrate that human communication cannot be reduced to influencing beliefs or actions alone. Taken together, our work provides a new foundation for grounding rational communication not only in what we believe, but in what those beliefs lead us to do.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/M34U4F5M/Sumers et al. - 2022 - Reconciling truthfulness and relevance as epistemi.pdf}
}

@online{suttonBitterLesson2019,
  title = {The {{Bitter Lesson}}},
  author = {Sutton, Rich},
  date = {2019},
  url = {http://incompleteideas.net/IncIdeas/BitterLesson.html},
  urldate = {2023-06-13},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/HP7I3W55/BitterLesson.html}
}

@article{touvronLlamaOpenFoundation,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  langid = {english},
  keywords = {/unread,⛔ No DOI found},
  file = {/Users/andrew/Zotero/storage/XYQWPWHY/Touvron et al. - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf}
}

@article{tschiatschekEquityFairnessBayesian2022a,
  title = {Equity and {{Fairness}} of {{Bayesian Knowledge Tracing}}},
  author = {Tschiatschek, Sebastian and Knobelsdorf, Maria and Adish Singla},
  editora = {Mitrovic, Antonija and Bosch, Nigel},
  editoratype = {collaborator},
  date = {2022-07-18},
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.6853012},
  url = {https://zenodo.org/record/6853012},
  urldate = {2023-03-08},
  abstract = {We consider the equity and fairness of curricula derived from Knowledge Tracing models. We begin by defining a unifying notion of an equitable tutoring system as a system that achieves maximum possible knowledge in minimal time for each student interacting with it. Realizing perfect equity requires tutoring systems that can provide individualized curricula per student. In particular, we investigate the design of equitable tutoring systems that derive their curricula from Knowledge Tracing models. We first show that the classical Bayesian Knowledge Tracing (BKT) model and their derived curricula can fall short of achieving equitable tutoring. To overcome this issue, we then propose a novel model, Bayesian-Bayesian Knowledge Tracing (B2KT), that naturally allows online individualization. We demonstrate that curricula derived from our model are more effective and equitable than those derived from existing models. Furthermore, we highlight that improving models with a focus on the fairness of next-step predictions can be insufficient to develop equitable tutoring systems.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/QGCP2NZ7/Tschiatschek et al. - 2022 - Equity and Fairness of Bayesian Knowledge Tracing.pdf}
}

@article{turingCOMPUTINGMACHINERYINTELLIGENCE1950,
  title = {I.—{{COMPUTING MACHINERY AND INTELLIGENCE}}},
  author = {TURING, A. M.},
  date = {1950-10-01},
  journaltitle = {Mind},
  shortjournal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  url = {https://doi.org/10.1093/mind/LIX.236.433},
  urldate = {2023-05-04},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/ENA265IY/TURING - 1950 - I.—COMPUTING MACHINERY AND INTELLIGENCE.pdf;/Users/andrew/Zotero/storage/NS3QIJKK/TURING_1950_I.pdf}
}

@article{universityoftasmaniaaustraliaPromptingHigherEducation2023,
  title = {Prompting {{Higher Education Towards AI-Augmented Teaching}} and {{Learning Practice}}},
  author = {{University of Tasmania, Australia} and Eager, Bronwyn and Brunton, Ryan and {University of Tasmania, Australia}},
  date = {2023-05-29},
  journaltitle = {Journal of University Teaching and Learning Practice},
  shortjournal = {JUTLP},
  volume = {20},
  number = {5},
  issn = {14499789, 14499789},
  doi = {10.53761/1.20.5.02},
  url = {https://ro.uow.edu.au/jutlp/vol20/iss5/02/},
  urldate = {2023-05-30},
  abstract = {Large Language Models (LLMs) and conversational-style generative artificial intelligence (AI) are causing major disruption to higher education pedagogy. The emergence of tools like ChatGPT has raised concerns about plagiarism detection but also presents opportunities for educators to leverage AI to build supportive learning environments. In this commentary, we explore the potential of AI-augmented teaching and learning practice in higher education, discussing both the productive affordances and challenges associated with these technologies. We offer instructional advice for writing instructional text to guide the generation of quality outputs from AI models, as well as a case study to illustrate using AI for assessment design. Ultimately, we suggest that AI should be seen as one tool among many that can be used to enhance teaching and learning outcomes in higher education.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/99UM9AJ2/University of Tasmania, Australia et al. - 2023 - Prompting Higher Education Towards AI-Augmented Te.pdf}
}

@report{vanrooijReclaimingAITheoretical2023,
  type = {preprint},
  title = {Reclaiming {{AI}} as a Theoretical Tool for Cognitive Science},
  author = {Van Rooij, Iris and Guest, Olivia and Adolfi, Federico G and De Haan, Ronald and Kolokolova, Antonina and Rich, Patricia},
  date = {2023-08-01},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/4cbuv},
  url = {https://osf.io/4cbuv},
  urldate = {2023-08-04},
  abstract = {The idea that human cognition is, or can be understood as, a form of computation is a useful conceptual tool for cognitive science. It was a foundational assumption during the birth of cognitive science as a multidisciplinary field, with Artificial Intelligence (AI) as one of its contributing fields. One conception of AI in this context is as a provider of computational tools (frameworks, concepts, formalisms, models, proofs, simulations, etc.) that support theory building in cognitive science. The contemporary field of AI, however, has taken the theoretical possibility of explaining human cognition as a form of computation to imply the practical feasibility of realising human(-like or -level) cognition in factual computational systems; and, the field frames this realisation as a short-term inevitability. Yet, as we formally prove herein, creating systems with human(-like or -level) cognition is intrinsically computationally intractable. This means that any factual AI systems created in the short-run are at best decoys. When we think these systems capture something deep about ourselves and our thinking, we induce distorted and impoverished images of ourselves and our cognition. In other words, AI in current practice is deteriorating our theoretical understanding of cognition rather than advancing and enhancing it. The situation could be remediated by releasing the grip of the currently dominant view on AI and by returning to the idea of AI as a theoretical tool for cognitive science. In reclaiming this older idea of AI, however, it is important not to repeat conceptual mistakes of the past (and present) that brought us to where we are today.},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/M3ZH6YY5/Van Rooij et al_2023_Reclaiming AI as a theoretical tool for cognitive science.pdf}
}

@online{vasilatosHowkGPTInvestigatingDetection2023,
  title = {{{HowkGPT}}: {{Investigating}} the {{Detection}} of {{ChatGPT-generated University Student Homework}} through {{Context-Aware Perplexity Analysis}}},
  shorttitle = {{{HowkGPT}}},
  author = {Vasilatos, Christoforos and Alam, Manaar and Rahwan, Talal and Zaki, Yasir and Maniatakos, Michail},
  date = {2023-06-07},
  eprint = {2305.18226},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.18226},
  urldate = {2023-06-27},
  abstract = {As the use of Large Language Models (LLMs) in text generation tasks proliferates, concerns arise over their potential to compromise academic integrity. The education sector currently tussles with distinguishing student-authored homework assignments from AI-generated ones. This paper addresses the challenge by introducing HowkGPT, designed to identify homework assignments generated by AI. HowkGPT is built upon a dataset of academic assignments and accompanying metadata [18] and employs a pretrained LLM to compute perplexity scores for student-authored and ChatGPT-generated responses. These scores then assist in establishing a threshold for discerning the origin of a submitted assignment. Given the specificity and contextual nature of academic work, HowkGPT further refines its analysis by defining category-specific thresholds derived from the metadata, enhancing the precision of the detection. This study emphasizes the critical need for effective strategies to uphold academic integrity amidst the growing influence of LLMs and provides an approach to ensuring fair and accurate grading in educational institutions.},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/XBEWKXVL/Vasilatos et al. - 2023 - HowkGPT Investigating the Detection of ChatGPT-ge.pdf}
}

@unpublished{vaswaniAttentionAllYou2017b,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-05-15},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/JKYDXK2K/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/andrew/Zotero/storage/2DZV3P2H/1706.html}
}

@article{velezTeachersRecruitMentalizing2023,
  title = {Teachers Recruit Mentalizing Regions to Represent Learners’ Beliefs},
  author = {Vélez, Natalia and Chen, Alicia M. and Burke, Taylor and Cushman, Fiery A. and Gershman, Samuel J.},
  date = {2023-05-30},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {22},
  pages = {e2215015120},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2215015120},
  url = {https://www.pnas.org/doi/10.1073/pnas.2215015120},
  urldate = {2023-05-24},
  abstract = {Teaching enables humans to impart vast stores of culturally specific knowledge and skills. However, little is known about the neural computations that guide teachers’ decisions about what information to communicate. Participants (N = 28) played the role of teachers while being scanned using fMRI; their task was to select examples that would teach learners how to answer abstract multiple-choice questions. Participants’ examples were best described by a model that selects evidence that maximizes the learner’s belief in the correct answer. Consistent with this idea, participants’ predictions about how well learners would do closely tracked the performance of an independent sample of learners (N = 140) who were tested on the examples they had provided. In addition, regions that play specialized roles in processing social information, namely the bilateral temporoparietal junction and middle and dorsal medial prefrontal cortex, tracked learners’ posterior belief in the correct answer. Our results shed light on the computational and neural architectures that support our extraordinary abilities as teachers.},
  keywords = {/unread}
}

@article{victorAnalogyCoreCognition,
  title = {Analogy as the {{Core}} of {{Cognition}}},
  author = {Victor, Bret},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/TB7X6P3X/Victor - Analogy as the Core of Cognition.pdf}
}

@inproceedings{wambsganssArgueTutorAdaptiveDialogBased2021,
  title = {{{ArgueTutor}}: {{An Adaptive Dialog-Based Learning System}} for {{Argumentation Skills}}},
  shorttitle = {{{ArgueTutor}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wambsganss, Thiemo and Kueng, Tobias and Soellner, Matthias and Leimeister, Jan Marco},
  date = {2021-05-06},
  pages = {1--13},
  publisher = {{ACM}},
  location = {{Yokohama Japan}},
  doi = {10.1145/3411764.3445781},
  url = {https://dl.acm.org/doi/10.1145/3411764.3445781},
  urldate = {2023-09-21},
  eventtitle = {{{CHI}} '21: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-8096-6},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/NNRD6KIH/Wambsganss et al. - 2021 - ArgueTutor An Adaptive Dialog-Based Learning Syst.pdf}
}

@article{wambsganssEnhancingArgumentativeWriting2022,
  title = {Enhancing Argumentative Writing with Automated Feedback and Social Comparison Nudging},
  author = {Wambsganss, Thiemo and Janson, Andreas and Leimeister, Jan Marco},
  date = {2022-12-01},
  journaltitle = {Computers \& Education},
  shortjournal = {Computers \& Education},
  volume = {191},
  pages = {104644},
  issn = {0360-1315},
  doi = {10.1016/j.compedu.2022.104644},
  url = {https://www.sciencedirect.com/science/article/pii/S0360131522002159},
  urldate = {2023-08-17},
  abstract = {The advantages offered by natural language processing (NLP) and machine learning enable students to receive automated feedback on their argumentation skills, independent of educator, time, and location. Although there is a growing amount of literature on formative argumentation feedback, empirical evidence on the effects of adaptive feedback mechanisms and novel NLP approaches to enhance argumentative writing remains scarce. To help fill this gap, the aim of the present study is to investigate whether automated feedback and social comparison nudging enable students to internalize and improve logical argumentation writing abilities in an undergraduate business course. We conducted a mixed-methods study to investigate the impact of argumentative writing on 71 students in a field experiment. Students in treatment group 1 completed their assignment while receiving automated feedback, whereas students in treatment group 2 completed the same assignment while receiving automated feedback with a social comparison nudge that indicated how other students performed on the same assignment. Students in the control group received generalized feedback based on rules of syntax. We found that participants who received automated argumentation feedback with a social comparison nudge wrote more convincing texts with higher-quality argumentation compared to the two benchmark groups (p~{$<~$}0.05). The measured self-efficacy, perceived ease of use, and qualitative data provide valuable insights that help explain this effect. The results suggest that embedding automated feedback in combination with social comparison nudges enables students to increase their argumentative writing skills by triggering psychological processes. Receiving only automated feedback in the form of in-text argumentative highlighting without any further guidance appears not to significantly influence students’ writing abilities when compared to syntactic feedback.},
  keywords = {/unread,Argumentation,Automated feedback,Machine learning,Natural language processing,Social comparison nudging},
  file = {/Users/andrew/Zotero/storage/ER8NT2HJ/Wambsganss et al_2022_Enhancing argumentative writing with automated feedback and social comparison.pdf}
}

@online{wangInstructionsGuideDiagnostic2021,
  title = {Instructions and {{Guide}} for {{Diagnostic Questions}}: {{The NeurIPS}} 2020 {{Education Challenge}}},
  shorttitle = {Instructions and {{Guide}} for {{Diagnostic Questions}}},
  author = {Wang, Zichao and Lamb, Angus and Saveliev, Evgeny and Cameron, Pashmina and Zaykov, Yordan and Hernández-Lobato, José Miguel and Turner, Richard E. and Baraniuk, Richard G. and Barton, Craig and Jones, Simon Peyton and Woodhead, Simon and Zhang, Cheng},
  date = {2021-04-12},
  eprint = {2007.12061},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.12061},
  url = {http://arxiv.org/abs/2007.12061},
  urldate = {2023-03-28},
  abstract = {Digital technologies are becoming increasingly prevalent in education, enabling personalized, high quality education resources to be accessible by students across the world. Importantly, among these resources are diagnostic questions: the answers that the students give to these questions reveal key information about the specific nature of misconceptions that the students may hold. Analyzing the massive quantities of data stemming from students' interactions with these diagnostic questions can help us more accurately understand the students' learning status and thus allow us to automate learning curriculum recommendations. In this competition, participants will focus on the students' answer records to these multiple-choice diagnostic questions, with the aim of 1) accurately predicting which answers the students provide; 2) accurately predicting which questions have high quality; and 3) determining a personalized sequence of questions for each student that best predicts the student's answers. These tasks closely mimic the goals of a real-world educational platform and are highly representative of the educational challenges faced today. We provide over 20 million examples of students' answers to mathematics questions from Eedi, a leading educational platform which thousands of students interact with daily around the globe. Participants to this competition have a chance to make a lasting, real-world impact on the quality of personalized education for millions of students across the world.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/NSU32T9U/Wang et al_2021_Instructions and Guide for Diagnostic Questions.pdf;/Users/andrew/Zotero/storage/LEB5VMBT/2007.html}
}

@online{wangSelfConsistencyImprovesChain2023,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  date = {2023-03-07},
  eprint = {2203.11171},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.11171},
  url = {http://arxiv.org/abs/2203.11171},
  urldate = {2023-06-09},
  abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/KHTHU6S9/Wang et al_2023_Self-Consistency Improves Chain of Thought Reasoning in Language Models.pdf;/Users/andrew/Zotero/storage/MHHCVKHB/2203.html}
}

@online{wangVarFAVariationalFactor2020,
  title = {{{VarFA}}: {{A Variational Factor Analysis Framework For Efficient Bayesian Learning Analytics}}},
  shorttitle = {{{VarFA}}},
  author = {Wang, Zichao and Gu, Yi and Lan, Andrew and Baraniuk, Richard},
  date = {2020-08-14},
  eprint = {2005.13107},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2005.13107},
  url = {http://arxiv.org/abs/2005.13107},
  urldate = {2023-05-24},
  abstract = {We propose VarFA, a variational inference factor analysis framework that extends existing factor analysis models for educational data mining to efficiently output uncertainty estimation in the model's estimated factors. Such uncertainty information is useful, for example, for an adaptive testing scenario, where additional tests can be administered if the model is not quite certain about a students' skill level estimation. Traditional Bayesian inference methods that produce such uncertainty information are computationally expensive and do not scale to large data sets. VarFA utilizes variational inference which makes it possible to efficiently perform Bayesian inference even on very large data sets. We use the sparse factor analysis model as a case study and demonstrate the efficacy of VarFA on both synthetic and real data sets. VarFA is also very general and can be applied to a wide array of factor analysis models.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  file = {/Users/andrew/Zotero/storage/7JNRD8KS/Wang et al_2020_VarFA.pdf;/Users/andrew/Zotero/storage/F6N6SDAL/2005.html}
}

@article{webbEmergentAnalogicalReasoning2023,
  title = {Emergent Analogical Reasoning in Large Language Models},
  author = {Webb, Taylor and Holyoak, Keith J. and Lu, Hongjing},
  date = {2023-07-31},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  pages = {1--16},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-023-01659-w},
  url = {https://www.nature.com/articles/s41562-023-01659-w},
  urldate = {2023-08-04},
  abstract = {The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of Generative Pre-trained Transformer (GPT)-3) on a range of analogical tasks, including a non-visual matrix reasoning task based on the rule structure of Raven’s Standard Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings; preliminary tests of GPT-4 indicated even better performance. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.},
  langid = {english},
  keywords = {/unread,Computer science,Human behaviour},
  file = {/Users/andrew/Zotero/storage/8KB36P6W/Webb et al_2023_Emergent analogical reasoning in large language models.pdf}
}

@online{weber-wulffTestingDetectionTools2023,
  title = {Testing of {{Detection Tools}} for {{AI-Generated Text}}},
  author = {Weber-Wulff, Debora and Anohina-Naumeca, Alla and Bjelobaba, Sonja and Foltýnek, Tomáš and Guerrero-Dib, Jean and Popoola, Olumide and Šigut, Petr and Waddington, Lorna},
  date = {2023-06-21},
  eprint = {2306.15666},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.15666},
  url = {http://arxiv.org/abs/2306.15666},
  urldate = {2023-07-03},
  abstract = {Recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (AI) generated content in an academic environment and intensified efforts in searching for solutions to detect such content. The paper examines the general functionality of detection tools for artificial intelligence generated text and evaluates them based on accuracy and error type analysis. Specifically, the study seeks to answer research questions about whether existing detection tools can reliably differentiate between human-written text and ChatGPT-generated text, and whether machine translation and content obfuscation techniques affect the detection of AIgenerated text. The research covers 12 publicly available tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely used in the academic setting. The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bias towards classifying the output as human-written rather than detecting AIgenerated text. Furthermore, content obfuscation techniques significantly worsen the performance of tools. The study makes several significant contributions. First, it summarises up-to-date similar scientific and non-scientific efforts in the field. Second, it presents the result of one of the most comprehensive tests conducted so far, based on a rigorous research methodology, an original document set, and a broad coverage of tools. Third, it discusses the implications and drawbacks of using detection tools for AI-generated text in academic settings.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,I.2.7,I.2.m},
  file = {/Users/andrew/Zotero/storage/6NDXPP3B/Weber-Wulff et al_2023_Testing of Detection Tools for AI-Generated Text.pdf;/Users/andrew/Zotero/storage/W5I25PTG/2306.html}
}

@online{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2023-01-10},
  eprint = {2201.11903},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.11903},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2023-03-02},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/GJBNDTL9/Wei et al_2023_Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf;/Users/andrew/Zotero/storage/TJEZ8D29/2201.html}
}

@online{weiChainofThoughtPromptingElicits2023a,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2023-01-10},
  eprint = {2201.11903},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.11903},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2023-10-15},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/ECP3ZU5X/Wei et al_2023_Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf;/Users/andrew/Zotero/storage/I8L5EMWA/2201.html}
}

@online{weiEmergentAbilitiesLarge2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  date = {2022-10-26},
  eprint = {2206.07682},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.07682},
  url = {http://arxiv.org/abs/2206.07682},
  urldate = {2023-04-20},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/EHJETBL3/Wei et al_2022_Emergent Abilities of Large Language Models.pdf;/Users/andrew/Zotero/storage/BMBRKP3L/2206.html}
}

@online{weiEmergentAbilitiesLarge2022a,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  date = {2022-10-26},
  eprint = {2206.07682},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2206.07682},
  urldate = {2023-04-25},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/NN37SV4E/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf}
}

@online{weiEmergentAbilitiesLarge2022b,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  date = {2022-10-26},
  eprint = {2206.07682},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.07682},
  url = {http://arxiv.org/abs/2206.07682},
  urldate = {2023-10-25},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/NTZCUDAY/Wei et al_2022_Emergent Abilities of Large Language Models.pdf;/Users/andrew/Zotero/storage/2JF228HI/2206.html}
}

@inproceedings{weitekampInteractionDesignMachine2020,
  title = {An {{Interaction Design}} for {{Machine Teaching}} to {{Develop AI Tutors}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Weitekamp, Daniel and Harpstead, Erik and Koedinger, Ken R.},
  date = {2020-04-23},
  series = {{{CHI}} '20},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376226},
  url = {https://doi.org/10.1145/3313831.3376226},
  urldate = {2023-03-17},
  abstract = {Intelligent tutoring systems (ITSs) have consistently been shown to improve the educational outcomes of students when used alone or combined with traditional instruction. However, building an ITS is a time-consuming process which requires specialized knowledge of existing tools. Extant authoring methods, including the Cognitive Tutor Authoring Tools' (CTAT) example-tracing method and SimStudent's Authoring by Tutoring, use programming-by-demonstration to allow authors to build ITSs more quickly than they could by hand programming with model-tracing. Yet these methods still suffer from long authoring times or difficulty creating complete models. In this study, we demonstrate that Simulated Learners built with the Apprentice Learner (AL) Framework can be combined with a novel interaction design that emphasizes model transparency, input flexibility, and problem solving control to enable authors to achieve greater model completeness in less time than existing authoring methods.},
  isbn = {978-1-4503-6708-0},
  keywords = {\{simulated learners\vphantom\},/unread,intelligent tutoring systems,interaction design,machine teaching,programming-by-demonstration},
  file = {/Users/andrew/Zotero/storage/SSJXVDJL/Weitekamp et al_2020_An Interaction Design for Machine Teaching to Develop AI Tutors.pdf}
}

@online{WhatChatGPTDoing2023,
  title = {What {{Is ChatGPT Doing}} … and {{Why Does It Work}}?},
  date = {2023-02-14},
  url = {https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/},
  urldate = {2023-04-20},
  abstract = {Stephen Wolfram explores the broader picture of what's going on inside ChatGPT and why it produces meaningful text. Discusses models, training neural nets, embeddings, tokens, transformers, language syntax.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/3G5VCMNQ/what-is-chatgpt-doing-and-why-does-it-work.html}
}

@online{wolframWhatChatGPTDoing2023,
  title = {What {{Is ChatGPT Doing}} … and {{Why Does It Work}}?},
  author = {Wolfram, Stephen},
  date = {2023-02-14},
  url = {https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/},
  urldate = {2023-03-01},
  abstract = {Stephen Wolfram explores the broader picture of what's going on inside ChatGPT and why it produces meaningful text. Discusses models, training neural nets, embeddings, tokens, transformers, language syntax.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/SYQP8GVU/what-is-chatgpt-doing-and-why-does-it-work.html}
}

@online{wongWordModelsWorld2023,
  title = {From {{Word Models}} to {{World Models}}: {{Translating}} from {{Natural Language}} to the {{Probabilistic Language}} of {{Thought}}},
  shorttitle = {From {{Word Models}} to {{World Models}}},
  author = {Wong, Lionel and Grand, Gabriel and Lew, Alexander K. and Goodman, Noah D. and Mansinghka, Vikash K. and Andreas, Jacob and Tenenbaum, Joshua B.},
  date = {2023-06-22},
  eprint = {2306.12672},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.12672},
  url = {http://arxiv.org/abs/2306.12672},
  urldate = {2023-06-25},
  abstract = {How does language inform our downstream thinking? In particular, how do humans make meaning from language -- and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose \textbackslash textit\{rational meaning construction\}, a computational framework for language-informed thinking that combines neural models of language with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a \textbackslash textit\{probabilistic language of thought\} (PLoT) -- a general-purpose symbolic substrate for probabilistic, generative world modeling. Our architecture integrates two powerful computational tools that have not previously come together: we model thinking with \textbackslash textit\{probabilistic programs\}, an expressive representation for flexible commonsense reasoning; and we model meaning construction with \textbackslash textit\{large language models\} (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework in action through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning about agents and their plans. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Symbolic Computation},
  file = {/Users/andrew/Zotero/storage/4WACCWDP/Wong et al_2023_From Word Models to World Models.pdf;/Users/andrew/Zotero/storage/NRJ449IG/2306.html}
}

@online{wuLLMDetLargeLanguage2023,
  title = {{{LLMDet}}: {{A Large Language Models Detection Tool}}},
  shorttitle = {{{LLMDet}}},
  author = {Wu, Kangxi and Pang, Liang and Shen, Huawei and Cheng, Xueqi and Chua, Tat-Seng},
  date = {2023-05-24},
  eprint = {2305.15004},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.15004},
  urldate = {2023-06-27},
  abstract = {With the advancement of generative language models, the generated text has come remarkably close to high-quality human-authored text in terms of fluency and diversity. This calls for a highly practical detection tool that can identify the source of text, preferably pinpointing the language model it originates from. However, existing detection tools typically require access to language models and can only differentiate between machine-generated and human-authored text, failing to meet the requirements of rapid detection and text tracing. Therefore, in this paper, we propose an efficient, secure, and scalable detection tool called LLMDet, which calculates the proxy perplexity of text by utilizing the prior information of the model’s next-token probabilities, obtained through pre-training. Subsequently, we use the self-watermarking information of the model, as measured by proxy perplexity, to detect the source of the text. We found that our method demonstrates impressive detection performance while ensuring speed and security, particularly achieving a recognition accuracy of 97.97\% for human-authored text. Furthermore, our detection tool also shows promising results in identifying the large language model (e.g., GPT-2, OPT, LLaMA, Vicuna...) responsible for the text. We release the code and processed data at https://github.com/TrustedLLM/LLMDet.},
  langid = {english},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/2FNQGLR9/Wu et al. - 2023 - LLMDet A Large Language Models Detection Tool.pdf}
}

@online{wuReasoningRecitingExploring2023,
  title = {Reasoning or {{Reciting}}? {{Exploring}} the {{Capabilities}} and {{Limitations}} of {{Language Models Through Counterfactual Tasks}}},
  shorttitle = {Reasoning or {{Reciting}}?},
  author = {Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Akyürek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
  date = {2023-08-01},
  eprint = {2307.02477},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.02477},
  url = {http://arxiv.org/abs/2307.02477},
  urldate = {2023-10-02},
  abstract = {The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to a degree, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/KRIAIEZF/Wu et al_2023_Reasoning or Reciting.pdf;/Users/andrew/Zotero/storage/WNNJ2QA7/2307.html}
}

@article{xuePhyQMeasurePhysical2023,
  title = {Phy-{{Q}} as a Measure for Physical Reasoning Intelligence},
  author = {Xue, Cheng and Pinto, Vimukthini and Gamage, Chathura and Nikonova, Ekaterina and Zhang, Peng and Renz, Jochen},
  date = {2023-01},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {5},
  number = {1},
  pages = {83--93},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00583-4},
  url = {https://www.nature.com/articles/s42256-022-00583-4},
  urldate = {2023-02-11},
  abstract = {Humans are well versed in reasoning about the behaviours of physical objects and choosing actions accordingly to accomplish tasks, while this remains a major challenge for artificial intelligence. To facilitate research addressing this problem, we propose a new testbed that requires an agent to reason about physical scenarios and take an action appropriately. Inspired by the physical knowledge acquired in infancy and the capabilities required for robots to operate in real-world environments, we identify 15 essential physical scenarios. We create a wide variety of distinct task templates, and we ensure that all the task templates within the same scenario can be solved by using one specific strategic physical rule. By having such a design, we evaluate two distinct levels of generalization, namely local generalization and broad generalization. We conduct an extensive evaluation with human players, learning agents with various input types and architectures, and heuristic agents with different strategies. Inspired by how the human intelligence quotient is calculated, we define the physical reasoning quotient (Phy-Q score) that reflects the physical reasoning intelligence of an agent using the physical scenarios we considered. Our evaluation shows that (1) all the agents are far below human performance, and (2) learning agents, even with good local generalization ability, struggle to learn the underlying physical reasoning rules and fail to generalize broadly. We encourage the development of intelligent agents that can reach the human-level Phy-Q score.},
  issue = {1},
  langid = {english},
  keywords = {/unread,Computer science,Human behaviour,Scientific data,Software},
  file = {/Users/andrew/Zotero/storage/T8SET792/Xue et al_2023_Phy-Q as a measure for physical reasoning intelligence.pdf}
}

@online{yangLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Optimizers}}},
  author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
  date = {2023-09-06},
  eprint = {2309.03409},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.03409},
  urldate = {2023-10-18},
  abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/DLQJ5DLK/Yang et al_2023_Large Language Models as Optimizers.pdf;/Users/andrew/Zotero/storage/3SBQQ5XQ/2309.html}
}

@article{yangOneModelLearning2022,
  title = {One Model for the Learning of Language},
  author = {Yang, Yuan and Piantadosi, Steven T.},
  date = {2022-02},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {119},
  number = {5},
  pages = {e2021865119},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2021865119},
  url = {https://pnas.org/doi/full/10.1073/pnas.2021865119},
  urldate = {2023-03-14},
  abstract = {Significance             It has long been hypothesized that language acquisition may be impossible without innate knowledge of the structures that occur in natural language. Here, we show that a domain general learning setup, originally developed in cognitive psychology to model rule learning, is able to acquire key pieces of natural language from relatively few examples of sentences. This develops a new approach to formalizing linguistic learning and highlights some features of language and language acquisition that may arise from general cognitive processes.           ,                             A major goal of linguistics and cognitive science is to understand what class of learning systems can acquire natural language. Until recently, the computational requirements of language have been used to argue that learning is impossible without a highly constrained hypothesis space. Here, we describe a learning system that is maximally unconstrained, operating over the space of all computations, and is able to acquire many of the key structures present in natural language from positive evidence alone. We demonstrate this by providing the same learning model with data from 74 distinct formal languages which have been argued to capture key features of language, have been studied in experimental work, or come from an interesting complexity class. The model is able to successfully induce the latent system generating the observed strings from small amounts of evidence in almost all cases, including for regular (e.g.,                                a                 n                              ,                                                                                                                        (                         a                         b                         )                                              n                                                                                       , and                                                                                                                        \{                         a                         ,                         b                         \}                                              +                                                                                       ), context-free (e.g.,                                                                                               a                       n                                                                 b                       n                                          ,                     \,                                            a                       n                                                                 b                                                n                         +                         m                                                                                                              , and                                                                        x                                            x                       R                                                                                       ), and context-sensitive (e.g.,                                                                                               a                       n                                                                 b                       n                                                                 c                       n                                          ,                     \,                                            a                       n                                                                 b                       m                                                                 c                       n                                                                 d                       m                                                                                       , and               xx               ) languages, as well as for many languages studied in learning experiments. These results show that relatively small amounts of positive evidence can support learning of rich classes of generative computations over structures. The model provides an idealized learning setup upon which additional cognitive constraints and biases can be formalized.},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/VWA6RP6X/Yang_Piantadosi_2022_One model for the learning of language.pdf}
}

@online{yaoTreeThoughtsDeliberate2023,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  date = {2023-05-17},
  eprint = {2305.10601},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.10601},
  url = {http://arxiv.org/abs/2305.10601},
  urldate = {2023-05-23},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.},
  pubstate = {preprint},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/QVACSS9L/Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with .pdf;/Users/andrew/Zotero/storage/Q545MW3W/2305.html}
}

@incollection{yudelsonIndividualizedBayesianKnowledge2013a,
  title = {Individualized {{Bayesian Knowledge Tracing Models}}},
  booktitle = {Artificial {{Intelligence}} in {{Education}}},
  author = {Yudelson, Michael V. and Koedinger, Kenneth R. and Gordon, Geoffrey J.},
  editor = {Lane, H. Chad and Yacef, Kalina and Mostow, Jack and Pavlik, Philip},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  date = {2013},
  volume = {7926},
  pages = {171--180},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39112-5_18},
  url = {http://link.springer.com/10.1007/978-3-642-39112-5_18},
  urldate = {2023-03-08},
  abstract = {Bayesian Knowledge Tracing (BKT)[1] is a user modeling method extensively used in the area of Intelligent Tutoring Systems. In the standard BKT implementation, there are only skill-specific parameters. However, a large body of research strongly suggests that studentspecific variability in the data, when accounted for, could enhance model accuracy [5, 6, 8]. In this work, we revisit the problem of introducing student-specific parameters into BKT on a larger scale. We show that student-specific parameters lead to a tangible improvement when predicting the data of unseen students, and that parameterizing students’ speed of learning is more beneficial than parameterizing a priori knowledge.},
  isbn = {978-3-642-39111-8 978-3-642-39112-5},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/CI8MTT3S/Yudelson et al. - 2013 - Individualized Bayesian Knowledge Tracing Models.pdf}
}

@incollection{yudelsonIndividualizedBayesianKnowledge2013b,
  title = {Individualized {{Bayesian Knowledge Tracing Models}}},
  booktitle = {Artificial {{Intelligence}} in {{Education}}},
  author = {Yudelson, Michael V. and Koedinger, Kenneth R. and Gordon, Geoffrey J.},
  editor = {Lane, H. Chad and Yacef, Kalina and Mostow, Jack and Pavlik, Philip},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  date = {2013},
  volume = {7926},
  pages = {171--180},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39112-5_18},
  url = {http://link.springer.com/10.1007/978-3-642-39112-5_18},
  urldate = {2023-03-24},
  abstract = {Bayesian Knowledge Tracing (BKT)[1] is a user modeling method extensively used in the area of Intelligent Tutoring Systems. In the standard BKT implementation, there are only skill-specific parameters. However, a large body of research strongly suggests that studentspecific variability in the data, when accounted for, could enhance model accuracy [5, 6, 8]. In this work, we revisit the problem of introducing student-specific parameters into BKT on a larger scale. We show that student-specific parameters lead to a tangible improvement when predicting the data of unseen students, and that parameterizing students’ speed of learning is more beneficial than parameterizing a priori knowledge.},
  isbn = {978-3-642-39111-8 978-3-642-39112-5},
  langid = {english},
  keywords = {/unread},
  file = {/Users/andrew/Zotero/storage/I7K7Z9MI/Yudelson et al. - 2013 - Individualized Bayesian Knowledge Tracing Models.pdf}
}

@inproceedings{yudelsonIndividualizedBayesianKnowledge2013c,
  title = {Individualized {{Bayesian Knowledge Tracing Models}}},
  booktitle = {Artificial {{Intelligence}} in {{Education}}},
  author = {Yudelson, Michael V. and Koedinger, Kenneth R. and Gordon, Geoffrey J.},
  editor = {Lane, H. Chad and Yacef, Kalina and Mostow, Jack and Pavlik, Philip},
  date = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {171--180},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-39112-5_18},
  abstract = {Bayesian Knowledge Tracing (BKT)[1] is a user modeling method extensively used in the area of Intelligent Tutoring Systems. In the standard BKT implementation, there are only skill-specific parameters. However, a large body of research strongly suggests that student-specific variability in the data, when accounted for, could enhance model accuracy [5,6,8]. In this work, we revisit the problem of introducing student-specific parameters into BKT on a larger scale. We show that student-specific parameters lead to a tangible improvement when predicting the data of unseen students, and that parameterizing students’ speed of learning is more beneficial than parameterizing a priori knowledge.},
  isbn = {978-3-642-39112-5},
  langid = {english},
  keywords = {/unread,Bayesian knowledge tracing,model fitting,model selection,student-specific model parameters},
  file = {/Users/andrew/Zotero/storage/EK43IS7Z/Yudelson et al_2013_Individualized Bayesian Knowledge Tracing Models.pdf}
}
