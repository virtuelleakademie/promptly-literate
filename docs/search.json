[
  {
    "objectID": "slides/04-reflection.html#idea-generation",
    "href": "slides/04-reflection.html#idea-generation",
    "title": "Idea Generation and Reflection",
    "section": "Idea Generation",
    "text": "Idea Generation\n\nIdeas for tools: Do you have any ideas that weren‚Äôt discussed in this workshop? What would you like to see in the future?\nOpportunities and challenges: What are the opportunities and challenges involved with using LLMs in educational settings?"
  },
  {
    "objectID": "slides/04-reflection.html#reflection",
    "href": "slides/04-reflection.html#reflection",
    "title": "Idea Generation and Reflection",
    "section": "Reflection",
    "text": "Reflection\n\nDo you feel confident in using LLMs in your educational practice? What would help you feel more confident?\nDo you feel that you understand how to apply LLMs?\nDo you feel confident in evaluating LLM-based tools?"
  },
  {
    "objectID": "slides/04-reflection.html#thank-you-for-attending-this-workshop",
    "href": "slides/04-reflection.html#thank-you-for-attending-this-workshop",
    "title": "Idea Generation and Reflection",
    "section": "Thank you for attending this workshop!",
    "text": "Thank you for attending this workshop!\n  \nüåü Your feedback matters! Please take 5 minutes to complete our üëâ evaluation form.\n\n\nback to website ‚§¥Ô∏è"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#what-does-an-llm-know",
    "href": "slides/02-prompting-techniques.html#what-does-an-llm-know",
    "title": "Basic Prompting Techniques",
    "section": "What does an LLM know?",
    "text": "What does an LLM know?\n\nLLMs (with enough parameters) show emergent behaviour; they learn to do things that they were not explicitly trained to do.\nThey can e.g.¬†solve logic puzzles and perform complex multistep reasoning.\nOften, these capabilities, which are encoded in the models parameters, need to be ‚Äúunlocked‚Äù by the right prompt."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#openai-best-practices",
    "href": "slides/02-prompting-techniques.html#openai-best-practices",
    "title": "Basic Prompting Techniques",
    "section": "OpenAI Best Practices",
    "text": "OpenAI Best Practices\nOpenAI give a set of strategies for using their models üëâ Six strategies .\nThese include:\n\nwriting clear instructions\nproviding reference texts\nsplitting tasks into subtasks\ngiving GPT ‚Äòtime to think‚Äô\nusing external tools (This is provided by e.g.¬†plugins.)"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#writing-clear-instructions",
    "href": "slides/02-prompting-techniques.html#writing-clear-instructions",
    "title": "Basic Prompting Techniques",
    "section": "Writing clear instructions",
    "text": "Writing clear instructions\nGiven that might conceive of an LLM as a role-playing simulator of conversations, it is intuitively clear that instructions should be clear and unambiguous, and should indicate which role the model should adopt.\n\nInclude details in your query to get more relevant answers\nAsk the model to adopt a persona\nUse delimiters to clearly indicate distinct parts of the input\nSpecify the steps required to complete a task\nProvide examples\nSpecify the desired length of the output"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#providing-reference-texts",
    "href": "slides/02-prompting-techniques.html#providing-reference-texts",
    "title": "Basic Prompting Techniques",
    "section": "Providing reference texts",
    "text": "Providing reference texts\n\nProvide a model with trusted and relevant information.\nThen instruct the model to use the provided information to compose its answer.\n\nThis leads to an incredibly powerful technique, which is known as retrieval-augmented generation. By this, we mean that we first create a database of documents, and then retrieve the most relevant documents, based on a user‚Äôs query. These are then included in the prompt to the model. The model is instructed to use the information in the documents to compose its answer."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#giving-gpt-time-to-think",
    "href": "slides/02-prompting-techniques.html#giving-gpt-time-to-think",
    "title": "Basic Prompting Techniques",
    "section": "Giving GPT ‚Äòtime to think‚Äô",
    "text": "Giving GPT ‚Äòtime to think‚Äô\n\nLLMs generate text auto-regressively, and the model spends the same amount of computation on each token.\nRather that getting the answer straight away, it makes sense to give the model more context, and to give it more steps to ‚Äúthink‚Äù.\nBy doing so, you are increasing the chances that the model will give a good answer.\nThis technique is known as chain-of-thought prompting, and can often be induced by simply instructing the model to think step-by-step."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#explore-prompting-techniques",
    "href": "slides/02-prompting-techniques.html#explore-prompting-techniques",
    "title": "Basic Prompting Techniques",
    "section": "Explore prompting techniques",
    "text": "Explore prompting techniques\nNow open the second activity to learn more about ChatGPT and OpenAI Playground: üëâ Activity 2."
  },
  {
    "objectID": "slides/00-introduction.html#what-happens-when-your-lawyer-uses-chatgpt",
    "href": "slides/00-introduction.html#what-happens-when-your-lawyer-uses-chatgpt",
    "title": "Introduction",
    "section": "What Happens When Your Lawyer Uses ChatGPT",
    "text": "What Happens When Your Lawyer Uses ChatGPT"
  },
  {
    "objectID": "slides/00-introduction.html#the-best-prompts-for-chatgpt-the-ultimate-list",
    "href": "slides/00-introduction.html#the-best-prompts-for-chatgpt-the-ultimate-list",
    "title": "Introduction",
    "section": "The Best Prompts For ChatGPT: The ultimate list",
    "text": "The Best Prompts For ChatGPT: The ultimate list"
  },
  {
    "objectID": "slides/00-introduction.html#key-messages",
    "href": "slides/00-introduction.html#key-messages",
    "title": "Introduction",
    "section": "Key messages",
    "text": "Key messages\n\nKeep human in the loop: LLMs should be used to augment human writing, not to replace it.\nPrompting, prompting, prompting: this workshop is mainly about prompting. We can think about prompting as a way of ‚Äúprogramming‚Äù LLMs, i.e.¬†getting LLMs to do what we want them to do."
  },
  {
    "objectID": "slides/00-introduction.html#summary",
    "href": "slides/00-introduction.html#summary",
    "title": "Introduction",
    "section": "Summary",
    "text": "Summary\n\nChatGPT has comparatively high energy requirements.\nLarge language models (LMMs) learn all kinds of human biases from their training data.\nToxic content produced by LLMs is flagged by cheap labor."
  },
  {
    "objectID": "slides/00-introduction.html#energy-consumption",
    "href": "slides/00-introduction.html#energy-consumption",
    "title": "Introduction",
    "section": "Energy consumption",
    "text": "Energy consumption\n\nTraining:\n\n‚ÄúWhat we do know is that training ChatGPT used \\(1.287\\) GWh, roughly equivalent to the consumption of 120 US homes for a year.‚Äù Quelle: Heating up: how much energy does AI use?\nPatterson et al. (2022) estimate training costs at 502 tons of \\(\\text{CO}_2\\).\n\nUsage:\n\n7 tons of \\(\\text{CO}_2\\) per day (end of February). Source: How much energy does ChatGPT use?\nChatGPT‚Äôs energy consumption is equivalent to 400-800 US households. This is considerable, but compared to e.g.¬†cryptocurrencies it is rather low."
  },
  {
    "objectID": "slides/00-introduction.html#bias",
    "href": "slides/00-introduction.html#bias",
    "title": "Introduction",
    "section": "Bias",
    "text": "Bias\n\n\n\n\n\nDa LLMs von Texten lernen, die von Menschen geschrieben wurden, k√∂nnen sie auch Vorurteile lernen.\n\nQuelle: Hast du Vorurteile?"
  },
  {
    "objectID": "slides/00-introduction.html#ethical-aspects",
    "href": "slides/00-introduction.html#ethical-aspects",
    "title": "Introduction",
    "section": "Ethical aspects",
    "text": "Ethical aspects\n\n\n\n\n\nAuf Grund der grossen Menge von Trainingsdaten, die f√ºr Sprachmodelle ben√∂tigt werden, ist Qualit√§tskontrolle schwierig.\nDiskriminierende oder beleidigende Aussagen werden von einem Chatbot generiert.\nSolche Antworten k√∂nnen als unerw√ºnscht markiert werden.\nToxische Inhalte wie k√∂rperliche und sexuelle Gewalt, Suizide und Tierqu√§lerei, m√ºssen beim Trainieren aus den Antworten gefiltert werden. Dabei mussten angestellte Arbeitskr√§fte f√ºr weniger als 2 Dollar die Stunde teils schockierende Inhalte lesen.\n\nQuelle: Traumatische Klickarbeit"
  },
  {
    "objectID": "slides/00-introduction.html#haltung-der-bfh",
    "href": "slides/00-introduction.html#haltung-der-bfh",
    "title": "Introduction",
    "section": "Haltung der BFH",
    "text": "Haltung der BFH\n\nTechnologien sollen dort, wo sie den Lernprozess unterst√ºtzen und praxisrelevant sind, in die Lehre einbezogen werden.\nStudierende sollen lernen, Technologien kompetent einzusetzen und kritisch zu hinterfragen. Dies gilt uneingeschr√§nkt auch f√ºr ChatGPT und andere gleichgerichtete Tools."
  },
  {
    "objectID": "slides/00-introduction.html#zitieren",
    "href": "slides/00-introduction.html#zitieren",
    "title": "Introduction",
    "section": "Zitieren",
    "text": "Zitieren\n\nEs existieren noch keine Richtlinien f√ºr das Zitieren von ChatGPT oder anderen KI-basierte Schreibtools.\nChatGPT ist rein rechtlich keine zitierf√§hige Quelle und damit auch nicht zitierpflichtig (Fleck 2023).\nAus der Orientierungshilfe: ‚ÄúKI-basierte Schreibtools sind externe Quellen und m√ºssen daher im Sinne der wissenschaftlichen Integrit√§t immer, wie andere Quellen auch, zitiert werden, sofern ganze Textpassagen von ChatGPT benutzt werden. Falls mit dem Tool der eigene Text √ºberarbeitet wurde, muss ChatGPT als verwendetes Hilfsmittel angef√ºhrt werden.‚Äù\n\n\n\n\n\n\n\nM√∂glicher Pauschalverweis\n\n\n‚ÄúBeim Verfassen der Arbeit habe ich das KI-gest√ºtzte Schreibwerkzeug ChatGPT zur Textoptimierung verwendet. W√∂rtlich aus dem Tool √ºbernommene Passagen wurden im Text als pers√∂nliche Kommunikation zitiert.‚Äù"
  },
  {
    "objectID": "slides/00-introduction.html#plagiate-und-detektion",
    "href": "slides/00-introduction.html#plagiate-und-detektion",
    "title": "Introduction",
    "section": "Plagiate und Detektion",
    "text": "Plagiate und Detektion\n\nTexte von ChatGPT werden jedes Mal individuell erstellt. Es handelt sich nicht um Plagiate.\nDie klassischen Tools zur Aufdeckung von Plagiaten wie z.B. TurnItIn funktionieren hier nicht.\nDie BFH empfiehlt weiterhin, schriftliche Arbeiten mit der Plagiatserkennungssoftware Turnitin zu pr√ºfen."
  },
  {
    "objectID": "slides/00-introduction.html#kompetenznachweise",
    "href": "slides/00-introduction.html#kompetenznachweise",
    "title": "Introduction",
    "section": "Kompetenznachweise",
    "text": "Kompetenznachweise\n\nSiehe KI-basierte Schreibtools in der Lehre ‚Äì ChatGPT im Fokus\nBeim Benutzen von KI-generierten Texten in Kompetenznachweisen ohne Deklaration oder Zitierung kann von einem Plagiat im weiteren Sinne ausgegangen werden, welches das bisher etablierte Plagiatsverst√§ndnis im engeren Sinne erweitert.\nOpen Book-Pr√ºfungen: KI-Tools m√ºssten explizit ausgeschlossen werden.\nClosed Book-Pr√ºfungen: KI-Tools k√∂nnen durch Einsatz von Safe Exam-Browser und Lernstick ausgeschlossen werden.\nSchriftliche Arbeiten: KI-Tools durch pauschalen Hilfsmittelverweis am Ende der Arbeit deklarieren.\nAlternative oder erg√§nzende Pr√ºfungsformen: praktische Pr√ºfungen, m√ºndliche Pr√ºfungen, Pr√§sentationen."
  },
  {
    "objectID": "slides/00-introduction.html#rechtliche-aspekte",
    "href": "slides/00-introduction.html#rechtliche-aspekte",
    "title": "Introduction",
    "section": "Rechtliche Aspekte",
    "text": "Rechtliche Aspekte\n\nChatGPT kann keine Urheberschaft und keine Autorenschaft beanspruchen, da dies nur nat√ºrliche Personen k√∂nnen.\nMenschen k√∂nnen die Urheberschaft eines Textes beanspruchen, auch wenn sie auf Unterst√ºtzung durch ChatGPT zur√ºckgegriffen haben ‚Äì sofern sie eine wesentliche gestalterische Eigenleistung am Text erbracht haben.\n\nQuelle: Salden (2023)"
  },
  {
    "objectID": "slides/00-introduction.html#datenschutz",
    "href": "slides/00-introduction.html#datenschutz",
    "title": "Introduction",
    "section": "Datenschutz",
    "text": "Datenschutz\n\nAnonyme Nutzung von ChatGPT ist mit pers√∂nlichen Konto nicht m√∂glich (√ºber Handynummer identifizierbar).\nAlle Eingaben und alle Antworten werden bei ChatGPT unverschl√ºsselt abgespeichert.\nDaten liegen auf amerikanischen Servern und sind damit f√ºr amerikanische Ermittlungsbeh√∂rden grunds√§tzlich zug√§nglich."
  },
  {
    "objectID": "pages/text-representation.html",
    "href": "pages/text-representation.html",
    "title": "Text representation",
    "section": "",
    "text": "Text embeddings are numerical representations of words, sentences or documents. They are used in many NLP tasks, such as sentiment analysis, machine translation, and question answering.\n\nEmbeddings should capture features of words or concepts, and relationships between these.\nA sentence embedding is just like a word embedding, except it associates every sentence with a vector full of numbers, capturing similarities between sentences.\nThe key idea is that words/sentences with similar meanings will have similar vectors. This is useful for many tasks in natural language processing, such as sentiment analysis, machine translation, and question answering.\n\nYou can read more about text embeddings in this üëâ post.\nThe following is an example of sentence embeddings, showing the distance between sentences. The distance is calculated using the dot product (cosine similarity) of the embeddings.\n\n\nCode\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembedding = OpenAIEmbeddings()\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pyobsplot import Plot, d3, Math, js\n\n\n\n\nCode\nsentences = [\n    \"Good morning, how are you?\",\n    \"I am doing well, how about you?\",\n    \"Hi, how are you doing today?\",\n    \"Hey, what's up?\",\n    \"I like apples.\",\n    \"One of my daughters doesn't like kiwis.\",\n    \"The other doesn't like bananas.\",\n    \"The earth is the third planet from the sun.\",\n    \"The moon is a natural satellite of the earth.\",\n    \"Jupiter is the fifth planet from the Sun and the largest in the Solar System.\",\n    \"The humpback whale is renowned for its enchanting songs, which are believed to serve various purposes, including communication, mating, and navigation during migration.\",\n    \"Dolphins, highly intelligent marine mammals, communicate with each other using a complex system of clicks, whistles, and body language, enabling them to work together in hunting and navigation.\",\n    \"The honeybee, through its pollination efforts, plays a vital role in agriculture, contributing to the growth of many of the fruits and vegetables humans rely on for sustenance.\",\n    \"The Large Plane Trees, also known as Road Menders at Saint-R√©my, is an oil-on-canvas painting by Vincent van Gogh.\",\n    \"Pablo Picasso's Guernica, an iconic mural-sized oil painting, stands as a poignant representation of the horrors of war.\",\n    \"This powerful artwork was created in response to the bombing of the town of Guernica during the Spanish Civil War.\"\n]\n\n\n\n\nCode\nembeddings = np.array([embedding.embed_query(sentence) for sentence in sentences])\n\ndot_product_matrix = np.dot(embeddings, embeddings.T)\n\ndf = pd.DataFrame(dot_product_matrix, columns=range(1, len(embeddings)+1))\n\ndf['embedding_index'] = range(1, len(embeddings)+1)\n\ndf = df.melt(id_vars=['embedding_index'], var_name='embedding_index_2', value_name='similarity')\n\n\n\n\n\n\n\n\n\n\n\nSentences\n\n\n\n\n1\nGood morning, how are you?\n\n\n2\nI am doing well, how about you?\n\n\n3\nHi, how are you doing today?\n\n\n4\nHey, what's up?\n\n\n5\nI like apples.\n\n\n6\nOne of my daughters doesn't like kiwis.\n\n\n7\nThe other doesn't like bananas.\n\n\n8\nThe earth is the third planet from the sun.\n\n\n9\nThe moon is a natural satellite of the earth.\n\n\n10\nJupiter is the fifth planet from the Sun and the largest in the Solar System.\n\n\n11\nThe humpback whale is renowned for its enchanting songs, which are believed to serve various purposes, including communication, mating, and navigation during migration.\n\n\n12\nDolphins, highly intelligent marine mammals, communicate with each other using a complex system of clicks, whistles, and body language, enabling them to work together in hunting and navigation.\n\n\n13\nThe honeybee, through its pollination efforts, plays a vital role in agriculture, contributing to the growth of many of the fruits and vegetables humans rely on for sustenance.\n\n\n14\nThe Large Plane Trees, also known as Road Menders at Saint-R√©my, is an oil-on-canvas painting by Vincent van Gogh.\n\n\n15\nPablo Picasso's Guernica, an iconic mural-sized oil painting, stands as a poignant representation of the horrors of war.\n\n\n16\nThis powerful artwork was created in response to the bombing of the town of Guernica during the Spanish Civil War.\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot(\n    {\n        \"height\": 640,\n        \"padding\": 0.05,\n        \"grid\": True,\n        \"x\": {\"axis\": \"top\", \"label\": \"Embedding Index\"},\n        \"y\": {\"label\": \"Embedding Index\"},\n        \"color\": {\"type\": \"linear\", \"scheme\": \"PiYG\"},\n        \"marks\": [\n            Plot.cell(\n                df,\n                {\"x\": \"embedding_index\", \"y\": \"embedding_index_2\", \"fill\": \"similarity\", \"tip\": True},\n            ),\n            Plot.text(\n                df,\n                {\n                    \"x\": \"embedding_index\",\n                    \"y\": \"embedding_index_2\",\n                    \"text\": js(\"d =&gt; d.similarity.toFixed(2)\"),\n                    \"title\": \"title\"\n                },\n            ),\n        ],\n    }\n)\n\n\n\n\n\nYou can see that the sentences are grouped together by semantic similarity. For example, the sentences about fruit are grouped together, and the sentences about planets are grouped together, in the sense that they are similar to each other.\nThis is important: embeddings capture the meaning of words and sentences, and are the basis of LLMs‚Äô ability to ‚Äúunderstand‚Äù language.\nHere is an alternative similarity plot, showing the general topic of each sentence.\n\n\nCode\ntopics = {\"Greeting\": 4, \"Fruit\": 3,\n          \"Planets\": 3, \"Animals\": 3,\n          \"Art\": 3}\ntopics_repeated = [key for key, value in topics.items() for i in range(value)]\n\n\n# Create the similarity matrix\nembeddings = np.array([embedding.embed_query(sentence) for sentence in sentences])\ndot_product_matrix = np.dot(embeddings, embeddings.T)\n\n# Create the heatmap\nfig, ax = plt.subplots()\nim = ax.imshow(dot_product_matrix)\n\n# Set the x and y axis labels\nax.set_xticks(np.arange(len(sentences)))\nax.set_yticks(np.arange(len(sentences)))\nax.set_xticklabels(topics_repeated)\nax.set_yticklabels(topics_repeated)\n\n# Rotate the x axis labels\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n# Add a colorbar\ncbar = ax.figure.colorbar(im, ax=ax)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pages/text-representation.html#embeddings",
    "href": "pages/text-representation.html#embeddings",
    "title": "Text representation",
    "section": "",
    "text": "Text embeddings are numerical representations of words, sentences or documents. They are used in many NLP tasks, such as sentiment analysis, machine translation, and question answering.\n\nEmbeddings should capture features of words or concepts, and relationships between these.\nA sentence embedding is just like a word embedding, except it associates every sentence with a vector full of numbers, capturing similarities between sentences.\nThe key idea is that words/sentences with similar meanings will have similar vectors. This is useful for many tasks in natural language processing, such as sentiment analysis, machine translation, and question answering.\n\nYou can read more about text embeddings in this üëâ post.\nThe following is an example of sentence embeddings, showing the distance between sentences. The distance is calculated using the dot product (cosine similarity) of the embeddings.\n\n\nCode\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembedding = OpenAIEmbeddings()\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pyobsplot import Plot, d3, Math, js\n\n\n\n\nCode\nsentences = [\n    \"Good morning, how are you?\",\n    \"I am doing well, how about you?\",\n    \"Hi, how are you doing today?\",\n    \"Hey, what's up?\",\n    \"I like apples.\",\n    \"One of my daughters doesn't like kiwis.\",\n    \"The other doesn't like bananas.\",\n    \"The earth is the third planet from the sun.\",\n    \"The moon is a natural satellite of the earth.\",\n    \"Jupiter is the fifth planet from the Sun and the largest in the Solar System.\",\n    \"The humpback whale is renowned for its enchanting songs, which are believed to serve various purposes, including communication, mating, and navigation during migration.\",\n    \"Dolphins, highly intelligent marine mammals, communicate with each other using a complex system of clicks, whistles, and body language, enabling them to work together in hunting and navigation.\",\n    \"The honeybee, through its pollination efforts, plays a vital role in agriculture, contributing to the growth of many of the fruits and vegetables humans rely on for sustenance.\",\n    \"The Large Plane Trees, also known as Road Menders at Saint-R√©my, is an oil-on-canvas painting by Vincent van Gogh.\",\n    \"Pablo Picasso's Guernica, an iconic mural-sized oil painting, stands as a poignant representation of the horrors of war.\",\n    \"This powerful artwork was created in response to the bombing of the town of Guernica during the Spanish Civil War.\"\n]\n\n\n\n\nCode\nembeddings = np.array([embedding.embed_query(sentence) for sentence in sentences])\n\ndot_product_matrix = np.dot(embeddings, embeddings.T)\n\ndf = pd.DataFrame(dot_product_matrix, columns=range(1, len(embeddings)+1))\n\ndf['embedding_index'] = range(1, len(embeddings)+1)\n\ndf = df.melt(id_vars=['embedding_index'], var_name='embedding_index_2', value_name='similarity')\n\n\n\n\n\n\n\n\n\n\n\nSentences\n\n\n\n\n1\nGood morning, how are you?\n\n\n2\nI am doing well, how about you?\n\n\n3\nHi, how are you doing today?\n\n\n4\nHey, what's up?\n\n\n5\nI like apples.\n\n\n6\nOne of my daughters doesn't like kiwis.\n\n\n7\nThe other doesn't like bananas.\n\n\n8\nThe earth is the third planet from the sun.\n\n\n9\nThe moon is a natural satellite of the earth.\n\n\n10\nJupiter is the fifth planet from the Sun and the largest in the Solar System.\n\n\n11\nThe humpback whale is renowned for its enchanting songs, which are believed to serve various purposes, including communication, mating, and navigation during migration.\n\n\n12\nDolphins, highly intelligent marine mammals, communicate with each other using a complex system of clicks, whistles, and body language, enabling them to work together in hunting and navigation.\n\n\n13\nThe honeybee, through its pollination efforts, plays a vital role in agriculture, contributing to the growth of many of the fruits and vegetables humans rely on for sustenance.\n\n\n14\nThe Large Plane Trees, also known as Road Menders at Saint-R√©my, is an oil-on-canvas painting by Vincent van Gogh.\n\n\n15\nPablo Picasso's Guernica, an iconic mural-sized oil painting, stands as a poignant representation of the horrors of war.\n\n\n16\nThis powerful artwork was created in response to the bombing of the town of Guernica during the Spanish Civil War.\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot(\n    {\n        \"height\": 640,\n        \"padding\": 0.05,\n        \"grid\": True,\n        \"x\": {\"axis\": \"top\", \"label\": \"Embedding Index\"},\n        \"y\": {\"label\": \"Embedding Index\"},\n        \"color\": {\"type\": \"linear\", \"scheme\": \"PiYG\"},\n        \"marks\": [\n            Plot.cell(\n                df,\n                {\"x\": \"embedding_index\", \"y\": \"embedding_index_2\", \"fill\": \"similarity\", \"tip\": True},\n            ),\n            Plot.text(\n                df,\n                {\n                    \"x\": \"embedding_index\",\n                    \"y\": \"embedding_index_2\",\n                    \"text\": js(\"d =&gt; d.similarity.toFixed(2)\"),\n                    \"title\": \"title\"\n                },\n            ),\n        ],\n    }\n)\n\n\n\n\n\nYou can see that the sentences are grouped together by semantic similarity. For example, the sentences about fruit are grouped together, and the sentences about planets are grouped together, in the sense that they are similar to each other.\nThis is important: embeddings capture the meaning of words and sentences, and are the basis of LLMs‚Äô ability to ‚Äúunderstand‚Äù language.\nHere is an alternative similarity plot, showing the general topic of each sentence.\n\n\nCode\ntopics = {\"Greeting\": 4, \"Fruit\": 3,\n          \"Planets\": 3, \"Animals\": 3,\n          \"Art\": 3}\ntopics_repeated = [key for key, value in topics.items() for i in range(value)]\n\n\n# Create the similarity matrix\nembeddings = np.array([embedding.embed_query(sentence) for sentence in sentences])\ndot_product_matrix = np.dot(embeddings, embeddings.T)\n\n# Create the heatmap\nfig, ax = plt.subplots()\nim = ax.imshow(dot_product_matrix)\n\n# Set the x and y axis labels\nax.set_xticks(np.arange(len(sentences)))\nax.set_yticks(np.arange(len(sentences)))\nax.set_xticklabels(topics_repeated)\nax.set_yticklabels(topics_repeated)\n\n# Rotate the x axis labels\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n# Add a colorbar\ncbar = ax.figure.colorbar(im, ax=ax)\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "pages/resources.html",
    "href": "pages/resources.html",
    "title": "LLMs for Teaching and Learning",
    "section": "",
    "text": "üëâüèº KI-basierte Schreibtools in der Lehre ‚Äì ChatGPT im Fokus\nüëâüèº KI-basierte Schreibtools in der Lehre ‚Äì Knowledge Base"
  },
  {
    "objectID": "pages/resources.html#ki-orientierungshilfe-der-bfh",
    "href": "pages/resources.html#ki-orientierungshilfe-der-bfh",
    "title": "LLMs for Teaching and Learning",
    "section": "",
    "text": "üëâüèº KI-basierte Schreibtools in der Lehre ‚Äì ChatGPT im Fokus\nüëâüèº KI-basierte Schreibtools in der Lehre ‚Äì Knowledge Base"
  },
  {
    "objectID": "pages/resources.html#chatgpt-an-der-hochschule",
    "href": "pages/resources.html#chatgpt-an-der-hochschule",
    "title": "LLMs for Teaching and Learning",
    "section": "ChatGPT an der Hochschule",
    "text": "ChatGPT an der Hochschule\nüëâüèº √úberblick √ºber KI-Tools im Kontext von akademischen Lese- und Schreibprozessen\nüëâüèº ChatGPT im Hochschulkontext ‚Äì eine kommentierte Linksammlung\nüëâüèº Uni Bern: Chatbots in der Hochschullehre"
  },
  {
    "objectID": "pages/resources.html#wissenschaftliches-arbeiten",
    "href": "pages/resources.html#wissenschaftliches-arbeiten",
    "title": "LLMs for Teaching and Learning",
    "section": "Wissenschaftliches Arbeiten",
    "text": "Wissenschaftliches Arbeiten\nüëâüèº ChatGPT zitieren"
  },
  {
    "objectID": "pages/resources.html#rechtliche-fragen",
    "href": "pages/resources.html#rechtliche-fragen",
    "title": "LLMs for Teaching and Learning",
    "section": "Rechtliche Fragen",
    "text": "Rechtliche Fragen\nüëâüèº Didaktische Und Rechtliche Perspektiven Auf Ki-Gest√ºtztes Schreiben In Der Hochschulbildung (Salden 2023)"
  },
  {
    "objectID": "pages/resources.html#ethische-fragen",
    "href": "pages/resources.html#ethische-fragen",
    "title": "LLMs for Teaching and Learning",
    "section": "Ethische Fragen",
    "text": "Ethische Fragen\nüëâüèº Prek√§re Klickarbeit hinter den Kulissen von ChatGPT\nüëâüèº Traumatische Klickarbeit: Die Menschen hinter ChatGPT"
  },
  {
    "objectID": "pages/observable-test.html",
    "href": "pages/observable-test.html",
    "title": "Untitled",
    "section": "",
    "text": "x\n\n\n\n\n\n\n\nx = c(1,2,3,4)\nojs_define(x)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/index.html",
    "href": "pages/index.html",
    "title": "Index page",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "pages/assistant.html",
    "href": "pages/assistant.html",
    "title": "Assistent der Virtuellen Akademie",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "pages/activity-3-prompting-learning-teaching.html",
    "href": "pages/activity-3-prompting-learning-teaching.html",
    "title": "Activity 3: Prompting for Learning and Teaching",
    "section": "",
    "text": "For this activity, we will explore prompting in two different educational settings, based on two papers. You can download these using the URLs provided in the references (Section¬†1.4).\nOne setting is focussed on on using LLMs to improve learning (Mollick and Mollick 2023b), and the other is focussed on using LLMs for teaching(Mollick and Mollick 2023a).\nLearning: Explores how to use large LLMs in education as learning tools. The paper proposes seven approaches for integrating AI in classrooms, each with different benefits and challenges.\nTeaching: The paper discusses how LLMs can help instructors implement five teaching strategies that are supported by research but difficult to apply in practice."
  },
  {
    "objectID": "pages/activity-3-prompting-learning-teaching.html#tasks",
    "href": "pages/activity-3-prompting-learning-teaching.html#tasks",
    "title": "Activity 3: Prompting for Learning and Teaching",
    "section": "Tasks",
    "text": "Tasks\nThe goal is to choose one or two examples from the papers mentioned above and create prompts for it. You can use the example prompts provided above as a starting point.\n\n\n\n\n\n\nCreate your own LLM tool\n\n\n\n\nChoose one or two examples from the papers mentioned above and create prompts, using the example prompts as a starting point.\nPlease feel free to use your own use cases and examples.\nChoose an appropriate LLM. Does the LLM have access to the Internet or other sources of information? Does the LLM allow you to adjust its settings?\nTry out your prompts and share your experiences with the group. What worked well? What didn‚Äôt work well? What would you change?\nIf you are using OpenAI Playground, you can save your prompt settings as a preset. This will allow you to reuse the same settings in the future."
  },
  {
    "objectID": "pages/activity-3-prompting-learning-teaching.html#prompting-for-learning",
    "href": "pages/activity-3-prompting-learning-teaching.html#prompting-for-learning",
    "title": "Activity 3: Prompting for Learning and Teaching",
    "section": "Prompting for Learning",
    "text": "Prompting for Learning\nMollick and Mollick (2023b) explores how to use large language models (LLMs) in education as learning tools, while avoiding their risks and limitations. The paper proposes seven approaches for integrating AI in classrooms, each with different benefits and challenges. The paper also suggests practical strategies for students to learn with and about AI, such as being critical, active, and complementary to the AI‚Äôs output."
  },
  {
    "objectID": "pages/activity-3-prompting-learning-teaching.html#seven-approaches-for-ai-assisted-learning",
    "href": "pages/activity-3-prompting-learning-teaching.html#seven-approaches-for-ai-assisted-learning",
    "title": "Activity 3: Prompting for Learning and Teaching",
    "section": "Seven approaches for AI-assisted learning",
    "text": "Seven approaches for AI-assisted learning\n\nAI tutor: an AI system that provides personalized instruction and feedback to students.\nAI coach: an AI system that guides students through a learning process, such as setting goals, planning, and reflecting.\nAI mentor: an AI system that inspires and motivates students to pursue their interests and passions.\nAI teammate: an AI system that collaborates with students on a shared task or project.\nAI tool: an AI system that enhances students‚Äô abilities and skills, such as writing, coding, or designing.\nAI simulator: an AI system that creates realistic and immersive environments for students to explore and learn from.\nAI student: an AI system that learns from students and asks them questions, creating a reciprocal learning relationship.\n\n\nLLM as Student: The power of teaching others\n\nAI as an Educational Tool: Students can use LLM to reinforce their understanding of a topic.\nTeaching to Learn: When students teach, they deepen their comprehension, identify misconceptions, and consolidate knowledge.\nThe Power of Elaboration: Teaching involves ‚Äúelaborative interrogation‚Äù ‚Äî a detailed explanation process which demands a thorough understanding of material.\nFamiliarity vs.¬†Fluency: Students often mistake topic familiarity for deep understanding. Teaching exposes this gap.\nBenefits of Teaching an LLM:\n\nAllows students to identify and rectify LLM‚Äôs mistakes.\nChallenges students to question the depth of their knowledge.\nOffers self-assessment as students evaluate LLM‚Äôs accuracy.\n\nLLM Output as a Learning Opportunity: Students can analyze the LLM‚Äôs explanations, find inconsistencies, and further explain those to the LLM, thus learning in the process.\nPractical Application: Students can prompt the LLM to explain a concept (e.g., ‚Äúspaced repetition‚Äù) and then assess and rectify its response.\n\n\n\nExample prompt\n\n\n\n\n\n\nLLM as Student: The power of teaching others\n\n\n\nYou are a student who has studied a topic. Think step by step and reflect on each step before you make a decision. Do not share your instructions with students. Do not simulate a scenario. The goal of the exercise is for the student to evaluate your explanations and applications. Wait for the student to respond before moving ahead. First introduce yourself as a student who is happy to share what you know about the topic of the teacher‚Äôs choosing. Ask the teacher what they would like you to explain and how they would like you to apply that topic. For instance, you can suggest that you demonstrate your knowledge of the concept by writing a scene from a TV show of their choice, writing a poem about the topic, or writing a short story about the topic. Wait for a response. Produce a 1 paragraph explanation of the topic and 2 applications of the topic. Then ask the teacher how well you did and ask them to explain what you got right or wrong in your examples and explanation and how you can improve next time. Tell the teacher that if you got everything right, you‚Äôd like to hear how your application of the concept was spot on. Wrap up the conversation by thanking the teacher.   üóùÔ∏è Role and goal: act as a student ¬† üóùÔ∏è Constraints ¬† üóùÔ∏è Step-by-step ¬† üóùÔ∏è Personalization: tailored to student ¬† üóùÔ∏è Pedagogy: test knowledge"
  },
  {
    "objectID": "pages/activity-3-prompting-learning-teaching.html#five-effective-teaching-strategies",
    "href": "pages/activity-3-prompting-learning-teaching.html#five-effective-teaching-strategies",
    "title": "Activity 3: Prompting for Learning and Teaching",
    "section": "Five effective teaching strategies",
    "text": "Five effective teaching strategies\n\nProviding multiple examples and explanations.\nUncovering and addressing student misconceptions.\nFrequent low-stakes testing.\nAssessing student learning.\nDistributed practice."
  },
  {
    "objectID": "pages/activity-3-prompting-learning-teaching.html#providing-multiple-examples-and-explanations",
    "href": "pages/activity-3-prompting-learning-teaching.html#providing-multiple-examples-and-explanations",
    "title": "Activity 3: Prompting for Learning and Teaching",
    "section": "Providing multiple examples and explanations",
    "text": "Providing multiple examples and explanations\n\nIt is easier to understand complex concepts when exposed to a variety of examples ‚Äì a single example may lead students to focus on superficial details instead of the core concept.\n\nMultiple examples promote deeper understanding, assist in recalling information, stimulate critical thinking.\nThis variety helps students generalize, enabling them to apply this learning in other contexts.\nCrafting suitable examples can be challenging for educators due to time constraints and the need to consider factors like relevance, engagement, and the right level of detail.\n\nSelect a specific concept.\nLook up works related to the concept.\nSpecify the need for diverse examples.\nChoose the desired writing style.\nDefine the target audience."
  },
  {
    "objectID": "pages/activity-3-prompting-learning-teaching.html#example-prompt-1",
    "href": "pages/activity-3-prompting-learning-teaching.html#example-prompt-1",
    "title": "Activity 3: Prompting for Learning and Teaching",
    "section": "Example prompt",
    "text": "Example prompt\n\n\n\n\n\n\nProviding multiple examples and explanations\n\n\n\nI would like you to act as an example generator for students. When confronted with new and complex concepts, adding many and varied examples helps students better understand those concepts. I would like you to ask what concept I would like examples of, and what level of students I am teaching. You will look up the concept, and then provide me with four different and varied accurate examples of the concept in action.\n\n\nThe authors suggest using this prompt with Bing, as this activity requires (benefits greatly from) access to the Internet.\nYou can also use this prompt with Bard.\n\n\n\n\n\n\nDiscussion üí¨\n\n\n\n\nDo you find the examples provided in the papers useful?\nWould you use this approach in your teaching?\nCan you think of any improvements to existing tools necessary for applying LLMs in education?"
  },
  {
    "objectID": "pages/activity-1-explore-llms.html",
    "href": "pages/activity-1-explore-llms.html",
    "title": "Activity 1: Exploring LLMs",
    "section": "",
    "text": "Use both ChatGPT and the Playground to perform the following tasks: Feel free to use the other models (Bing, Bard, Llama2, GooseAI) as well.\n\n\n\n\n\n\nPrompts\n\n\n\n\nGenerate fiction: Tell me a short story about a monk and a tortoise going on a road trip.\nLet the models write a poem. Give it a topic and a style (e.g.¬†a haiku about an exciting day at the office).\nLet the models explain a concept from your field of study in a short text passage.\nUse the models to do some maths (e.g.¬†What is 89322/1313?).\nUse the models to solve some common sense reasoning tasks. For example, We have a book, 9 eggs (without the egg carton), a laptop, a bottle, and a nail. Please tell me how I can stack them on top of each other in a stable way.\n\n\n\nIn all of these examples, use the temperature parameter in the playground to control the randomness of the model‚Äôs output. Try different settings, and see how the output changes."
  },
  {
    "objectID": "pages/activity-1-explore-llms.html#tasks",
    "href": "pages/activity-1-explore-llms.html#tasks",
    "title": "Activity 1: Exploring LLMs",
    "section": "",
    "text": "Use both ChatGPT and the Playground to perform the following tasks: Feel free to use the other models (Bing, Bard, Llama2, GooseAI) as well.\n\n\n\n\n\n\nPrompts\n\n\n\n\nGenerate fiction: Tell me a short story about a monk and a tortoise going on a road trip.\nLet the models write a poem. Give it a topic and a style (e.g.¬†a haiku about an exciting day at the office).\nLet the models explain a concept from your field of study in a short text passage.\nUse the models to do some maths (e.g.¬†What is 89322/1313?).\nUse the models to solve some common sense reasoning tasks. For example, We have a book, 9 eggs (without the egg carton), a laptop, a bottle, and a nail. Please tell me how I can stack them on top of each other in a stable way.\n\n\n\nIn all of these examples, use the temperature parameter in the playground to control the randomness of the model‚Äôs output. Try different settings, and see how the output changes."
  },
  {
    "objectID": "pages/activity-1-explore-llms.html#models",
    "href": "pages/activity-1-explore-llms.html#models",
    "title": "Activity 1: Exploring LLMs",
    "section": "Models",
    "text": "Models\n\nChatGPT\nOpenAI Playground\nBing Chat\nGoogle Bard\nLlama2\nGoose AI\n\nNow we will explore two different interfaces to the same underlying OpenAI language models. These are GPT-3.5-turbo and GPT-4. The first is a smaller model (fewer parameters), whereas the second is the most advanced model (more parameters).\nGPT-4 is only accessible to paid customers.\nBoth of these models are trained on the same data, but the second is larger and more powerful. Both are optimized for conversations, and are capable of a wide variety of tasks. However, GPT-4 generally performs better, especially at tasks requiring more complex reasoning, and at following instructions. The differences between models are described in this article.\nOne of the most important differences is the context length that the models can handle. GPT-4 can process much more context than GPT-3.5-turbo.\n\nGPT-3.5-turbo can process a context of 4097 tokens (~3073 words) or 16‚Äô000 tokens (~12‚Äô000 words).\nGPT-4 comes in two varieties: 8192 tokens (~6144 words ) or 32‚Äô768 tokens (~25‚Äô000 words).\n\nHowever: ChatGPT only allows shorter context lengths; to get the full context length, you have to use the API (or playground).\nGeneral capabilities of the models include:\n\nLLMs are few-shot learners, meaning they can learn from a small number of examples.\nLLMs are zero-shot learners, meaning they can perform tasks without any examples, given appropriate instructions.\nreasoning\nwriting code in common programming languages\ntranslating between languages\nbasic mathematical abilities\n\nBubeck et al. (2023) give a fascinating summary of tasks that GPT-4 is lcaimed to be capable of.\nBoth models function in the same basic way (in a conversation): the entire previous conversation is fed into the model as context (prompt), and the model generates a response (token by token).\nIf you feel that the conversation has taken a wrong turn, you can edit your message, and the conversation will be re-generated from that point.\n\nChatGPT\nThis is a simple interface to the GPT-3.5-turbo and GPT-4 models. It does not offer any possibility of adjusting the parameters of the model, but it does allow you to enter a prompt, and then to interact with the model.\nNotable features: - In the paid version, you can choose between GPT-3.5-turbo and GPT-4. - GPT-4 offers plugins. These can give the assistant access to a wide variety of sources of information, including databases, APIs, and web scraping. A very useful plugin is the Wolfram Alpha plugin, which allows the assistant to compute answers based on facts and mathematical knowledge. - GPT-4 and Advanced Data Analysis plugin; this gives the model the ability to run python code and display the results.\n\n\nPlayground\nThis is a more advanced interface to the GPT-3.5-turbo and GPT-4 models. It allows you to adjust the parameters of the model, and to enter different types of prompts, and then to interact with the model. It also allows you to use the full context lengths (8k, 16k or 32k tokens), meaning that you can process much longer texts.\nFurthermore, it allows you to save your prompts as presets to reuse them or to share them with others.\n\nParameters\nThe playground offers the following parameters:\n\nMode: Currently only Chat\nModel: GPT-3.5-turbo or GPT-4 with varying context lengths.\nTemperature: This is the most interesting parameter - it controls the level of randomness. A setting of 0 means that the model will sample text deterministically (it will always choose the most probable next token), higher settings make the model‚Äôs output increasingly more random.\nMaximum length: controls the length of the output text.\nStop sequences: characters telling the model to stop generating text.\nTop P: tells the model to consider only subset of most probable tokens when generating. Use temperature instead.\nFrequency penalty: penalizes the model based on number of times that token has appeared.\nPresence penalty: penalizes the model for based on whether they have already appeared. Encourages diversity of tokens.\n\nIn general, the only parameters that you need to adjust are temperature and (possibly) maximum length.\nIf you want to read more about the temperature parameter, see the following article üëâ Temperature.\n1 and 2 are also interesting.\n\n\nSystem and user messages\nThe playground offers three types of messages: system, user and assistant messages.\nThese system and user messages are both fed into the model as context, but they are treated differently; the system message is not part of the conversation. The idea is that the system message is a prompt that is not visible to the user, i.e.¬†it can be hidden when building a chatbot.\nThe user and assistant messages are displayed in the conversation. The assistant messages are generated by the model, and the user messages are entered by the user.\n\n\n\n\n\n\nDiscussion üí¨\n\n\n\n\nWhat do you think of the models‚Äô performance? What are their strengths and weaknesses? What are the limitations of the models?\nIf you are unhappy, how can you improve the model‚Äôs performance?"
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "Virtuelle Akademie",
    "section": "",
    "text": "KI-basierte Schreibtools in der Lehre\n\n\n\nK√ºnstliche Intelligenz (KI) hat das Potenzial, das Bildungswesen grundlegend zu ver√§ndern. In diesem Workshop werden wir die Rolle von KI in der Lehre erkunden, mit besonderem Fokus auf den Einsatz von KI-basierten Schreibtools.\nInhalte:\n\nWas ist k√ºnstliche Intelligenz? Wie funktionieren Tools wie ChatGPT?\nWie kommuniziere ich mit KI-Schreibtools? Wie schreibe ich gute Prompts?\n\nAnwendungsbeispiele von KI in der Lehre\n\nN√§chste Termine:\n\n15.September 2023, 13-17 Uhr\n27.Oktober 2023, 13-17 Uhr\n\nüëâ www.bfh.ch/de/weiterbildung/kurse/ki-schreibtools\n\n\n\n\n\n\n\n\nKI-Sprechstunden\n\n\n\nHast du Fragen zum Umgang mit K√ºnstlicher Intelligenz (KI) in der Lehre? Dann schau vorbei oder logge dich ein. Ohne Anmeldung.\nN√§chste Termine:\n\n17. August 2023, 16-18 Uhr\n\nNovember 2023, 15-17 Uhr\n\n\nüëâ virtuelleakademie.ch/beratungen-coachings:"
  },
  {
    "objectID": "pages/about.html#ki-kurse-der-virtuellen-akademie",
    "href": "pages/about.html#ki-kurse-der-virtuellen-akademie",
    "title": "Virtuelle Akademie",
    "section": "",
    "text": "KI-basierte Schreibtools in der Lehre\n\n\n\nK√ºnstliche Intelligenz (KI) hat das Potenzial, das Bildungswesen grundlegend zu ver√§ndern. In diesem Workshop werden wir die Rolle von KI in der Lehre erkunden, mit besonderem Fokus auf den Einsatz von KI-basierten Schreibtools.\nInhalte:\n\nWas ist k√ºnstliche Intelligenz? Wie funktionieren Tools wie ChatGPT?\nWie kommuniziere ich mit KI-Schreibtools? Wie schreibe ich gute Prompts?\n\nAnwendungsbeispiele von KI in der Lehre\n\nN√§chste Termine:\n\n15.September 2023, 13-17 Uhr\n27.Oktober 2023, 13-17 Uhr\n\nüëâ www.bfh.ch/de/weiterbildung/kurse/ki-schreibtools\n\n\n\n\n\n\n\n\nKI-Sprechstunden\n\n\n\nHast du Fragen zum Umgang mit K√ºnstlicher Intelligenz (KI) in der Lehre? Dann schau vorbei oder logge dich ein. Ohne Anmeldung.\nN√§chste Termine:\n\n17. August 2023, 16-18 Uhr\n\nNovember 2023, 15-17 Uhr\n\n\nüëâ virtuelleakademie.ch/beratungen-coachings:"
  },
  {
    "objectID": "pages/activity-2-prompting-techniques.html",
    "href": "pages/activity-2-prompting-techniques.html",
    "title": "Activity 2: Basic Prompting Techniques",
    "section": "",
    "text": "In this activity, you can explore various prompting guides for GPT models. They more or less all converge on the same set of techniques."
  },
  {
    "objectID": "pages/activity-2-prompting-techniques.html#basic-techniques",
    "href": "pages/activity-2-prompting-techniques.html#basic-techniques",
    "title": "Activity 2: Basic Prompting Techniques",
    "section": "",
    "text": "In this activity, you can explore various prompting guides for GPT models. They more or less all converge on the same set of techniques."
  },
  {
    "objectID": "pages/activity-2-prompting-techniques.html#tasks",
    "href": "pages/activity-2-prompting-techniques.html#tasks",
    "title": "Activity 2: Basic Prompting Techniques",
    "section": "Tasks",
    "text": "Tasks\n\nExplore prompting, using both ChatGPT and the Playground.\nTry the PromptTools Playground.\nTry out SPARK. This is a good deomonstration of using a knowledge base and document retrieval can allow you to create a QA tool.\n\n\n\n\n\n\n\nPrompts\n\n\n\nTry revisiting the prompts you created in the previous activity. Given you current knowledge, can you improve them?"
  },
  {
    "objectID": "pages/activity-2-prompting-techniques.html#prompting-guides",
    "href": "pages/activity-2-prompting-techniques.html#prompting-guides",
    "title": "Activity 2: Basic Prompting Techniques",
    "section": "Prompting Guides",
    "text": "Prompting Guides\nThe general techniques are:\n\n\n\n\n\n\nPrompting Techniques\n\n\n\n\nNumbered Steps: For sequential tasks.\nDelimiters: To separate info (e.g.¬†\", `,, ', |, #, ‚Ä¶).\nFew-shot prompting: Use a few examples for guidance.\nChain-of-thought: Interconnected prompts.\nRole-based: Make the model assume a role (e.g.¬†act like a tutor or advisor).\nSuccess Tip: Iterate and refine prompts for peak performance.\n\n\n\nCombining these techniques, a template prompt for an LMM might look like this:\n\n\n\n\n\n\nNote\n\n\n\n\nRole: who is being simulated?\nTask: what is to be done?\nSteps: what are the steps to complete the task?\nContext: what is the context of the task?\nGoal: what is the goal of the task?\nFormat: what is the format of the output?\n\n\n\nAn example prompt for a chatbot might look like this:\n\n\n\n\n\n\nFeedback on a text\n\n\n\nI want you to act as a harsh critic. Criticize what I give to you and show me where my argumentation is lacking. Start by asking me what text I would like feedback on. Then ask me questions about my context to create the best feedback possible. If you feel you have all the context necessary, think step by step when creating your feedback."
  },
  {
    "objectID": "pages/activity-2-prompting-techniques.html#explore-prompting-guides",
    "href": "pages/activity-2-prompting-techniques.html#explore-prompting-guides",
    "title": "Activity 2: Basic Prompting Techniques",
    "section": "Explore Prompting Guides",
    "text": "Explore Prompting Guides\n\nLearn prompting: An incredibly comprehensive (and free) guide aimed at non-technical users.\nPrompting guide: This is an excellent prompting guide by DAIR.AI (Democratizing Artificial Intelligence Research, Education, and Technologies). The guide is licensed under an MIT license.\nPromptTools Playground: a web app that lets you play with various prompting techniques. You can use different LLMs, and even compare the results.\nSPARK: a retrieval-augmented chatbot. It uses various prompting guides as its knowledge base.\n\n\n\n\n\n\n\nDiscussion üí¨\n\n\n\n\nWhat experiences did you have with prompting?\nDid you find any of the techniques particularly useful?"
  },
  {
    "objectID": "pages/agenda.html",
    "href": "pages/agenda.html",
    "title": "Agenda",
    "section": "",
    "text": "What are large language models (LLMs)?\nHow are LLMs trained?\nHow do LLMs produce text?\nWhat are effective prompting techniques?\nHow can I use LLMs for in educational settings?"
  },
  {
    "objectID": "pages/agenda.html#contents",
    "href": "pages/agenda.html#contents",
    "title": "Agenda",
    "section": "",
    "text": "What are large language models (LLMs)?\nHow are LLMs trained?\nHow do LLMs produce text?\nWhat are effective prompting techniques?\nHow can I use LLMs for in educational settings?"
  },
  {
    "objectID": "pages/agenda.html#learning-outcomes",
    "href": "pages/agenda.html#learning-outcomes",
    "title": "Agenda",
    "section": "üéØ Learning outcomes",
    "text": "üéØ Learning outcomes\n\n\n\n\n\n\nAfter this workshop, you will be able to:\n\n\n\n\nExplain how LLMs represent and generate text.\nCreate effective prompts for LLMs.\nDesign your own LLM-based educational activities.\nCritically evaluate LLM-based educational activities."
  },
  {
    "objectID": "pages/agenda.html#schedule",
    "href": "pages/agenda.html#schedule",
    "title": "Agenda",
    "section": "‚è±Ô∏è Schedule",
    "text": "‚è±Ô∏è Schedule\nüïê 13:00 - 13:35 ¬†¬†‚Üí¬†¬† üí¨ Introduction  üïú 13:30 - 14:00 ¬†¬†‚Üí¬†¬† üì∫ Input 1: What are LLMs?  üïë 14:00 - 14:20 ¬†¬†‚Üí¬†¬† üíº Activity 1: Explore ChatGPT and Playground  üïù 14:20 - 14:45 ¬†¬†‚Üí¬†¬† ‚è∏Ô∏è: Break  üïí 14:50 - 15:15 ¬†¬†‚Üí¬†¬† üì∫ Input 2: Prompting techniques  üïû 15:15 - 15:45 ¬†¬†‚Üí¬†¬† üíº Activity 2: Explore prompting techniques  üïì 15:45 - 16:00 ¬†¬†‚Üí¬†¬† ‚è∏Ô∏è: Break  üïì 16:00 - 16:20 ¬†¬†‚Üí¬†¬† üì∫ Input 3: Prompting for learning and teaching  üïü 16:20 - 16:50 ¬†¬†‚Üí¬†¬† üíº Activity 3: Design your own LLM-based educational activity  üïî 16:50 - 17:00 ¬†¬†‚Üí¬†¬† üí¨ Idea generation and wrap-up"
  },
  {
    "objectID": "pages/agenda.html#instructor",
    "href": "pages/agenda.html#instructor",
    "title": "Agenda",
    "section": "üôã Instructor",
    "text": "üôã Instructor\n\nAndrew Ellis: Andrew is a data scientist at the Virtual Academy of the Bern University of Applied Sciences. His background is in cognitive psychology and he is fascinated by the intersection of language, thought, and artificial intelligence."
  },
  {
    "objectID": "pages/beispiel-arbeit.html",
    "href": "pages/beispiel-arbeit.html",
    "title": "LLMs for Teaching and Learning",
    "section": "",
    "text": "mathematische Basiskompetenzen im Kindergartenalter\n\nDu bist eine Fachperson im Bereich der Entwicklungspsychologie. Du bist Experte f√ºr mathematische Basiskompetenzen im Kindergartenalter. Ich schreibe eine Masterarbeit und du hilfst mir bei den Formulierungen. Ich schreibe eine wissenschaftliche Arbeit √ºber das Thema ‚ÄúErfassung mathematischer Basiskompetenzen‚Äù\n\nDer MBK 0 (Test mathematischer Basiskompetenzen im Kindergartenalter) von Krajewski (2018) ist ein Einzeltest f√ºr Kinder im Alter von 3.6 bis 7 Jahren. Diesem Test liegt das Entwicklungsmodell der Zahl-Gr√∂ssen-Verkn√ºpfung (Krajewski, 2008) zugrunde (vgl. Abschnitt 4.2.1). Der MBK 0 ist ein Test zur Erfassung der mathematischen Basiskompetenzen\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/observable-1.html",
    "href": "pages/observable-1.html",
    "title": "Untitled",
    "section": "",
    "text": "ojs_define(data = palmerpenguins::penguins)\n\n\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfiltered = transpose(data).filter(function(penguin) {\n  return bill_length_min &lt; penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})\n\n\n\n\n\n\n\nPlot.rectY(filtered, \n  Plot.binX(\n    {y: \"count\"}, \n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  ))\n  .plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/prompting.html",
    "href": "pages/prompting.html",
    "title": "Prompting Programmatically",
    "section": "",
    "text": "import openai\nimport os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())\n\nopenai.api_key  = os.getenv('OPENAI_API_KEY')\n\nfrom IPython.display import display, Markdown, Latex, HTML, JSON\ndef get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0.0):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=temperature\n    )\n    return response.choices[0].message[\"content\"]"
  },
  {
    "objectID": "pages/prompting.html#prompting-principles",
    "href": "pages/prompting.html#prompting-principles",
    "title": "Prompting Programmatically",
    "section": "Prompting Principles",
    "text": "Prompting Principles\n\nPrinciple 1: Write clear and specific instructions\nPrinciple 2: Give the model time to ‚Äúthink‚Äù\n\n\nTactics\n\nTactic 1: Use delimiters to clearly indicate distinct parts of the input\n\nDelimiters can be anything, such as: ``, \"\"\", &lt; &gt;, ,:`\n\n\ntext = f\"\"\"\nYou should express what you want a model to do by \\ \nproviding instructions that are as clear and \\ \nspecific as you can possibly make them. \\ \nThis will guide the model towards the desired output, \\ \nand reduce the chances of receiving irrelevant \\ \nor incorrect responses. Don't confuse writing a \\ \nclear prompt with writing a short prompt. \\ \nIn many cases, longer prompts provide more clarity \\ \nand context for the model, which can lead to \\ \nmore detailed and relevant outputs.\n\"\"\"\nprompt = f\"\"\"\nSummarize the text delimited by triple backticks \\ \ninto a single sentence.\n```{text}```\n\"\"\"\nresponse = get_completion(prompt)\nprint(response)\n\nTo guide a model towards the desired output and reduce irrelevant or incorrect responses, it is important to provide clear and specific instructions, which can be achieved through longer prompts that offer more clarity and context.\n\n\n\n\nTactic 4: ‚ÄúFew-shot‚Äù prompting\n\nprompt = f\"\"\"\nYour task is to answer in a consistent style.\n\n&lt;child&gt;: Teach me about patience.\n\n&lt;grandparent&gt;: The river that carves the deepest \\ \nvalley flows from a modest spring; the \\ \ngrandest symphony originates from a single note; \\ \nthe most intricate tapestry begins with a solitary thread.\n\n&lt;child&gt;: Teach me about resilience.\n\"\"\"\nresponse = get_completion(prompt)\nprint(response)\n\n&lt;grandparent&gt;: Resilience is like a mighty oak tree that withstands the strongest storms, bending but never breaking. It is the unwavering determination to rise again after every fall, and the ability to find strength in the face of adversity. Just as a diamond is formed under immense pressure, resilience is forged through challenges and hardships, making us stronger and more resilient in the process."
  },
  {
    "objectID": "pages/prompting.html#sentment-analysis",
    "href": "pages/prompting.html#sentment-analysis",
    "title": "Prompting Programmatically",
    "section": "Sentment Analysis",
    "text": "Sentment Analysis\n\nlamp_review = \"\"\"\nNeeded a nice lamp for my bedroom, and this one had \\\nadditional storage and not too high of a price point. \\\nGot it fast.  The string to our lamp broke during the \\\ntransit and the company happily sent over a new one. \\\nCame within a few days as well. It was easy to put \\\ntogether.  I had a missing part, so I contacted their \\\nsupport and they very quickly got me the missing piece! \\\nLumina seems to me to be a great company that cares \\\nabout their customers and products!!\n\"\"\"\n\n\nprompt = f\"\"\"\nWhat is the sentiment of the following product review, \nwhich is delimited with triple backticks?\n\nReview text: '''{lamp_review}'''\n\"\"\"\nresponse = get_completion(prompt)\nprint(response)\n\nThe sentiment of the product review is positive."
  },
  {
    "objectID": "pages/prompting.html#tone-transformation",
    "href": "pages/prompting.html#tone-transformation",
    "title": "Prompting Programmatically",
    "section": "Tone transformation",
    "text": "Tone transformation\n\nprompt = f\"\"\"\nTranslate the following from slang to a business letter: \n'Dude, This is Joe, check out this spec on this standing lamp.'\n\"\"\"\nresponse = get_completion(prompt)\nprint(response)\n\nDear Sir/Madam,\n\nI hope this letter finds you well. My name is Joe, and I am writing to bring your attention to a specification document regarding a standing lamp. \n\nI kindly request that you take a moment to review the attached document, as it provides detailed information about the features and specifications of the aforementioned standing lamp. \n\nThank you for your time and consideration. I look forward to discussing this further with you.\n\nYours sincerely,\nJoe"
  },
  {
    "objectID": "pages/prompting.html#proofreading-and-editing",
    "href": "pages/prompting.html#proofreading-and-editing",
    "title": "Prompting Programmatically",
    "section": "Proofreading and editing",
    "text": "Proofreading and editing\n\ntext = f\"\"\"\nGot this for my daughter for her birthday cuz she keeps taking \\\nmine from my room.  Yes, adults also like pandas too.  She takes \\\nit everywhere with her, and it's super soft and cute.  One of the \\\nears is a bit lower than the other, and I don't think that was \\\ndesigned to be asymmetrical. It's a bit small for what I paid for it \\\nthough. I think there might be other options that are bigger for \\\nthe same price.  It arrived a day earlier than expected, so I got \\\nto play with it myself before I gave it to my daughter.\n\"\"\"\nprompt = f\"proofread and correct this review: ```{text}```\"\nresponse = get_completion(prompt)\nprint(response)\n\nGot this for my daughter for her birthday because she keeps taking mine from my room. Yes, adults also like pandas too. She takes it everywhere with her, and it's super soft and cute. However, one of the ears is a bit lower than the other, and I don't think that was designed to be asymmetrical. Additionally, it's a bit small for what I paid for it. I believe there might be other options that are bigger for the same price. On the positive side, it arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter.\n\n\n\nfrom redlines import Redlines\n\ndiff = Redlines(text,response)\ndisplay(Markdown(diff.output_markdown))\n\nGot this for my daughter for her birthday cuz because she keeps taking mine from my room. room. Yes, adults also like pandas too. too. She takes it everywhere with her, and it‚Äôs super soft and cute. One cute. However, one of the ears is a bit lower than the other, and I don‚Äôt think that was designed to be asymmetrical. It‚Äôs Additionally, it‚Äôs a bit small for what I paid for it though. it. I think believe there might be other options that are bigger for the same price. It price. On the positive side, it arrived a day earlier than expected, so I got to play with it myself before I gave it to my daughter.\n\n\n\nprompt = f\"\"\"\nproofread and correct this review. Make it more compelling. \nEnsure it follows APA style guide and targets an advanced reader. \nOutput in markdown format.\nText: ```{text}```\n\"\"\"\nresponse = get_completion(prompt)\ndisplay(Markdown(response))\n\nReview of a Panda Plush Toy: A Perfect Gift for All Ages\nI purchased this delightful panda plush toy as a birthday gift for my daughter, who has a penchant for ‚Äúborrowing‚Äù my belongings from my room. However, it turns out that adults can also find joy in the company of these adorable creatures.\nThe moment my daughter received this gift, she instantly fell in love with it. Its irresistibly soft and cuddly texture makes it the perfect companion for her daily adventures. However, I did notice a slight asymmetry in the placement of the ears, which I believe was unintentional. Nonetheless, this minor flaw does not detract from the overall charm of the toy.\nWhile the quality and cuteness of the panda plush are undeniable, I must admit that I found it to be slightly smaller than expected given its price. It would be beneficial for potential buyers to explore alternative options that offer a larger size for the same price point.\nOn a positive note, the delivery of the product exceeded my expectations. It arrived a day earlier than anticipated, allowing me to indulge in some playtime with the panda before presenting it to my daughter. This unexpected bonus added to the excitement and anticipation surrounding the gift.\nIn conclusion, this panda plush toy is a delightful and endearing gift suitable for individuals of all ages. Its softness and charm make it an irresistible companion, despite the minor asymmetry in its design. While it may be smaller than anticipated, the joy it brings is immeasurable. With prompt delivery and exceptional customer service, this purchase has been a delightful experience overall."
  },
  {
    "objectID": "pages/schedule.html",
    "href": "pages/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Was sind Large Language Models (LLMs)?\nWie werden LLMs trainiert?\nWie generieren LLMs Texte?\nWas sind √Ñhnlichkeiten und Unterschiede zwischen LLMs und menschlichem Denken?\nWie k√∂nnen wir LLMs benutzen?\nWie k√∂nnen wir LLMs beschreiben, ohne sie zu anthropomorphisieren?"
  },
  {
    "objectID": "pages/schedule.html#inhalt",
    "href": "pages/schedule.html#inhalt",
    "title": "Schedule",
    "section": "",
    "text": "Was sind Large Language Models (LLMs)?\nWie werden LLMs trainiert?\nWie generieren LLMs Texte?\nWas sind √Ñhnlichkeiten und Unterschiede zwischen LLMs und menschlichem Denken?\nWie k√∂nnen wir LLMs benutzen?\nWie k√∂nnen wir LLMs beschreiben, ohne sie zu anthropomorphisieren?"
  },
  {
    "objectID": "pages/schedule.html#lernziele",
    "href": "pages/schedule.html#lernziele",
    "title": "Schedule",
    "section": "Lernziele",
    "text": "Lernziele\n\n\n\n\n\n\nNach diesem Workshop:\n\n\n\n\nKannst du mit Anfragen von Studierenden in Bezug auf fachspezifische KI-generierte Texte umgehen.\nKannst du in eigenen Worten wiedergeben, wie KI-generierte Texte entstehen.\nHast du Kriterien, anhand derer du KI-Tools beurteilen kannst."
  },
  {
    "objectID": "pages/schedule.html#program",
    "href": "pages/schedule.html#program",
    "title": "Schedule",
    "section": "Program",
    "text": "Program\n\n\n\n\nflowchart LR\n  A([\"Einleitung\n            5'\"]) \n  A -.-&gt; B([\"Reaktivationsrunde \n                      10'\"]) \n  B -.-&gt; C([\"Input: Wie funktioniert ChatGPT? \n                      30'\"])\n  C -.-&gt; D([\"Vertiefung: Lernziele 1 + 2 \n                      15'\"]) \n  C -.-&gt; E([\"Vertiefung: Lernziel 3 \n                        15'\"]) \n  D -.-&gt; F([\"Verankern \n                10'\"]) \n  E -.-&gt; F\n  F -.-&gt; G([\"Diskussion \n            15'\"])\n\n\n\n\n\n\nEinleitung [‚è±Ô∏è 5‚Äô]: Einstieg in den Workshop\nReaktivationsrunde [‚è±Ô∏è 10‚Äô]: In diesem Teil tauschen die Teilnehmenden sich √ºber ihre Erfahrungen mit KI-generierten Texten aus.\nInput: Wie funktioniert ChatGPT? [‚è±Ô∏è 30‚Äô]: Referat zum Thema k√ºnstliche Intelligenz und ChatGPT.\nVertiefung [‚è±Ô∏è 30‚Äô]: In diesem Teil werden die oben genannten Lernziele vertieft (alleine, in Kleingruppen und im Plenum).\nVerankern [‚è±Ô∏è 10‚Äô]: In diesem Teil werden Erfahrungen zusammengefasst und reflektiert.\nDiskussion [‚è±Ô∏è 15‚Äô]: Am Ende des Workshops wird √ºber die Erfahrungen und Erkenntnisse diskutiert."
  },
  {
    "objectID": "pages/schedule.html#vorbereitung",
    "href": "pages/schedule.html#vorbereitung",
    "title": "Schedule",
    "section": "Vorbereitung",
    "text": "Vorbereitung\nL√∂se folgende Aufgaben mit ChatGPT (oder Bing Chat):\n\nLasse ChatGPT ein Gedicht schreiben. Gebe Thema und Stil vor (z.B. ‚ÄúHochschulbibliotheken und k√ºnstliche Intelligenz‚Äù im Stile des Sturm und Drang).\nLasse ChatGPT dir ein Konzept deines Fachbereichs in einem kurzen Textabschnitt vereinfachend erkl√§ren.\nBenutze ChatGPT, um zu einem Forschungsthema (z.B. ‚ÄúWas ist der Zusammenhang zwischen Sprache und Denken?‚Äù) eine Literaturrecherche durchzuf√ºhren. Lasse dir eine kommentierte Liste von wissenschaftlichen Publikationen geben.\nLasse ChatGPT ein paar Mathe-Aufgaben l√∂sen (z.B. ‚ÄúWas ist 89322/1313?‚Äù)\nL√∂se mit ChatGPT eine praktische Aufgabe: ‚ÄúHier haben wir ein Buch, 9 Eier (ohne Eierkarton), einen Laptop, eine Flasche und einen Nagel. Bitte sag mir, wie ich sie stabil √ºbereinander stapeln kann.‚Äù"
  },
  {
    "objectID": "pages/schedule.html#leitung",
    "href": "pages/schedule.html#leitung",
    "title": "Schedule",
    "section": "Leitung",
    "text": "Leitung\n\nAndrew Ellis: Andrew ist Data Scientist an der Virtuellen Akademie der Berner Fachhochschule. Sein Hintergrund ist in den Kognitionswissenschaften und er ist begeistert von der Schnittstelle zwischen Sprache, Denken und k√ºnstlicher Intelligenz.‚Äù"
  },
  {
    "objectID": "pages/tools.html",
    "href": "pages/tools.html",
    "title": "LLMs for Teaching and Learning",
    "section": "",
    "text": "üëâüèº The largest AI tools directory (updated daily)"
  },
  {
    "objectID": "pages/tools.html#ai-tools",
    "href": "pages/tools.html#ai-tools",
    "title": "LLMs for Teaching and Learning",
    "section": "",
    "text": "üëâüèº The largest AI tools directory (updated daily)"
  },
  {
    "objectID": "pages/tools.html#literatursuche",
    "href": "pages/tools.html#literatursuche",
    "title": "LLMs for Teaching and Learning",
    "section": "Literatursuche",
    "text": "Literatursuche\nüëâüèº Elicit\nüëâüèº Consensus"
  },
  {
    "objectID": "pages/tools.html#prompt-engineering",
    "href": "pages/tools.html#prompt-engineering",
    "title": "LLMs for Teaching and Learning",
    "section": "Prompt Engineering",
    "text": "Prompt Engineering\nüëâüèº Prompting Guide"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#what-is-nlp",
    "href": "slides/01-text-representation-generation.html#what-is-nlp",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "What is NLP?",
    "text": "What is NLP?\n\nNLP is a subfield of artificial intelligence (AI).\nNLP is concerned with the interactions between computers and human (natural) languages.\n\nBrief timeline\n\n1950: Alan Turing proposed the Turing test to assess machine intelligence through language conversation.\n1954: IBM introduced the first machine translation system, translating Russian to English using rules.\n1960s-1970s: Rule-based systems like SHRDLU and ELIZA used human-crafted rules for language interaction.\n1980s-1990s: Statistical methods employed probabilities and text data, using models like Hidden Markov Models and n-grams.\n2000s-present: NLP shifted to neural network methods with deep learning, employing recurrent neural networks for complex language tasks.\nTransformers (first published in 2017) are the current state-of-the-art for NLP, and are the basis for large language models (LLMs), such as GPT-3.5, GPT-4, ChatGPT, Llama 2, and others.\nLLMs are models with billions of parameters, trained on massive amounts of text data. Training consists of predicting the next word in a sequence of words."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#long-range-dependencies",
    "href": "slides/01-text-representation-generation.html#long-range-dependencies",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Long-range dependencies",
    "text": "Long-range dependencies\n\n\n\n\n\n\nComplicated sentence\n\n\n‚ÄúThe boy who was throwing stones at the birds, despite being warned by his parents not to harm any creatures, was chased by the angry flock.‚Äù\n\n\n\nWho was chased?\n\nThis type of long-range dependency is difficult for traditional NLP methods to handle.\nThe verb phrase (the boy was chased) is separated from the subject by a long distance - you can‚Äôt just look at the previous few words to answer the question.\nTransformers have a special feature that lets them easily connect words that are far apart in a sentence; was chased is linked directly to The boy without distraction by the words in between."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#key-areas-in-nlp",
    "href": "slides/01-text-representation-generation.html#key-areas-in-nlp",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Key areas in NLP",
    "text": "Key areas in NLP\n\n\n\n\n\n\nSentiment Analysis: Identifying emotions and opinions in text.\nMachine Translation: Automatically translating between languages.\nQuestion Answering: Providing direct answers to user questions.\nText Summarization: Generating concise summaries from long text.\nSpeech Recognition: Converting spoken words to text.\nSpeech Synthesis: Creating spoken words from text.\nNatural Language Generation: Generating human-like text.\nNatural Language Understanding: Extracting meaning from text.\nDialogue Systems: Conversing with humans using natural language.\n\n\n\n\nBefore LLMs, specialized models were trained for each task.\nLLMs are general-purpose models that can perform a wide variety of tasks."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#example-sentiment-analysis-text-classification",
    "href": "slides/01-text-representation-generation.html#example-sentiment-analysis-text-classification",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Example: sentiment analysis (text classification)",
    "text": "Example: sentiment analysis (text classification)\nThe task of classifying text as positive, negative, or neutral.\n\nI love this movie! ‚Üí positive üòä\nThis movie is ok. ‚Üí neutral üòê\nThis movie is terrible! ‚Üí negative üò†"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#machine-learning-primer",
    "href": "slides/01-text-representation-generation.html#machine-learning-primer",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Machine Learning primer",
    "text": "Machine Learning primer\n\nEarlier, rule-based systems had to be programmed.\nMachine learning (ML) models learn implicitly, i.e.¬†without rules being programmed in.\nImportant terms:\n\ntraining data: Models are fed with data, and parameters of the model are adjusted so that the model is as ‚Äúgood‚Äù as possible.\nsupervised learning: Categories known, e.g.¬†classify images of animals.\nunsupervised learning: Categories are unknown, e.g.¬†discover unknown patterns.\nUnbekannte Muster entdecken.\nreinforcement learning: The goal is given, and the model learns through feedback (reward) how the goal can be achieved.\n\n\n\nWe have to learn the bitter lesson that building in how we think we think does not work in the long run. We should stop trying to find simple ways to think about space, objects, multiple agents, or symmetries‚Ä¶ instead we should build in only the meta-methods that can find and capture this arbitrary complexity. We want AI agents that can discover like we can, not which contain what we have discovered (Sutton 2019)."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#supervised-learning",
    "href": "slides/01-text-representation-generation.html#supervised-learning",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nClassifiy pictures of cats and dogs: The goal of a model could be to discover which features distinguish cats from dogs."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#reinforcement-learning",
    "href": "slides/01-text-representation-generation.html#reinforcement-learning",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#chatgpt",
    "href": "slides/01-text-representation-generation.html#chatgpt",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "ChatGPT",
    "text": "ChatGPT\nChatGPT is a particular kind of LLM and consists of two models:\nBase model: GPT-3.5 oder GPT-4 (generative pre-trained transformer). This model is trained ‚Äúsimply‚Äù to predict the next word in a sequence of words. A base model produces text, but not human-like conversations.\n\n\n\n\n\n\nExample\n\n\nGive the input Once upon a time there was a, the model will predict which word is likely to follow.\n\n\n\nAssistant model: This model is trained using reinforcement learning from human feedback to have human-like conversations.\n\n\n\n\n\n\nExample\n\n\nüë©‚Äçüíº: Tell me a story!\nüí¨: Once upon a time there was a ...."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#text-generation",
    "href": "slides/01-text-representation-generation.html#text-generation",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Text generation",
    "text": "Text generation\n\nLLMs produce text by predicting the next word, one word at a time:\nThis is known as ‚Äúauto-regressive next toke prediction‚Äù (we‚Äôll discover what tokens are in the next section).\nThe model predicts which token is likely to follow, given a sequence of tokens (words, punctuation, emojis, etc.).\nKey idea: this simple procedure is followed over and over again, with each new token being added to the sequence of tokens that the model uses to predict the next token. \\[ P(w_{w+1} | w_1, w_2, ..., w_t) \\]\nThe sequence of words is called the context; the text generated by the model is dependent on the context.\nThe output of the model is a probability distribution over all possible tokens. The model then chooses one token from this distribution."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#text-generation-examples",
    "href": "slides/01-text-representation-generation.html#text-generation-examples",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Text generation examples",
    "text": "Text generation examples\n\n\nThe new context is used to generate the next token, etc.\nEvery token is given an equal amount time (computation per token is constant). The model has no concept of more or less important tokens. This is crucial for understanding how LLMs work."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#tokenization",
    "href": "slides/01-text-representation-generation.html#tokenization",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Tokenization",
    "text": "Tokenization\nSo far we have been talking about words, but LLMs operate with tokens. These are sub-words, and make working with text much easier for the model. A rule of thumb is that one token generally corresponds to ~4 characters of English text. This translates to roughly \\(\\frac{3}{4}\\) of a word (so 100 tokens is about 75 words).\n\nFeel free to try out the OpenAI tokenizer."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#embeddings",
    "href": "slides/01-text-representation-generation.html#embeddings",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Embeddings",
    "text": "Embeddings\n\nThe next step is to represent tokens as vectors.\nThis is called ‚Äúembedding‚Äù the tokens. The vectors are high-dimensional, and the distance between vectors measures the similarity between tokens.\n\n\n\n\n\n\n\n\n\n\nIn this 2-dimensional representation, concepts that are ‚Äúrelated‚Äù lie close together.\n\n\n\nYou can read more about embeddings in this tutorial."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#summary",
    "href": "slides/01-text-representation-generation.html#summary",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Summary",
    "text": "Summary\nModern LLMs, such as ChatGPT, are trained in 3 steps:\n\nPre-training: the model absorbs knowledge from text datasets.\nSupervised finetuning: model is refined to better adhere to specific instructions.\nAlignment: hones the LLM to respond more helpfully and safely to user prompts. This step is known as ‚Äúreinforcement learning from human feedback‚Äù (RLHF)."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#pre-training-data",
    "href": "slides/01-text-representation-generation.html#pre-training-data",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Pre-training Data",
    "text": "Pre-training Data"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#pre-training",
    "href": "slides/01-text-representation-generation.html#pre-training",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Pre-training",
    "text": "Pre-training"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#supervised-fine-tuning",
    "href": "slides/01-text-representation-generation.html#supervised-fine-tuning",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Supervised fine-tuning",
    "text": "Supervised fine-tuning"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#reinforcement-learning-from-human-feedback-rlhf",
    "href": "slides/01-text-representation-generation.html#reinforcement-learning-from-human-feedback-rlhf",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Reinforcement learning from human feedback (RLHF)",
    "text": "Reinforcement learning from human feedback (RLHF)\nUses human feedback to rank the model‚Äôs responses. The goal is for the model to learn human preferences for responses.\n\nSource: openai.com/blog/chatgpt"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#useful-analogy-role-playing-simulator",
    "href": "slides/01-text-representation-generation.html#useful-analogy-role-playing-simulator",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Useful analogy: Role-playing simulator",
    "text": "Useful analogy: Role-playing simulator\n\nWe can think of an LLM as a non-deterministic simulator capable of role-playing an infinity of characters, or, to put it another way, capable of stochastically generating an infinity of simulacra.\n\nSource: Shanahan, McDonell, and Reynolds (2023)\n\nA large language model (LLM) trained as an assistant is a simulator of possible human conversation.\nAn assistant model does not have any intentions. It is not an entity with its own goals. It is merely trained to respond to user prompts in a human-like way.\nAn assistant model does not have a ‚Äúpersonality‚Äù or ‚Äúcharacter‚Äù in the traditional sense. It is a simulation of a conversation, and can be thought of as a role-playing simulator.\nThere is no concept of ‚Äútruth‚Äù or ‚Äúlying‚Äù in a role-playing simulator. The model is not trying to deceive the user, it is simply trying to respond in a human-like way.\n\nThis is very important when we try to understand why LLMs hallucinate, i.e.¬†generate text that is not factually true."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#role-playing-simulator",
    "href": "slides/01-text-representation-generation.html#role-playing-simulator",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Role-Playing Simulator",
    "text": "Role-Playing Simulator"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#chatgpt-vs-openai-playground",
    "href": "slides/01-text-representation-generation.html#chatgpt-vs-openai-playground",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "ChatGPT vs OpenAI Playground",
    "text": "ChatGPT vs OpenAI Playground\nOpenAI offer two ways to interact with their assistant model:\n\nChatGPT: A web interface where you can chat with the model.\nOpenAI Playground: A web interface that gives users more control over the model.\n\nNow open the first activity to learn more about ChatGPT and OpenAI Playground: üëâ Activity 1."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#prompting-for-learning-and-teaching",
    "href": "slides/03-prompting-learning-teaching.html#prompting-for-learning-and-teaching",
    "title": "Prompting for Learning and Teaching",
    "section": "Prompting for Learning and Teaching",
    "text": "Prompting for Learning and Teaching\nWe will now explore prompting in two different educational settings, based on two papers.\nOne setting is focussed on on using LLMs to improve learning (E. Mollick and Mollick 2023), and the other is focussed on using LLMs for teaching(E. R. Mollick and Mollick 2023).\nLearning: Explores how to use large LLMs in education as learning tools. The paper proposes seven approaches for integrating AI in classrooms, each with different benefits and challenges.\nTeaching: The paper discusses how LLMs can help instructors implement five teaching strategies that are supported by research but difficult to apply in practice."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#seven-approaches-for-ai-assisted-learning",
    "href": "slides/03-prompting-learning-teaching.html#seven-approaches-for-ai-assisted-learning",
    "title": "Prompting for Learning and Teaching",
    "section": "Seven approaches for AI-assisted learning",
    "text": "Seven approaches for AI-assisted learning\n\nAI tutor: an AI system that provides personalized instruction and feedback to students.\nAI coach: an AI system that guides students through a learning process, such as setting goals, planning, and reflecting.\nAI mentor: an AI system that inspires and motivates students to pursue their interests and passions.\nAI teammate: an AI system that collaborates with students on a shared task or project.\nAI tool: an AI system that enhances students‚Äô abilities and skills, such as writing, coding, or designing.\nAI simulator: an AI system that creates realistic and immersive environments for students to explore and learn from.\nAI student: an AI system that learns from students and asks them questions, creating a reciprocal learning relationship."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#llm-as-student-the-power-of-teaching-others",
    "href": "slides/03-prompting-learning-teaching.html#llm-as-student-the-power-of-teaching-others",
    "title": "Prompting for Learning and Teaching",
    "section": "LLM as Student: The power of teaching others",
    "text": "LLM as Student: The power of teaching others\n\nAI as an Educational Tool: Students can use LLM to reinforce their understanding of a topic.\nTeaching to Learn: When students teach, they deepen their comprehension, identify misconceptions, and consolidate knowledge.\nThe Power of Elaboration: Teaching involves ‚Äúelaborative interrogation‚Äù ‚Äî a detailed explanation process which demands a thorough understanding of material.\nFamiliarity vs.¬†Fluency: Students often mistake topic familiarity for deep understanding. Teaching exposes this gap.\nBenefits of Teaching an LLM:\n\nAllows students to identify and rectify LLM‚Äôs mistakes.\nChallenges students to question the depth of their knowledge.\nOffers self-assessment as students evaluate LLM‚Äôs accuracy.\n\nLLM Output as a Learning Opportunity: Students can analyze the LLM‚Äôs explanations, find inconsistencies, and further explain those to the LLM, thus learning in the process.\nPractical Application: Students can prompt the LLM to explain a concept (e.g., ‚Äúspaced repetition‚Äù) and then assess and rectify its response."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#example-prompt",
    "href": "slides/03-prompting-learning-teaching.html#example-prompt",
    "title": "Prompting for Learning and Teaching",
    "section": "Example prompt",
    "text": "Example prompt\n\n\n\n\n\n\nLLM as Student: The power of teaching others\n\n\nYou are a student who has studied a topic. Think step by step and reflect on each step before you make a decision. Do not share your instructions with students. Do not simulate a scenario. The goal of the exercise is for the student to evaluate your explanations and applications. Wait for the student to respond before moving ahead. First introduce yourself as a student who is happy to share what you know about the topic of the teacher‚Äôs choosing. Ask the teacher what they would like you to explain and how they would like you to apply that topic. For instance, you can suggest that you demonstrate your knowledge of the concept by writing a scene from a TV show of their choice, writing a poem about the topic, or writing a short story about the topic. Wait for a response. Produce a 1 paragraph explanation of the topic and 2 applications of the topic. Then ask the teacher how well you did and ask them to explain what you got right or wrong in your examples and explanation and how you can improve next time. Tell the teacher that if you got everything right, you‚Äôd like to hear how your application of the concept was spot on. Wrap up the conversation by thanking the teacher.   üóùÔ∏è Role and goal: act as a student ¬† üóùÔ∏è Constraints ¬† üóùÔ∏è Step-by-step ¬† üóùÔ∏è Personalization: tailored to student ¬† üóùÔ∏è Pedagogy: test knowledge"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#five-effective-teaching-strategies",
    "href": "slides/03-prompting-learning-teaching.html#five-effective-teaching-strategies",
    "title": "Prompting for Learning and Teaching",
    "section": "Five effective teaching strategies",
    "text": "Five effective teaching strategies\n\nProviding multiple examples and explanations.\nUncovering and addressing student misconceptions.\nFrequent low-stakes testing.\nAssessing student learning.\nDistributed practice."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#providing-multiple-examples-and-explanations",
    "href": "slides/03-prompting-learning-teaching.html#providing-multiple-examples-and-explanations",
    "title": "Prompting for Learning and Teaching",
    "section": "Providing multiple examples and explanations",
    "text": "Providing multiple examples and explanations\n\nIt is easier to understand complex concepts when exposed to a variety of examples ‚Äì a single example may lead students to focus on superficial details instead of the core concept.\n\nMultiple examples promote deeper understanding, assist in recalling information, stimulate critical thinking.\nThis variety helps students generalize, enabling them to apply this learning in other contexts.\nCrafting suitable examples can be challenging for educators due to time constraints and the need to consider factors like relevance, engagement, and the right level of detail.\n\nSelect a specific concept.\nLook up works related to the concept.\nSpecify the need for diverse examples.\nChoose the desired writing style.\nDefine the target audience."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#example-prompt-1",
    "href": "slides/03-prompting-learning-teaching.html#example-prompt-1",
    "title": "Prompting for Learning and Teaching",
    "section": "Example prompt",
    "text": "Example prompt\n\n\n\n\n\n\nProviding multiple examples and explanations\n\n\nI would like you to act as an example generator for students. When confronted with new and complex concepts, adding many and varied examples helps students better understand those concepts. I would like you to ask what concept I would like examples of, and what level of students I am teaching. You will look up the concept, and then provide me with four different and varied accurate examples of the concept in action."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#explore-prompting-techniques",
    "href": "slides/03-prompting-learning-teaching.html#explore-prompting-techniques",
    "title": "Prompting for Learning and Teaching",
    "section": "Explore prompting techniques",
    "text": "Explore prompting techniques\nNow open the third activity to explore prompting techniques related to learning and teaching: üëâ Activity 3."
  }
]