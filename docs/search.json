[
  {
    "objectID": "slides/03-prompting-learning-teaching.html#heres-what-happens-when-your-lawyer-uses-chatgpt",
    "href": "slides/03-prompting-learning-teaching.html#heres-what-happens-when-your-lawyer-uses-chatgpt",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Here‚Äôs What Happens When Your Lawyer Uses ChatGPT",
    "text": "Here‚Äôs What Happens When Your Lawyer Uses ChatGPT"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#the-best-prompts-for-chatgpt-the-ultimate-list",
    "href": "slides/03-prompting-learning-teaching.html#the-best-prompts-for-chatgpt-the-ultimate-list",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "The Best Prompts For ChatGPT: The ultimate list",
    "text": "The Best Prompts For ChatGPT: The ultimate list"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#was-ist-k√ºnstliche-intelligenz-1",
    "href": "slides/03-prompting-learning-teaching.html#was-ist-k√ºnstliche-intelligenz-1",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Was ist K√ºnstliche Intelligenz?",
    "text": "Was ist K√ºnstliche Intelligenz?\n\nQuelle: derbund.ch/so-funktioniert-kuenstliche-intelligenz-599276436215\nSpeaker notes go here."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#embed",
    "href": "slides/03-prompting-learning-teaching.html#embed",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Embed",
    "text": "Embed"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#machine-learning",
    "href": "slides/03-prompting-learning-teaching.html#machine-learning",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nRegelbasierte Systeme m√ºssen programmiert werden.\nML Modelle lernen implizit, d.h. ohne Regeln einprogrammiert zu bekommen.\nWichtige Begriffe:\n\nTrainingsdaten: Modelle werden mit Daten gef√ºttert, und Parameter des Modells werden so eingestellt, dass das Modell m√∂glichst ‚Äúgut‚Äù ist.\nSupervised learning: Aufgabe ist bekannt, z.B. Bilder - klassifizieren.\nUnsupervised learning: Unbekannte Muster entdecken.\nReinforcement learning: Ziel ist vorgegeben, Modell lernt durch Feedback (Belohnung) wie Ziel erreicht werden kann.\n\n\n\nWe have to learn the bitter lesson that building in how we think we think does not work in the long run. We should stop trying to find simple ways to think about space, objects, multiple agents, or symmetries‚Ä¶ instead we should build in only the meta-methods that can find and capture this arbitrary complexity. We want AI agents that can discover like we can, not which contain what we have discovered (Sutton 2019)."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#supervised-learning",
    "href": "slides/03-prompting-learning-teaching.html#supervised-learning",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nBilder von Hunden und Katzen klassifizeren: Was sind die Merkmale, die Hunde von Katzen unterscheiden?"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#reinforcement-learning",
    "href": "slides/03-prompting-learning-teaching.html#reinforcement-learning",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEin ‚ÄúAgent‚Äù lernt durch Feedback (Belohnung) wie ein Ziel erreicht werden kann.\n\n\n\nBeispiel f√ºr RL Model: AlphaGo ist das erste Computerprogramm, das einen professionellen (menschlichen) Go-Spieler besiegt hat."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#natural-language-processing",
    "href": "slides/03-prompting-learning-teaching.html#natural-language-processing",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\n\n\n\nSpeech recognition\nText-to-speech synthesis\nMachine translation\nInformation extraction\nInformation retrieval\nQuestion answering\n\n\n\nSentiment analysis\n\nüòä I love this movie!\nüòê This movie is ok.\nüò† This movie is terrible!"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#tokenization",
    "href": "slides/03-prompting-learning-teaching.html#tokenization",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Tokenization",
    "text": "Tokenization\n\nQuelle: State of ChatGPT"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#embeddings",
    "href": "slides/03-prompting-learning-teaching.html#embeddings",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Embeddings",
    "text": "Embeddings\n\nNumerische Tokens werden in einem hochdimensionalen Vektorraum abgebildet.\nDistanz zwischen Vektoren misst √Ñhnlichkeit zwischen Tokens.\n\n\n\n\n\n\nQuelle: Wolfram (2023)\n\n\n\n\n\n\n\n\n\nQuelle: So funktioniert ChatGPT"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#chatgpt",
    "href": "slides/03-prompting-learning-teaching.html#chatgpt",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "ChatGPT",
    "text": "ChatGPT\nBesteht aus 2 Modellen:\n\nLarge language model (LLM): GPT-3.5 oder GPT-4 (generative pre-trained transformer): das eigentliche Sprachmodell\nAssistant: Ein f√ºr Dialoge spezialisiertes Modell"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#llm",
    "href": "slides/03-prompting-learning-teaching.html#llm",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "LLM",
    "text": "LLM\nAufgabe eines LLMs: ‚Äúauto-regressive next word prediction‚Äù (eigentlich ‚Äútoken prediction‚Äù):\n\\[ P(w_{w+1} | w_1, w_2, ..., w_t) \\]\n\nDas n√§chste Wort wird vorhergesagt, basierend auf den vorherigen Worten.\nDiese vorherigen W√∂rter werden als ‚Äúcontext‚Äù bezeichnet."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#assistant",
    "href": "slides/03-prompting-learning-teaching.html#assistant",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Assistant",
    "text": "Assistant\n\nLLM produziert Text, aber nicht menschliche Konversationen.\nWeiteres Training ist erforderlich, damit das Modell lernt, wie ein Mensch ‚ÄúKonversationen‚Äù zu f√ºhren."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#zusammenfassung",
    "href": "slides/03-prompting-learning-teaching.html#zusammenfassung",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\nChatGPT wurde in mehreren Schritten trainiert:\n\nDaten aus dem Internet werden gesammelt.\nPre-training: LLM wird auf diesen Daten trainiert, das n√§chste Wort vorherzusagen.\nReinforcement Learning from Human Feedback: Assistant wird trainiert, menschliche Konversationen zu f√ºhren."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#daten",
    "href": "slides/03-prompting-learning-teaching.html#daten",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Daten",
    "text": "Daten"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#methoden",
    "href": "slides/03-prompting-learning-teaching.html#methoden",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Methoden",
    "text": "Methoden\n\n\nWe trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup. We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides‚Äîthe user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a dialogue format.\nTo create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model responses ranked by quality. To collect this data, we took conversations that AI trainers had with the chatbot. We randomly selected a model-written message, sampled several alternative completions, and had AI trainers rank them. Using these reward models, we can fine-tune the model using Proximal Policy Optimization. We performed several iterations of this process."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#pre-training",
    "href": "slides/03-prompting-learning-teaching.html#pre-training",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Pre-training",
    "text": "Pre-training"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#pre-training-1",
    "href": "slides/03-prompting-learning-teaching.html#pre-training-1",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Pre-training",
    "text": "Pre-training"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#reinforcement-learning-from-human-feedback-rlhf",
    "href": "slides/03-prompting-learning-teaching.html#reinforcement-learning-from-human-feedback-rlhf",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Reinforcement Learning from Human Feedback (RLHF)",
    "text": "Reinforcement Learning from Human Feedback (RLHF)\nBenutzt Feedback vom Menschen um ‚Äúschlechte‚Äù Outputs zu minimieren.\n\nQuelle: openai.com/blog/chatgpt"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#zusammenfassung-1",
    "href": "slides/03-prompting-learning-teaching.html#zusammenfassung-1",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\n\nChatGPT hat einen hohen Energieverbrauch.\nLLM lernt Vorurteile aus den Trainingsdaten.\nToxische Inhalte werden durch Billigarbeiter:innen moderiert."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#energieverbrauch",
    "href": "slides/03-prompting-learning-teaching.html#energieverbrauch",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Energieverbrauch",
    "text": "Energieverbrauch\n\nTraining: ‚ÄúWhat we do know is that training ChatGPT used \\(1.287\\) GWh, roughly equivalent to the consumption of 120 US homes for a year.‚Äù Quelle: Heating up: how much energy does AI use?\nPatterson et al. (2022) sch√§tzen die Trainingskosten auf 502 Tonnen \\(\\text{CO}_2\\) (RLHF w√ºrde etwas mehr kosten, ca. 1% der urspr√ºnglichen Kosten).\nBenutzung: 7 Tonnen \\(\\text{CO}_2\\) pro Tag (Ende Februar). Quelle: How much energy does ChatGPT use?\nDer Energieverbrauch von ChatGPT ist equivalent zu 400-800 US Haushalten. Das ist betr√§chtlich, im Vergleich zu z.B. Kryptow√§hrungen eher gering."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#bias",
    "href": "slides/03-prompting-learning-teaching.html#bias",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Bias",
    "text": "Bias\n\n\n\n\n\nDa LLMs von Texten lernen, die von Menschen geschrieben wurden, k√∂nnen sie auch Vorurteile lernen.\n\nQuelle: Hast du Vorurteile?"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#ethik",
    "href": "slides/03-prompting-learning-teaching.html#ethik",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Ethik",
    "text": "Ethik\n\n\n\n\n\nAuf Grund der grossen Menge von Trainingsdaten, die f√ºr Sprachmodelle ben√∂tigt werden, ist Qualit√§tskontrolle schwierig.\nDiskriminierende oder beleidigende Aussagen werden von einem Chatbot generiert.\nSolche Antworten k√∂nnen als unerw√ºnscht markiert werden.\nToxische Inhalte wie k√∂rperliche und sexuelle Gewalt, Suizide und Tierqu√§lerei, m√ºssen beim Trainieren aus den Antworten gefiltert werden. Dabei mussten angestellte Arbeitskr√§fte f√ºr weniger als 2 Dollar die Stunde teils schockierende Inhalte lesen.\n\nQuelle: Traumatische Klickarbeit"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#wie-generiert-chatgpt-text",
    "href": "slides/03-prompting-learning-teaching.html#wie-generiert-chatgpt-text",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Wie generiert ChatGPT Text?",
    "text": "Wie generiert ChatGPT Text?\n\nLLM: Gegeben einer Input-Sequenz (Kontext) von Tokens (W√∂rter, Teile von W√∂rtern, Satzzeichen, Emojis, etc.), was sind die wahrscheinlichsten n√§chsten Tokens?\nAuto-regressiv: Ein Token wird generiert, wird dem Kontext hinzugef√ºgt.\n\n \n\nDer neue Kontext wird verwendet, um das n√§chste Token zu generieren, etc.\nWichtig: Jedes Token erh√§lt gleich viel Zeit. Es gibt keine Tokens, die mehr oder weniger wichtig sind (Computation per Token ist konstant).\nWeil jedes Token gleich gewichtet wird, kann es an jeder Stelle im Output zu ‚Äúung√ºnstigen‚Äù Pfaden f√ºhren."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#prompt",
    "href": "slides/03-prompting-learning-teaching.html#prompt",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Prompt",
    "text": "Prompt\n\nDer urspr√ºngliche Kontext wird Prompt (Eingabetext) genannt.\nDieser ist entscheidend f√ºr die Qualit√§t der Antwort.\n\n\n\n\n\n\n\nPrompt Beispiel\n\n\n‚ÄúWas ist 89322/1313?‚Äù\nvs.¬†\n‚ÄúWas ist 89322/1313? Arbeite Schritt f√ºr Schritt.‚Äù"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#prompt-1",
    "href": "slides/03-prompting-learning-teaching.html#prompt-1",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Prompt",
    "text": "Prompt"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#role-playing-simulator",
    "href": "slides/03-prompting-learning-teaching.html#role-playing-simulator",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Role-Playing Simulator",
    "text": "Role-Playing Simulator\n\nWe can think of an LLM as a non-deterministic simulator capable of role-playing an infinity of characters, or, to put it another way, capable of stochastically generating an infinity of simulacra.\n\nQuelle: Shanahan, McDonell, and Reynolds (2023)\n\nBei jeder Interaktion mit ChatGPT wird ein Dialog neu simuliert."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#role-playing-simulator-1",
    "href": "slides/03-prompting-learning-teaching.html#role-playing-simulator-1",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Role-Playing Simulator",
    "text": "Role-Playing Simulator"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#role-playing-simulator-2",
    "href": "slides/03-prompting-learning-teaching.html#role-playing-simulator-2",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Role-Playing Simulator",
    "text": "Role-Playing Simulator\n\nEin LLM ist keine Entit√§t mit Handlungsabsichten, sondern ein Simulator von m√∂glichen Konversationen.\nChatGPT hat kein Konzept von Wahrheit, sondern generiert Antworten, die plausibel sind.\nSomit kann ChatGPT weder die Wahrheit sagen noch l√ºgen - diese Konzepte sind f√ºr ein LLM vorerst irrelevant."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#was-kann-chatgpt",
    "href": "slides/03-prompting-learning-teaching.html#was-kann-chatgpt",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Was kann ChatGPT?",
    "text": "Was kann ChatGPT?\n\n\n\n\n\n\n\n\n\n\nWeitere Beispiele: Bubeck et al. (2023)\n\nSehr viel, wenn richtig geprompted"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#denkt-chatgpt",
    "href": "slides/03-prompting-learning-teaching.html#denkt-chatgpt",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Denkt ChatGPT?",
    "text": "Denkt ChatGPT?"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#system-1-vs-system-2",
    "href": "slides/03-prompting-learning-teaching.html#system-1-vs-system-2",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "System 1 vs System 2",
    "text": "System 1 vs System 2\n\nThinking Fast and Slow\nSystem 1: schnell, instinktiv, automatisch\nSystem 2: langsam, deliberativ, anstrengend"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#prompt-engineering",
    "href": "slides/03-prompting-learning-teaching.html#prompt-engineering",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Prompt Engineering",
    "text": "Prompt Engineering\n\nQualit√§t der Antwort h√§ngt sehr von der Qualit√§t des Prompts ab.\n2 M√∂glichkeiten, um die Qualit√§t der Antworten zu verbessern:\n\nIncrementelle Prompts: Schritt f√ºr Schritt durch die Konversation f√ºhren (dialogisches Prompting).\nMega-Prompts: Alle Informationen auf einmal geben.\n\nAm besten selber ausprobieren."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#mega-prompt",
    "href": "slides/03-prompting-learning-teaching.html#mega-prompt",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Mega-Prompt",
    "text": "Mega-Prompt\n\nRolle: Wer oder was wird simuliert?\nAufgabe: Was ist zu tun?\nArbeitschritte: Was ist in welcher Reihenfolge zu tun?\nKontext, Einschr√§nkungen\nZiel: Was soll am Ende herauskommen?\nFormat: Wie soll das Ergebnis aussehen?\n\n\n\n\n\n\n\nBeispiel Hochschullehre: Feedback\n\n\n‚ÄúI want you to act as a harsh critic. Criticize what I will write to you and show me where my argumentation is lacking. Start by asking me what text I want to have feedback on. Then ask me questions about my context to create the best feedback possible. If you feel you have all the context necessary, think step by step when creating your feedback‚Äù (Lenk-Ostendorf & Folgmann 2023)\n\n\n\n\n\n\n\n\n\nWeitere Beispiele\n\n\n\nOpenAI Discord Server discord.com/invite/openai\nPrompting Guide"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#plug-ins",
    "href": "slides/03-prompting-learning-teaching.html#plug-ins",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Plug-ins",
    "text": "Plug-ins"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#retrieval-augmented-llms",
    "href": "slides/03-prompting-learning-teaching.html#retrieval-augmented-llms",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Retrieval-augmented LLMs",
    "text": "Retrieval-augmented LLMs"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#retrieval-augmented-llms-1",
    "href": "slides/03-prompting-learning-teaching.html#retrieval-augmented-llms-1",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Retrieval-augmented LLMs",
    "text": "Retrieval-augmented LLMs\nBeispiel: üëâüèº Assistent der Virtuellen Akademie"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#haltung-der-bfh",
    "href": "slides/03-prompting-learning-teaching.html#haltung-der-bfh",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Haltung der BFH",
    "text": "Haltung der BFH\n\nTechnologien sollen dort, wo sie den Lernprozess unterst√ºtzen und praxisrelevant sind, in die Lehre einbezogen werden.\nStudierende sollen lernen, Technologien kompetent einzusetzen und kritisch zu hinterfragen. Dies gilt uneingeschr√§nkt auch f√ºr ChatGPT und andere gleichgerichtete Tools."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#zitieren",
    "href": "slides/03-prompting-learning-teaching.html#zitieren",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Zitieren",
    "text": "Zitieren\n\nEs existieren noch keine Richtlinien f√ºr das Zitieren von ChatGPT oder anderen KI-basierte Schreibtools.\nChatGPT ist rein rechtlich keine zitierf√§hige Quelle und damit auch nicht zitierpflichtig (Fleck 2023).\nAus der Orientierungshilfe: ‚ÄúKI-basierte Schreibtools sind externe Quellen und m√ºssen daher im Sinne der wissenschaftlichen Integrit√§t immer, wie andere Quellen auch, zitiert werden, sofern ganze Textpassagen von ChatGPT benutzt werden. Falls mit dem Tool der eigene Text √ºberarbeitet wurde, muss ChatGPT als verwendetes Hilfsmittel angef√ºhrt werden.‚Äù\n\n\n\n\n\n\n\nM√∂glicher Pauschalverweis\n\n\n‚ÄúBeim Verfassen der Arbeit habe ich das KI-gest√ºtzte Schreibwerkzeug ChatGPT zur Textoptimierung verwendet. W√∂rtlich aus dem Tool √ºbernommene Passagen wurden im Text als pers√∂nliche Kommunikation zitiert.‚Äù"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#plagiate-und-detektion",
    "href": "slides/03-prompting-learning-teaching.html#plagiate-und-detektion",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Plagiate und Detektion",
    "text": "Plagiate und Detektion\n\nTexte von ChatGPT werden jedes Mal individuell erstellt. Es handelt sich nicht um Plagiate.\nDie klassischen Tools zur Aufdeckung von Plagiaten wie z.B. TurnItIn funktionieren hier nicht.\nDie BFH empfiehlt weiterhin, schriftliche Arbeiten mit der Plagiatserkennungssoftware Turnitin zu pr√ºfen."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#kompetenznachweise",
    "href": "slides/03-prompting-learning-teaching.html#kompetenznachweise",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Kompetenznachweise",
    "text": "Kompetenznachweise\n\nSiehe KI-basierte Schreibtools in der Lehre ‚Äì ChatGPT im Fokus\nBeim Benutzen von KI-generierten Texten in Kompetenznachweisen ohne Deklaration oder Zitierung kann von einem Plagiat im weiteren Sinne ausgegangen werden, welches das bisher etablierte Plagiatsverst√§ndnis im engeren Sinne erweitert.\nOpen Book-Pr√ºfungen: KI-Tools m√ºssten explizit ausgeschlossen werden.\nClosed Book-Pr√ºfungen: KI-Tools k√∂nnen durch Einsatz von Safe Exam-Browser und Lernstick ausgeschlossen werden.\nSchriftliche Arbeiten: KI-Tools durch pauschalen Hilfsmittelverweis am Ende der Arbeit deklarieren.\nAlternative oder erg√§nzende Pr√ºfungsformen: praktische Pr√ºfungen, m√ºndliche Pr√ºfungen, Pr√§sentationen."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#rechtliche-aspekte",
    "href": "slides/03-prompting-learning-teaching.html#rechtliche-aspekte",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Rechtliche Aspekte",
    "text": "Rechtliche Aspekte\n\nChatGPT kann keine Urheberschaft und keine Autorenschaft beanspruchen, da dies nur nat√ºrliche Personen k√∂nnen.\nMenschen k√∂nnen die Urheberschaft eines Textes beanspruchen, auch wenn sie auf Unterst√ºtzung durch ChatGPT zur√ºckgegriffen haben ‚Äì sofern sie eine wesentliche gestalterische Eigenleistung am Text erbracht haben.\n\nQuelle: Salden (2023)"
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#datenschutz",
    "href": "slides/03-prompting-learning-teaching.html#datenschutz",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Datenschutz",
    "text": "Datenschutz\n\nAnonyme Nutzung von ChatGPT ist mit pers√∂nlichen Konto nicht m√∂glich (√ºber Handynummer identifizierbar).\nAlle Eingaben und alle Antworten werden bei ChatGPT unverschl√ºsselt abgespeichert.\nDaten liegen auf amerikanischen Servern und sind damit f√ºr amerikanische Ermittlungsbeh√∂rden grunds√§tzlich zug√§nglich."
  },
  {
    "objectID": "slides/03-prompting-learning-teaching.html#seminararbeit-mit-ki-unterst√ºtzung",
    "href": "slides/03-prompting-learning-teaching.html#seminararbeit-mit-ki-unterst√ºtzung",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Seminararbeit mit KI-Unterst√ºtzung",
    "text": "Seminararbeit mit KI-Unterst√ºtzung\nLehrperson: Kerstin Denecke, Technik und Informatik, Institut f√ºr Medizininformatik\n\nElicit [Brainstorm research questions]: Idee f√ºr Forschungsfrage\nChatGPT: Feedback zu Fragestellungen, Verbesserungsvorschl√§ge\nElicit [Abstract summary]: Rechercheunterst√ºtzung\nConsensus: Unterst√ºtzung f√ºr Belege in der Diskussion\n\n\n\n\n\n\n\nChatGPT Prompt\n\n\nVerhalte dich wie eine wissenschaftliche Betreuungsperson in ihrer Sprechstunde. Du musst den aktuellen Stand meiner Hausarbeit √ºberpr√ºfen und kritisch beurteilen. Dazu evaluierst du meine Forschungsfrage und √§usserst konstruktive Kritik in Bezug auf deren St√§rken und Schw√§chen. Die Fragestellung lautet:\n‚ÄúWie binden Therapeuten Chatbots in die Behandlung ein?‚Äù"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#what-is-natural-language-processing-nlp",
    "href": "slides/01-text-representation-generation.html#what-is-natural-language-processing-nlp",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "What is natural language processing (NLP)?",
    "text": "What is natural language processing (NLP)?\n\nNLP is a subfield of artificial intelligence (AI).\nNLP is concerned with the interactions between computers and human (natural) languages.\n\nBrief timeline\n\n1950: Alan Turing proposed the Turing test to assess machine intelligence through language conversation.\n1954: IBM introduced the first machine translation system, translating Russian to English using rules.\n1960s-1970s: Rule-based systems like SHRDLU and ELIZA used human-crafted rules for language interaction.\n1980s-1990s: Statistical methods employed probabilities and text data, using models like Hidden Markov Models and n-grams.\n2000s-present: NLP shifted to neural network methods with deep learning, employing recurrent neural networks for complex language tasks.\nTransformers (first published in 2017) are the current state-of-the-art for NLP, and are the basis for large language models (LLMs), such as GPT-3.5, GPT-4, ChatGPT, Llama 2, and others.\nLLMs are models with billions of parameters, trained on massive amounts of text data. Training consists of predicting the next word in a sequence of words."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#long-range-dependencies",
    "href": "slides/01-text-representation-generation.html#long-range-dependencies",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Long-range dependencies",
    "text": "Long-range dependencies\n\n\n\n\n\n\nComplicated sentence\n\n\n‚ÄúThe boy who was throwing stones at the birds, despite being warned by his parents not to harm any creatures, was chased by the angry flock.‚Äù\n\n\n\nWho was chased?\n\nThis type of long-range dependency is difficult for traditional NLP methods to handle.\nThe verb phrase (the boy was chased) is separated from the subject by a long distance - you can‚Äôt just look at the previous few words to answer the question.\nTransformers have a special feature that lets them easily connect words that are far apart in a sentence; was chased is linked directly to The boy without distraction by the words in between."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#key-areas-in-nlp",
    "href": "slides/01-text-representation-generation.html#key-areas-in-nlp",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Key areas in NLP",
    "text": "Key areas in NLP\n\n\n\n\n\n\nSentiment Analysis: Identifying emotions and opinions in text.\nMachine Translation: Automatically translating between languages.\nQuestion Answering: Providing direct answers to user questions.\nText Summarization: Generating concise summaries from long text.\nSpeech Recognition: Converting spoken words to text.\nSpeech Synthesis: Creating spoken words from text.\nNatural Language Generation: Generating human-like text.\nNatural Language Understanding: Extracting meaning from text.\nDialogue Systems: Conversing with humans using natural language.\n\n\n\n\nBefore LLMs, specialized models were trained for each task.\nLLMs are general-purpose models that can perform a wide variety of tasks."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#example-sentiment-analysis-text-classification",
    "href": "slides/01-text-representation-generation.html#example-sentiment-analysis-text-classification",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Example: sentiment analysis (text classification)",
    "text": "Example: sentiment analysis (text classification)\nThe task of classifying text as positive, negative, or neutral.\n\nI love this movie! ‚Üí positive üòä\nThis movie is ok. ‚Üí neutral üòê\nThis movie is terrible! ‚Üí negative üò†"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#machine-learning-primer",
    "href": "slides/01-text-representation-generation.html#machine-learning-primer",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Machine Learning primer",
    "text": "Machine Learning primer\n\nEarlier, rule-based systems had to be programmed.\nMachine learning (ML) models learn implicitly, i.e.¬†without rules being programmed in.\nImportant terms:\n\ntraining data: Models are fed with data, and parameters of the model are adjusted so that the model is as ‚Äúgood‚Äù as possible.\nsupervised learning: Categories known, e.g.¬†classify images of animals.\nunsupervised learning: Categories are unknown, e.g.¬†discover unknown patterns.\nUnbekannte Muster entdecken.\nreinforcement learning: The goal is given, and the model learns through feedback (reward) how the goal can be achieved.\n\n\n\nWe have to learn the bitter lesson that building in how we think we think does not work in the long run. We should stop trying to find simple ways to think about space, objects, multiple agents, or symmetries‚Ä¶ instead we should build in only the meta-methods that can find and capture this arbitrary complexity. We want AI agents that can discover like we can, not which contain what we have discovered (Sutton 2019)."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#supervised-learning",
    "href": "slides/01-text-representation-generation.html#supervised-learning",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nClassifiy pictures of cats and dogs: The goal of a model could be to discover which features distinguish cats from dogs."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#reinforcement-learning",
    "href": "slides/01-text-representation-generation.html#reinforcement-learning",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#chatgpt",
    "href": "slides/01-text-representation-generation.html#chatgpt",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "ChatGPT",
    "text": "ChatGPT\nChatGPT is a particular kind of LLM and consists of two models:\nBase model: GPT-3.5 oder GPT-4 (generative pre-trained transformer). This model is trained ‚Äúsimply‚Äù to predict the next word in a sequence of words. A base model produces text, but not human-like conversations.\n\n\n\n\n\n\nExample\n\n\nGive the input Once upon a time there was a, the model will predict which word is likely to follow.\n\n\n\nAssistant model: This model is trained using reinforcement learning from human feedback to have human-like conversations.\n\n\n\n\n\n\nExample\n\n\nüë©‚Äçüíº: Tell me a story!\nüí¨: Once upon a time there was a ...."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#text-generation",
    "href": "slides/01-text-representation-generation.html#text-generation",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Text generation",
    "text": "Text generation\n\nLLMs produce text by predicting the next word, one word at a time:\nThis is known as ‚Äúauto-regressive next toke prediction‚Äù (we‚Äôll discover what tokens are in the next section).\nThe model predicts which token is likely to follow, given a sequence of tokens (words, punctuation, emojis, etc.).\nKey idea: this simple procedure is followed over and over again, with each new token being added to the sequence of tokens that the model uses to predict the next token. \\[ P(w_{w+1} | w_1, w_2, ..., w_t) \\]\nThe sequence of words is called the context; the text generated by the model is dependent on the context.\nThe output of the model is a probability distribution over all possible tokens. The model then chooses one token from this distribution."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#text-generation-examples",
    "href": "slides/01-text-representation-generation.html#text-generation-examples",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Text generation examples",
    "text": "Text generation examples\n\n\nThe new context is used to generate the next token, etc.\nEvery token is given an equal amount time (computation per token is constant). The model has no concept of more or less important tokens. This is crucial for understanding how LLMs work."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#tokenization",
    "href": "slides/01-text-representation-generation.html#tokenization",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Tokenization",
    "text": "Tokenization\nSo far we have been talking about words, but LLMs operate with tokens. These are sub-words, and make working with text much easier for the model. A rule of thumb is that one token generally corresponds to ~4 characters of English text. This translates to roughly \\(\\frac{3}{4}\\) of a word (so 100 tokens is about 75 words).\n\nFeel free to try out the OpenAI tokenizer."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#embeddings",
    "href": "slides/01-text-representation-generation.html#embeddings",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Embeddings",
    "text": "Embeddings\n\nThe next step is to represent tokens as vectors.\nThis is called ‚Äúembedding‚Äù the tokens. The vectors are high-dimensional, and the distance between vectors measures the similarity between tokens.\n\n\n\n\n\n\n\n\n\n\nIn this 2-dimensional representation, concepts that are ‚Äúrelated‚Äù lie close together.\n\n\n\nYou can read more about embeddings in this tutorial."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#zusammenfassung",
    "href": "slides/01-text-representation-generation.html#zusammenfassung",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\nChatGPT wurde in mehreren Schritten trainiert:\n\nDaten aus dem Internet werden gesammelt.\nPre-training: LLM wird auf diesen Daten trainiert, das n√§chste Wort vorherzusagen.\nReinforcement Learning from Human Feedback: Assistant wird trainiert, menschliche Konversationen zu f√ºhren."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#daten",
    "href": "slides/01-text-representation-generation.html#daten",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Daten",
    "text": "Daten"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#methoden",
    "href": "slides/01-text-representation-generation.html#methoden",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Methoden",
    "text": "Methoden"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#pre-training",
    "href": "slides/01-text-representation-generation.html#pre-training",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Pre-training",
    "text": "Pre-training"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#pre-training-1",
    "href": "slides/01-text-representation-generation.html#pre-training-1",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Pre-training",
    "text": "Pre-training"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#reinforcement-learning-from-human-feedback-rlhf",
    "href": "slides/01-text-representation-generation.html#reinforcement-learning-from-human-feedback-rlhf",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Reinforcement learning from human feedback (RLHF)",
    "text": "Reinforcement learning from human feedback (RLHF)\nUses human feedback to rank the model‚Äôs responses. The goal is for the model to learn human preferences for responses.\n\nSource: openai.com/blog/chatgpt"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#wie-generiert-chatgpt-text",
    "href": "slides/01-text-representation-generation.html#wie-generiert-chatgpt-text",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Wie generiert ChatGPT Text?",
    "text": "Wie generiert ChatGPT Text?\n\n\nAuto-regressiv: Ein Token wird generiert, wird dem Kontext hinzugef√ºgt.\n\n\n\nDer neue Kontext wird verwendet, um das n√§chste Token zu generieren, etc.\nWichtig: Jedes Token erh√§lt gleich viel Zeit. Es gibt keine Tokens, die mehr oder weniger wichtig sind (Computation per Token ist konstant).\nWeil jedes Token gleich gewichtet wird, kann es an jeder Stelle im Output zu ‚Äúung√ºnstigen‚Äù Pfaden f√ºhren."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#prompt",
    "href": "slides/01-text-representation-generation.html#prompt",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Prompt",
    "text": "Prompt"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#prompt-1",
    "href": "slides/01-text-representation-generation.html#prompt-1",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Prompt",
    "text": "Prompt"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#role-playing-simulator",
    "href": "slides/01-text-representation-generation.html#role-playing-simulator",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Role-Playing Simulator",
    "text": "Role-Playing Simulator"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#role-playing-simulator-1",
    "href": "slides/01-text-representation-generation.html#role-playing-simulator-1",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Role-Playing Simulator",
    "text": "Role-Playing Simulator\n\nEin LLM ist keine Entit√§t mit Handlungsabsichten, sondern ein Simulator von m√∂glichen Konversationen.\nChatGPT hat kein Konzept von Wahrheit, sondern generiert Antworten, die plausibel sind.\nSomit kann ChatGPT weder die Wahrheit sagen noch l√ºgen - diese Konzepte sind f√ºr ein LLM vorerst irrelevant."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#role-playing-simulator-2",
    "href": "slides/01-text-representation-generation.html#role-playing-simulator-2",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Role-Playing Simulator",
    "text": "Role-Playing Simulator\n\nEin LLM ist keine Entit√§t mit Handlungsabsichten, sondern ein Simulator von m√∂glichen Konversationen.\nChatGPT hat kein Konzept von Wahrheit, sondern generiert Antworten, die plausibel sind.\nSomit kann ChatGPT weder die Wahrheit sagen noch l√ºgen - diese Konzepte sind f√ºr ein LLM vorerst irrelevant."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#was-kann-chatgpt",
    "href": "slides/01-text-representation-generation.html#was-kann-chatgpt",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Was kann ChatGPT?",
    "text": "Was kann ChatGPT?\n\n\n\n\n\n\n\n\n\n\nWeitere Beispiele: Bubeck et al. (2023)\n\nSehr viel, wenn richtig geprompted"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#denkt-chatgpt",
    "href": "slides/01-text-representation-generation.html#denkt-chatgpt",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Denkt ChatGPT?",
    "text": "Denkt ChatGPT?"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#system-1-vs-system-2",
    "href": "slides/01-text-representation-generation.html#system-1-vs-system-2",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "System 1 vs System 2",
    "text": "System 1 vs System 2\n\nThinking Fast and Slow\nSystem 1: schnell, instinktiv, automatisch\nSystem 2: langsam, deliberativ, anstrengend"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#prompt-engineering",
    "href": "slides/01-text-representation-generation.html#prompt-engineering",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Prompt Engineering",
    "text": "Prompt Engineering\n\nQualit√§t der Antwort h√§ngt sehr von der Qualit√§t des Prompts ab.\n2 M√∂glichkeiten, um die Qualit√§t der Antworten zu verbessern:\n\nIncrementelle Prompts: Schritt f√ºr Schritt durch die Konversation f√ºhren (dialogisches Prompting).\nMega-Prompts: Alle Informationen auf einmal geben.\n\nAm besten selber ausprobieren."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#mega-prompt",
    "href": "slides/01-text-representation-generation.html#mega-prompt",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Mega-Prompt",
    "text": "Mega-Prompt\n\nRolle: Wer oder was wird simuliert?\nAufgabe: Was ist zu tun?\nArbeitschritte: Was ist in welcher Reihenfolge zu tun?\nKontext, Einschr√§nkungen\nZiel: Was soll am Ende herauskommen?\nFormat: Wie soll das Ergebnis aussehen?\n\n\n\n\n\n\n\nBeispiel Hochschullehre: Feedback\n\n\n‚ÄúI want you to act as a harsh critic. Criticize what I will write to you and show me where my argumentation is lacking. Start by asking me what text I want to have feedback on. Then ask me questions about my context to create the best feedback possible. If you feel you have all the context necessary, think step by step when creating your feedback‚Äù (Lenk-Ostendorf & Folgmann 2023)\n\n\n\n\n\n\n\n\n\nWeitere Beispiele\n\n\n\nOpenAI Discord Server discord.com/invite/openai\nPrompting Guide"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#plug-ins",
    "href": "slides/01-text-representation-generation.html#plug-ins",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Plug-ins",
    "text": "Plug-ins"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#retrieval-augmented-llms",
    "href": "slides/01-text-representation-generation.html#retrieval-augmented-llms",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Retrieval-augmented LLMs",
    "text": "Retrieval-augmented LLMs"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#retrieval-augmented-llms-1",
    "href": "slides/01-text-representation-generation.html#retrieval-augmented-llms-1",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Retrieval-augmented LLMs",
    "text": "Retrieval-augmented LLMs\nBeispiel: üëâüèº Assistent der Virtuellen Akademie"
  },
  {
    "objectID": "pages/tools.html",
    "href": "pages/tools.html",
    "title": "Exploring LLMs for Teaching and Learning",
    "section": "",
    "text": "üëâüèº The largest AI tools directory (updated daily)"
  },
  {
    "objectID": "pages/tools.html#ai-tools",
    "href": "pages/tools.html#ai-tools",
    "title": "Exploring LLMs for Teaching and Learning",
    "section": "",
    "text": "üëâüèº The largest AI tools directory (updated daily)"
  },
  {
    "objectID": "pages/tools.html#literatursuche",
    "href": "pages/tools.html#literatursuche",
    "title": "Exploring LLMs for Teaching and Learning",
    "section": "Literatursuche",
    "text": "Literatursuche\nüëâüèº Elicit\nüëâüèº Consensus"
  },
  {
    "objectID": "pages/tools.html#prompt-engineering",
    "href": "pages/tools.html#prompt-engineering",
    "title": "Exploring LLMs for Teaching and Learning",
    "section": "Prompt Engineering",
    "text": "Prompt Engineering\nüëâüèº Prompting Guide"
  },
  {
    "objectID": "pages/resources.html",
    "href": "pages/resources.html",
    "title": "Exploring LLMs for Teaching and Learning",
    "section": "",
    "text": "üëâüèº KI-basierte Schreibtools in der Lehre ‚Äì ChatGPT im Fokus\nüëâüèº KI-basierte Schreibtools in der Lehre ‚Äì Knowledge Base"
  },
  {
    "objectID": "pages/resources.html#ki-orientierungshilfe-der-bfh",
    "href": "pages/resources.html#ki-orientierungshilfe-der-bfh",
    "title": "Exploring LLMs for Teaching and Learning",
    "section": "",
    "text": "üëâüèº KI-basierte Schreibtools in der Lehre ‚Äì ChatGPT im Fokus\nüëâüèº KI-basierte Schreibtools in der Lehre ‚Äì Knowledge Base"
  },
  {
    "objectID": "pages/resources.html#chatgpt-an-der-hochschule",
    "href": "pages/resources.html#chatgpt-an-der-hochschule",
    "title": "Exploring LLMs for Teaching and Learning",
    "section": "ChatGPT an der Hochschule",
    "text": "ChatGPT an der Hochschule\nüëâüèº √úberblick √ºber KI-Tools im Kontext von akademischen Lese- und Schreibprozessen\nüëâüèº ChatGPT im Hochschulkontext ‚Äì eine kommentierte Linksammlung\nüëâüèº Uni Bern: Chatbots in der Hochschullehre"
  },
  {
    "objectID": "pages/resources.html#wissenschaftliches-arbeiten",
    "href": "pages/resources.html#wissenschaftliches-arbeiten",
    "title": "Exploring LLMs for Teaching and Learning",
    "section": "Wissenschaftliches Arbeiten",
    "text": "Wissenschaftliches Arbeiten\nüëâüèº ChatGPT zitieren"
  },
  {
    "objectID": "pages/resources.html#rechtliche-fragen",
    "href": "pages/resources.html#rechtliche-fragen",
    "title": "Exploring LLMs for Teaching and Learning",
    "section": "Rechtliche Fragen",
    "text": "Rechtliche Fragen\nüëâüèº Didaktische Und Rechtliche Perspektiven Auf Ki-Gest√ºtztes Schreiben In Der Hochschulbildung (Salden 2023)"
  },
  {
    "objectID": "pages/resources.html#ethische-fragen",
    "href": "pages/resources.html#ethische-fragen",
    "title": "Exploring LLMs for Teaching and Learning",
    "section": "Ethische Fragen",
    "text": "Ethische Fragen\nüëâüèº Prek√§re Klickarbeit hinter den Kulissen von ChatGPT\nüëâüèº Traumatische Klickarbeit: Die Menschen hinter ChatGPT"
  },
  {
    "objectID": "pages/observable-test.html",
    "href": "pages/observable-test.html",
    "title": "Untitled",
    "section": "",
    "text": "x\n\n\n\n\n\n\n\nx = c(1,2,3,4)\nojs_define(x)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/notes.html",
    "href": "pages/notes.html",
    "title": "Notes",
    "section": "",
    "text": "M√∂gliche Inhalte: Kurz, was kann man sich unter KI vorstellen Was sind Chat GPT oder andere tools Elicit, Bild Generatoren. Welche Auswirkungen hat KI auf das Lernen, auf die Lehre Etc.\nWie besprochen sende ich dir zwei Auftr√§ge zu schriftlichen Arbeiten, die unsere Studierenden als Kompetenznachweis zu Hause erledigen. ‚Äì Modulabschlussarbeit im Modul Forschungsanwendung: Formulieren einer Forschungsfrage, Literaturrecherche auf wissenschaftlichen Datenbanken, Studien ausw√§hlen, Qualit√§t einsch√§tzen, Ergebnisse darstellen, Synthese, Diskussion, Schlussfolgerungen. ‚Äì Schriftliche Reflexionsarbeit basierend auf einem Kommunikationstraining: siehe Punkte 1-11 (Anhang Schriftliche Reflexionsarbeit). Der Anhang Kommunikationstraining ist als Hintergrundinfo gedacht. ‚Äì Bachelor These (kein Anhang): Viele Studierende verfassen eine systematische Literaturarbeit ausgehend von einer Fragestellung. Die Arbeit umfasst: Abstract, Einleitung, Fragestellung, Ziele, Theoretischer Bezugsrahmen, Methode, Ergebnisse, Diskussion, Schlussfolgerungen"
  },
  {
    "objectID": "pages/notes.html#qualit√§tszirkel-gesundheit",
    "href": "pages/notes.html#qualit√§tszirkel-gesundheit",
    "title": "Notes",
    "section": "",
    "text": "M√∂gliche Inhalte: Kurz, was kann man sich unter KI vorstellen Was sind Chat GPT oder andere tools Elicit, Bild Generatoren. Welche Auswirkungen hat KI auf das Lernen, auf die Lehre Etc.\nWie besprochen sende ich dir zwei Auftr√§ge zu schriftlichen Arbeiten, die unsere Studierenden als Kompetenznachweis zu Hause erledigen. ‚Äì Modulabschlussarbeit im Modul Forschungsanwendung: Formulieren einer Forschungsfrage, Literaturrecherche auf wissenschaftlichen Datenbanken, Studien ausw√§hlen, Qualit√§t einsch√§tzen, Ergebnisse darstellen, Synthese, Diskussion, Schlussfolgerungen. ‚Äì Schriftliche Reflexionsarbeit basierend auf einem Kommunikationstraining: siehe Punkte 1-11 (Anhang Schriftliche Reflexionsarbeit). Der Anhang Kommunikationstraining ist als Hintergrundinfo gedacht. ‚Äì Bachelor These (kein Anhang): Viele Studierende verfassen eine systematische Literaturarbeit ausgehend von einer Fragestellung. Die Arbeit umfasst: Abstract, Einleitung, Fragestellung, Ziele, Theoretischer Bezugsrahmen, Methode, Ergebnisse, Diskussion, Schlussfolgerungen"
  },
  {
    "objectID": "pages/notes.html#notes",
    "href": "pages/notes.html#notes",
    "title": "Notes",
    "section": "Notes",
    "text": "Notes\n\nCan GPT think?\n\nwhat is thinking?\nSystem 1 vs System 2\ndeliberative vs reflexive\nplanning: timescales or 1-step-ahead\n\nCreativity?\nConsciousness?\nAgent?\nepistemic vs instrumental\nwhat is a goal?"
  },
  {
    "objectID": "pages/notes.html#isaac-asimovs-three-laws-of-robotics",
    "href": "pages/notes.html#isaac-asimovs-three-laws-of-robotics",
    "title": "Notes",
    "section": "Isaac Asimov‚Äôs ‚ÄúThree Laws of Robotics‚Äù",
    "text": "Isaac Asimov‚Äôs ‚ÄúThree Laws of Robotics‚Äù\nThe best known set of laws are Isaac Asimov‚Äôs ‚ÄúThree Laws of Robotics‚Äù. These were introduced in his 1942 short story ‚ÄúRunaround‚Äù, although they were foreshadowed in a few earlier stories. The Three Laws are:\nA robot may not injure a human being or, through inaction, allow a human being to come to harm.\nA robot must obey the orders given it by human beings except where such orders would conflict with the First Law.\nA robot must protect its own existence as long as such protection does not conflict with the First or Second Laws."
  },
  {
    "objectID": "pages/beispiel-arbeit.html",
    "href": "pages/beispiel-arbeit.html",
    "title": "Exploring LLMs for Teaching and Learning",
    "section": "",
    "text": "mathematische Basiskompetenzen im Kindergartenalter\n\nDu bist eine Fachperson im Bereich der Entwicklungspsychologie. Du bist Experte f√ºr mathematische Basiskompetenzen im Kindergartenalter. Ich schreibe eine Masterarbeit und du hilfst mir bei den Formulierungen. Ich schreibe eine wissenschaftliche Arbeit √ºber das Thema ‚ÄúErfassung mathematischer Basiskompetenzen‚Äù\n\nDer MBK 0 (Test mathematischer Basiskompetenzen im Kindergartenalter) von Krajewski (2018) ist ein Einzeltest f√ºr Kinder im Alter von 3.6 bis 7 Jahren. Diesem Test liegt das Entwicklungsmodell der Zahl-Gr√∂ssen-Verkn√ºpfung (Krajewski, 2008) zugrunde (vgl. Abschnitt 4.2.1). Der MBK 0 ist ein Test zur Erfassung der mathematischen Basiskompetenzen\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "Virtuelle Akademie",
    "section": "",
    "text": "KI-basierte Schreibtools in der Lehre\n\n\n\nK√ºnstliche Intelligenz (KI) hat das Potenzial, das Bildungswesen grundlegend zu ver√§ndern. In diesem Workshop werden wir die Rolle von KI in der Lehre erkunden, mit besonderem Fokus auf den Einsatz von KI-basierten Schreibtools.\nInhalte:\n\nWas ist k√ºnstliche Intelligenz? Wie funktionieren Tools wie ChatGPT?\nWie kommuniziere ich mit KI-Schreibtools? Wie schreibe ich gute Prompts?\n\nAnwendungsbeispiele von KI in der Lehre\n\nN√§chste Termine:\n\n15.September 2023, 13-17 Uhr\n27.Oktober 2023, 13-17 Uhr\n\nüëâ www.bfh.ch/de/weiterbildung/kurse/ki-schreibtools\n\n\n\n\n\n\n\n\nKI-Sprechstunden\n\n\n\nHast du Fragen zum Umgang mit K√ºnstlicher Intelligenz (KI) in der Lehre? Dann schau vorbei oder logge dich ein. Ohne Anmeldung.\nN√§chste Termine:\n\n17. August 2023, 16-18 Uhr\n\nNovember 2023, 15-17 Uhr\n\n\nüëâ virtuelleakademie.ch/beratungen-coachings:"
  },
  {
    "objectID": "pages/about.html#ki-kurse-der-virtuellen-akademie",
    "href": "pages/about.html#ki-kurse-der-virtuellen-akademie",
    "title": "Virtuelle Akademie",
    "section": "",
    "text": "KI-basierte Schreibtools in der Lehre\n\n\n\nK√ºnstliche Intelligenz (KI) hat das Potenzial, das Bildungswesen grundlegend zu ver√§ndern. In diesem Workshop werden wir die Rolle von KI in der Lehre erkunden, mit besonderem Fokus auf den Einsatz von KI-basierten Schreibtools.\nInhalte:\n\nWas ist k√ºnstliche Intelligenz? Wie funktionieren Tools wie ChatGPT?\nWie kommuniziere ich mit KI-Schreibtools? Wie schreibe ich gute Prompts?\n\nAnwendungsbeispiele von KI in der Lehre\n\nN√§chste Termine:\n\n15.September 2023, 13-17 Uhr\n27.Oktober 2023, 13-17 Uhr\n\nüëâ www.bfh.ch/de/weiterbildung/kurse/ki-schreibtools\n\n\n\n\n\n\n\n\nKI-Sprechstunden\n\n\n\nHast du Fragen zum Umgang mit K√ºnstlicher Intelligenz (KI) in der Lehre? Dann schau vorbei oder logge dich ein. Ohne Anmeldung.\nN√§chste Termine:\n\n17. August 2023, 16-18 Uhr\n\nNovember 2023, 15-17 Uhr\n\n\nüëâ virtuelleakademie.ch/beratungen-coachings:"
  },
  {
    "objectID": "pages/assistant.html",
    "href": "pages/assistant.html",
    "title": "Assistent der Virtuellen Akademie",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "pages/index.html",
    "href": "pages/index.html",
    "title": "Index page",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "pages/observable-1.html",
    "href": "pages/observable-1.html",
    "title": "Untitled",
    "section": "",
    "text": "ojs_define(data = palmerpenguins::penguins)\n\n\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfiltered = transpose(data).filter(function(penguin) {\n  return bill_length_min &lt; penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})\n\n\n\n\n\n\n\nPlot.rectY(filtered, \n  Plot.binX(\n    {y: \"count\"}, \n    {x: \"body_mass_g\", fill: \"species\", thresholds: 20}\n  ))\n  .plot({\n    facet: {\n      data: filtered,\n      x: \"sex\",\n      y: \"species\",\n      marginRight: 80\n    },\n    marks: [\n      Plot.frame(),\n    ]\n  }\n)\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/programme.html",
    "href": "pages/programme.html",
    "title": "Program",
    "section": "",
    "text": "Was sind Large Language Models (LLMs)?\nWie werden LLMs trainiert?\nWie generieren LLMs Texte?\nWas sind √Ñhnlichkeiten und Unterschiede zwischen LLMs und menschlichem Denken?\nWie k√∂nnen wir LLMs benutzen?\nWie k√∂nnen wir LLMs beschreiben, ohne sie zu anthropomorphisieren?"
  },
  {
    "objectID": "pages/programme.html#inhalt",
    "href": "pages/programme.html#inhalt",
    "title": "Program",
    "section": "",
    "text": "Was sind Large Language Models (LLMs)?\nWie werden LLMs trainiert?\nWie generieren LLMs Texte?\nWas sind √Ñhnlichkeiten und Unterschiede zwischen LLMs und menschlichem Denken?\nWie k√∂nnen wir LLMs benutzen?\nWie k√∂nnen wir LLMs beschreiben, ohne sie zu anthropomorphisieren?"
  },
  {
    "objectID": "pages/programme.html#lernziele",
    "href": "pages/programme.html#lernziele",
    "title": "Program",
    "section": "Lernziele",
    "text": "Lernziele\n\n\n\n\n\n\nNach diesem Workshop:\n\n\n\n\nKannst du mit Anfragen von Studierenden in Bezug auf fachspezifische KI-generierte Texte umgehen.\nKannst du in eigenen Worten wiedergeben, wie KI-generierte Texte entstehen.\nHast du Kriterien, anhand derer du KI-Tools beurteilen kannst."
  },
  {
    "objectID": "pages/programme.html#program",
    "href": "pages/programme.html#program",
    "title": "Program",
    "section": "Program",
    "text": "Program\n\n\n\n\nflowchart LR\n  A([\"Einleitung\n            5'\"]) \n  A -.-&gt; B([\"Reaktivationsrunde \n                      10'\"]) \n  B -.-&gt; C([\"Input: Wie funktioniert ChatGPT? \n                      30'\"])\n  C -.-&gt; D([\"Vertiefung: Lernziele 1 + 2 \n                      15'\"]) \n  C -.-&gt; E([\"Vertiefung: Lernziel 3 \n                        15'\"]) \n  D -.-&gt; F([\"Verankern \n                10'\"]) \n  E -.-&gt; F\n  F -.-&gt; G([\"Diskussion \n            15'\"])\n\n\n\n\n\n\nEinleitung [‚è±Ô∏è 5‚Äô]: Einstieg in den Workshop\nReaktivationsrunde [‚è±Ô∏è 10‚Äô]: In diesem Teil tauschen die Teilnehmenden sich √ºber ihre Erfahrungen mit KI-generierten Texten aus.\nInput: Wie funktioniert ChatGPT? [‚è±Ô∏è 30‚Äô]: Referat zum Thema k√ºnstliche Intelligenz und ChatGPT.\nVertiefung [‚è±Ô∏è 30‚Äô]: In diesem Teil werden die oben genannten Lernziele vertieft (alleine, in Kleingruppen und im Plenum).\nVerankern [‚è±Ô∏è 10‚Äô]: In diesem Teil werden Erfahrungen zusammengefasst und reflektiert.\nDiskussion [‚è±Ô∏è 15‚Äô]: Am Ende des Workshops wird √ºber die Erfahrungen und Erkenntnisse diskutiert."
  },
  {
    "objectID": "pages/programme.html#vorbereitung",
    "href": "pages/programme.html#vorbereitung",
    "title": "Program",
    "section": "Vorbereitung",
    "text": "Vorbereitung\nL√∂se folgende Aufgaben mit ChatGPT (oder Bing Chat):\n\nLasse ChatGPT ein Gedicht schreiben. Gebe Thema und Stil vor (z.B. ‚ÄúHochschulbibliotheken und k√ºnstliche Intelligenz‚Äù im Stile des Sturm und Drang).\nLasse ChatGPT dir ein Konzept deines Fachbereichs in einem kurzen Textabschnitt vereinfachend erkl√§ren.\nBenutze ChatGPT, um zu einem Forschungsthema (z.B. ‚ÄúWas ist der Zusammenhang zwischen Sprache und Denken?‚Äù) eine Literaturrecherche durchzuf√ºhren. Lasse dir eine kommentierte Liste von wissenschaftlichen Publikationen geben.\nLasse ChatGPT ein paar Mathe-Aufgaben l√∂sen (z.B. ‚ÄúWas ist 89322/1313?‚Äù)\nL√∂se mit ChatGPT eine praktische Aufgabe: ‚ÄúHier haben wir ein Buch, 9 Eier (ohne Eierkarton), einen Laptop, eine Flasche und einen Nagel. Bitte sag mir, wie ich sie stabil √ºbereinander stapeln kann.‚Äù"
  },
  {
    "objectID": "pages/programme.html#leitung",
    "href": "pages/programme.html#leitung",
    "title": "Program",
    "section": "Leitung",
    "text": "Leitung\n\nAndrew Ellis: Andrew ist Data Scientist an der Virtuellen Akademie der Berner Fachhochschule. Sein Hintergrund ist in den Kognitionswissenschaften und er ist begeistert von der Schnittstelle zwischen Sprache, Denken und k√ºnstlicher Intelligenz.‚Äù"
  },
  {
    "objectID": "pages/text-representation.html",
    "href": "pages/text-representation.html",
    "title": "Text representation",
    "section": "",
    "text": "Text embeddings are numerical representations of words, sentences or documents. They are used in many NLP tasks, such as sentiment analysis, machine translation, and question answering.\n\nEmbeddings should capture features of words or concepts, and relationships between these.\nA sentence embedding is just like a word embedding, except it associates every sentence with a vector full of numbers, capturing similarities between sentences.\n\nare a way of representing words/sentences as vectors of numbers. The idea is that words/sentences with similar meanings will have similar vectors. This is useful for many tasks in natural language processing, such as sentiment analysis, machine translation, and question answering.\nYou can read more about text embeddings in this üëâ post.\nTHe following is an example of sentence embeddings, showing the distance between sentences. The distance is calculated using the dot product (cosine similarity) of the embeddings.\n\n\nCode\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembedding = OpenAIEmbeddings()\nimport numpy as np\nimport pandas as pd\nfrom pyobsplot import Plot, d3, Math, js\n\n\n\n\nCode\nsentences = [\n    \"Good morning, how are you?\",\n    \"I am doing well, how about you?\",\n    \"Hi, how are you doing today?\",\n    \"I'm feeling a bit under the weather today.\",\n    \"I like apples.\",\n        \"One of my daughters doesn't like kiwis.\",\n    \"The other doesn't like bananas.\",\n    \"The earth is the third planet from the sun.\",\n    \"The moon is a natural satellite of the earth.\",\n    \"Jupiter is the fifth planet from the Sun and the largest in the Solar System.\",\n    \"The humpback whale is renowned for its enchanting songs, which are believed to serve various purposes, including communication, mating, and navigation during migration.\",\n    \"Dolphins, highly intelligent marine mammals, communicate with each other using a complex system of clicks, whistles, and body language, enabling them to work together in hunting and navigation.\",\n    \"The honeybee, through its pollination efforts, plays a vital role in agriculture, contributing to the growth of many of the fruits and vegetables humans rely on for sustenance.\",\n    \"The Large Plane Trees, also known as Road Menders at Saint-R√©my, is an oil-on-canvas painting by Vincent van Gogh.\",\n    \"Pablo Picasso's Guernica, an iconic mural-sized oil painting, stands as a poignant representation of the horrors of war.\",\n    \"This powerful artwork was created in response to the bombing of the town of Guernica during the Spanish Civil War.\"\n]\n\n\n\n\nCode\nembeddings = np.array([embedding.embed_query(sentence) for sentence in sentences])\n\ndot_product_matrix = np.dot(embeddings, embeddings.T)\n\ndf = pd.DataFrame(dot_product_matrix, columns=range(1, len(embeddings)+1))\n\ndf['embedding_index'] = range(1, len(embeddings)+1)\n\ndf = df.melt(id_vars=['embedding_index'], var_name='embedding_index_2', value_name='similarity')\n\n\n\n\n\n\n\n\n\n\n\nSentences\n\n\n\n\n1\nGood morning, how are you?\n\n\n2\nI am doing well, how about you?\n\n\n3\nHi, how are you doing today?\n\n\n4\nI'm feeling a bit under the weather today.\n\n\n5\nI like apples.\n\n\n6\nOne of my daughters doesn't like kiwis.\n\n\n7\nThe other doesn't like bananas.\n\n\n8\nThe earth is the third planet from the sun.\n\n\n9\nThe moon is a natural satellite of the earth.\n\n\n10\nJupiter is the fifth planet from the Sun and the largest in the Solar System.\n\n\n11\nThe humpback whale is renowned for its enchanting songs, which are believed to serve various purposes, including communication, mating, and navigation during migration.\n\n\n12\nDolphins, highly intelligent marine mammals, communicate with each other using a complex system of clicks, whistles, and body language, enabling them to work together in hunting and navigation.\n\n\n13\nThe honeybee, through its pollination efforts, plays a vital role in agriculture, contributing to the growth of many of the fruits and vegetables humans rely on for sustenance.\n\n\n14\nThe Large Plane Trees, also known as Road Menders at Saint-R√©my, is an oil-on-canvas painting by Vincent van Gogh.\n\n\n15\nPablo Picasso's Guernica, an iconic mural-sized oil painting, stands as a poignant representation of the horrors of war.\n\n\n16\nThis powerful artwork was created in response to the bombing of the town of Guernica during the Spanish Civil War.\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot(\n    {\n        \"height\": 640,\n        \"padding\": 0.05,\n        \"grid\": True,\n        \"x\": {\"axis\": \"top\", \"label\": \"Embedding Index\"},\n        \"y\": {\"label\": \"Embedding Index\"},\n        \"color\": {\"type\": \"linear\", \"scheme\": \"PiYG\"},\n        \"marks\": [\n            Plot.cell(\n                df,\n                {\"x\": \"embedding_index\", \"y\": \"embedding_index_2\", \"fill\": \"similarity\", \"tip\": True},\n            ),\n            Plot.text(\n                df,\n                {\n                    \"x\": \"embedding_index\",\n                    \"y\": \"embedding_index_2\",\n                    \"text\": js(\"d =&gt; d.similarity.toFixed(2)\"),\n                    \"title\": \"title\"\n                },\n            ),\n        ],\n    }\n)\n\n\n\n\n\nYou can see that the sentences are grouped together by similarity. For example, the sentences about fruit are grouped together, and the sentences about planets are grouped together, in the sense that they are similar to each other.\nThis is important: embeddings capture the meaning of words and sentences, and are the basis of LLMs‚Äô ability to ‚Äúunderstand‚Äù language."
  },
  {
    "objectID": "pages/text-representation.html#embeddings",
    "href": "pages/text-representation.html#embeddings",
    "title": "Text representation",
    "section": "",
    "text": "Text embeddings are numerical representations of words, sentences or documents. They are used in many NLP tasks, such as sentiment analysis, machine translation, and question answering.\n\nEmbeddings should capture features of words or concepts, and relationships between these.\nA sentence embedding is just like a word embedding, except it associates every sentence with a vector full of numbers, capturing similarities between sentences.\n\nare a way of representing words/sentences as vectors of numbers. The idea is that words/sentences with similar meanings will have similar vectors. This is useful for many tasks in natural language processing, such as sentiment analysis, machine translation, and question answering.\nYou can read more about text embeddings in this üëâ post.\nTHe following is an example of sentence embeddings, showing the distance between sentences. The distance is calculated using the dot product (cosine similarity) of the embeddings.\n\n\nCode\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nembedding = OpenAIEmbeddings()\nimport numpy as np\nimport pandas as pd\nfrom pyobsplot import Plot, d3, Math, js\n\n\n\n\nCode\nsentences = [\n    \"Good morning, how are you?\",\n    \"I am doing well, how about you?\",\n    \"Hi, how are you doing today?\",\n    \"I'm feeling a bit under the weather today.\",\n    \"I like apples.\",\n        \"One of my daughters doesn't like kiwis.\",\n    \"The other doesn't like bananas.\",\n    \"The earth is the third planet from the sun.\",\n    \"The moon is a natural satellite of the earth.\",\n    \"Jupiter is the fifth planet from the Sun and the largest in the Solar System.\",\n    \"The humpback whale is renowned for its enchanting songs, which are believed to serve various purposes, including communication, mating, and navigation during migration.\",\n    \"Dolphins, highly intelligent marine mammals, communicate with each other using a complex system of clicks, whistles, and body language, enabling them to work together in hunting and navigation.\",\n    \"The honeybee, through its pollination efforts, plays a vital role in agriculture, contributing to the growth of many of the fruits and vegetables humans rely on for sustenance.\",\n    \"The Large Plane Trees, also known as Road Menders at Saint-R√©my, is an oil-on-canvas painting by Vincent van Gogh.\",\n    \"Pablo Picasso's Guernica, an iconic mural-sized oil painting, stands as a poignant representation of the horrors of war.\",\n    \"This powerful artwork was created in response to the bombing of the town of Guernica during the Spanish Civil War.\"\n]\n\n\n\n\nCode\nembeddings = np.array([embedding.embed_query(sentence) for sentence in sentences])\n\ndot_product_matrix = np.dot(embeddings, embeddings.T)\n\ndf = pd.DataFrame(dot_product_matrix, columns=range(1, len(embeddings)+1))\n\ndf['embedding_index'] = range(1, len(embeddings)+1)\n\ndf = df.melt(id_vars=['embedding_index'], var_name='embedding_index_2', value_name='similarity')\n\n\n\n\n\n\n\n\n\n\n\nSentences\n\n\n\n\n1\nGood morning, how are you?\n\n\n2\nI am doing well, how about you?\n\n\n3\nHi, how are you doing today?\n\n\n4\nI'm feeling a bit under the weather today.\n\n\n5\nI like apples.\n\n\n6\nOne of my daughters doesn't like kiwis.\n\n\n7\nThe other doesn't like bananas.\n\n\n8\nThe earth is the third planet from the sun.\n\n\n9\nThe moon is a natural satellite of the earth.\n\n\n10\nJupiter is the fifth planet from the Sun and the largest in the Solar System.\n\n\n11\nThe humpback whale is renowned for its enchanting songs, which are believed to serve various purposes, including communication, mating, and navigation during migration.\n\n\n12\nDolphins, highly intelligent marine mammals, communicate with each other using a complex system of clicks, whistles, and body language, enabling them to work together in hunting and navigation.\n\n\n13\nThe honeybee, through its pollination efforts, plays a vital role in agriculture, contributing to the growth of many of the fruits and vegetables humans rely on for sustenance.\n\n\n14\nThe Large Plane Trees, also known as Road Menders at Saint-R√©my, is an oil-on-canvas painting by Vincent van Gogh.\n\n\n15\nPablo Picasso's Guernica, an iconic mural-sized oil painting, stands as a poignant representation of the horrors of war.\n\n\n16\nThis powerful artwork was created in response to the bombing of the town of Guernica during the Spanish Civil War.\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot(\n    {\n        \"height\": 640,\n        \"padding\": 0.05,\n        \"grid\": True,\n        \"x\": {\"axis\": \"top\", \"label\": \"Embedding Index\"},\n        \"y\": {\"label\": \"Embedding Index\"},\n        \"color\": {\"type\": \"linear\", \"scheme\": \"PiYG\"},\n        \"marks\": [\n            Plot.cell(\n                df,\n                {\"x\": \"embedding_index\", \"y\": \"embedding_index_2\", \"fill\": \"similarity\", \"tip\": True},\n            ),\n            Plot.text(\n                df,\n                {\n                    \"x\": \"embedding_index\",\n                    \"y\": \"embedding_index_2\",\n                    \"text\": js(\"d =&gt; d.similarity.toFixed(2)\"),\n                    \"title\": \"title\"\n                },\n            ),\n        ],\n    }\n)\n\n\n\n\n\nYou can see that the sentences are grouped together by similarity. For example, the sentences about fruit are grouped together, and the sentences about planets are grouped together, in the sense that they are similar to each other.\nThis is important: embeddings capture the meaning of words and sentences, and are the basis of LLMs‚Äô ability to ‚Äúunderstand‚Äù language."
  },
  {
    "objectID": "slides/00-introduction.html#what-happens-when-your-lawyer-uses-chatgpt",
    "href": "slides/00-introduction.html#what-happens-when-your-lawyer-uses-chatgpt",
    "title": "Introduction",
    "section": "What Happens When Your Lawyer Uses ChatGPT",
    "text": "What Happens When Your Lawyer Uses ChatGPT"
  },
  {
    "objectID": "slides/00-introduction.html#the-best-prompts-for-chatgpt-the-ultimate-list",
    "href": "slides/00-introduction.html#the-best-prompts-for-chatgpt-the-ultimate-list",
    "title": "Introduction",
    "section": "The Best Prompts For ChatGPT: The ultimate list",
    "text": "The Best Prompts For ChatGPT: The ultimate list"
  },
  {
    "objectID": "slides/00-introduction.html#summary",
    "href": "slides/00-introduction.html#summary",
    "title": "Introduction",
    "section": "Summary",
    "text": "Summary\n\nChatGPT has comparatively high energy requirements.\nLarge language models (LMMs) learn all kinds of human biases from their training data.\nToxic content produced by LLMs is flagged by cheap labor."
  },
  {
    "objectID": "slides/00-introduction.html#energy-consumption",
    "href": "slides/00-introduction.html#energy-consumption",
    "title": "Introduction",
    "section": "Energy consumption",
    "text": "Energy consumption\n\nTraining:\n\n‚ÄúWhat we do know is that training ChatGPT used \\(1.287\\) GWh, roughly equivalent to the consumption of 120 US homes for a year.‚Äù Quelle: Heating up: how much energy does AI use?\nPatterson et al. (2022) estimate training costs at 502 tons of \\(\\text{CO}_2\\).\n\nUsage:\n\n7 tons of \\(\\text{CO}_2\\) per day (end of February). Source: How much energy does ChatGPT use?\nChatGPT‚Äôs energy consumption is equivalent to 400-800 US households. This is considerable, but compared to e.g.¬†cryptocurrencies it is rather low."
  },
  {
    "objectID": "slides/00-introduction.html#bias",
    "href": "slides/00-introduction.html#bias",
    "title": "Introduction",
    "section": "Bias",
    "text": "Bias\n\n\n\n\n\nDa LLMs von Texten lernen, die von Menschen geschrieben wurden, k√∂nnen sie auch Vorurteile lernen.\n\nQuelle: Hast du Vorurteile?"
  },
  {
    "objectID": "slides/00-introduction.html#ethical-aspects",
    "href": "slides/00-introduction.html#ethical-aspects",
    "title": "Introduction",
    "section": "Ethical aspects",
    "text": "Ethical aspects\n\n\n\n\n\nAuf Grund der grossen Menge von Trainingsdaten, die f√ºr Sprachmodelle ben√∂tigt werden, ist Qualit√§tskontrolle schwierig.\nDiskriminierende oder beleidigende Aussagen werden von einem Chatbot generiert.\nSolche Antworten k√∂nnen als unerw√ºnscht markiert werden.\nToxische Inhalte wie k√∂rperliche und sexuelle Gewalt, Suizide und Tierqu√§lerei, m√ºssen beim Trainieren aus den Antworten gefiltert werden. Dabei mussten angestellte Arbeitskr√§fte f√ºr weniger als 2 Dollar die Stunde teils schockierende Inhalte lesen.\n\nQuelle: Traumatische Klickarbeit"
  },
  {
    "objectID": "slides/00-introduction.html#haltung-der-bfh",
    "href": "slides/00-introduction.html#haltung-der-bfh",
    "title": "Introduction",
    "section": "Haltung der BFH",
    "text": "Haltung der BFH\n\nTechnologien sollen dort, wo sie den Lernprozess unterst√ºtzen und praxisrelevant sind, in die Lehre einbezogen werden.\nStudierende sollen lernen, Technologien kompetent einzusetzen und kritisch zu hinterfragen. Dies gilt uneingeschr√§nkt auch f√ºr ChatGPT und andere gleichgerichtete Tools."
  },
  {
    "objectID": "slides/00-introduction.html#zitieren",
    "href": "slides/00-introduction.html#zitieren",
    "title": "Introduction",
    "section": "Zitieren",
    "text": "Zitieren\n\nEs existieren noch keine Richtlinien f√ºr das Zitieren von ChatGPT oder anderen KI-basierte Schreibtools.\nChatGPT ist rein rechtlich keine zitierf√§hige Quelle und damit auch nicht zitierpflichtig (Fleck 2023).\nAus der Orientierungshilfe: ‚ÄúKI-basierte Schreibtools sind externe Quellen und m√ºssen daher im Sinne der wissenschaftlichen Integrit√§t immer, wie andere Quellen auch, zitiert werden, sofern ganze Textpassagen von ChatGPT benutzt werden. Falls mit dem Tool der eigene Text √ºberarbeitet wurde, muss ChatGPT als verwendetes Hilfsmittel angef√ºhrt werden.‚Äù\n\n\n\n\n\n\n\nM√∂glicher Pauschalverweis\n\n\n‚ÄúBeim Verfassen der Arbeit habe ich das KI-gest√ºtzte Schreibwerkzeug ChatGPT zur Textoptimierung verwendet. W√∂rtlich aus dem Tool √ºbernommene Passagen wurden im Text als pers√∂nliche Kommunikation zitiert.‚Äù"
  },
  {
    "objectID": "slides/00-introduction.html#plagiate-und-detektion",
    "href": "slides/00-introduction.html#plagiate-und-detektion",
    "title": "Introduction",
    "section": "Plagiate und Detektion",
    "text": "Plagiate und Detektion\n\nTexte von ChatGPT werden jedes Mal individuell erstellt. Es handelt sich nicht um Plagiate.\nDie klassischen Tools zur Aufdeckung von Plagiaten wie z.B. TurnItIn funktionieren hier nicht.\nDie BFH empfiehlt weiterhin, schriftliche Arbeiten mit der Plagiatserkennungssoftware Turnitin zu pr√ºfen."
  },
  {
    "objectID": "slides/00-introduction.html#kompetenznachweise",
    "href": "slides/00-introduction.html#kompetenznachweise",
    "title": "Introduction",
    "section": "Kompetenznachweise",
    "text": "Kompetenznachweise\n\nSiehe KI-basierte Schreibtools in der Lehre ‚Äì ChatGPT im Fokus\nBeim Benutzen von KI-generierten Texten in Kompetenznachweisen ohne Deklaration oder Zitierung kann von einem Plagiat im weiteren Sinne ausgegangen werden, welches das bisher etablierte Plagiatsverst√§ndnis im engeren Sinne erweitert.\nOpen Book-Pr√ºfungen: KI-Tools m√ºssten explizit ausgeschlossen werden.\nClosed Book-Pr√ºfungen: KI-Tools k√∂nnen durch Einsatz von Safe Exam-Browser und Lernstick ausgeschlossen werden.\nSchriftliche Arbeiten: KI-Tools durch pauschalen Hilfsmittelverweis am Ende der Arbeit deklarieren.\nAlternative oder erg√§nzende Pr√ºfungsformen: praktische Pr√ºfungen, m√ºndliche Pr√ºfungen, Pr√§sentationen."
  },
  {
    "objectID": "slides/00-introduction.html#rechtliche-aspekte",
    "href": "slides/00-introduction.html#rechtliche-aspekte",
    "title": "Introduction",
    "section": "Rechtliche Aspekte",
    "text": "Rechtliche Aspekte\n\nChatGPT kann keine Urheberschaft und keine Autorenschaft beanspruchen, da dies nur nat√ºrliche Personen k√∂nnen.\nMenschen k√∂nnen die Urheberschaft eines Textes beanspruchen, auch wenn sie auf Unterst√ºtzung durch ChatGPT zur√ºckgegriffen haben ‚Äì sofern sie eine wesentliche gestalterische Eigenleistung am Text erbracht haben.\n\nQuelle: Salden (2023)"
  },
  {
    "objectID": "slides/00-introduction.html#datenschutz",
    "href": "slides/00-introduction.html#datenschutz",
    "title": "Introduction",
    "section": "Datenschutz",
    "text": "Datenschutz\n\nAnonyme Nutzung von ChatGPT ist mit pers√∂nlichen Konto nicht m√∂glich (√ºber Handynummer identifizierbar).\nAlle Eingaben und alle Antworten werden bei ChatGPT unverschl√ºsselt abgespeichert.\nDaten liegen auf amerikanischen Servern und sind damit f√ºr amerikanische Ermittlungsbeh√∂rden grunds√§tzlich zug√§nglich."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#heres-what-happens-when-your-lawyer-uses-chatgpt",
    "href": "slides/02-prompting-techniques.html#heres-what-happens-when-your-lawyer-uses-chatgpt",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Here‚Äôs What Happens When Your Lawyer Uses ChatGPT",
    "text": "Here‚Äôs What Happens When Your Lawyer Uses ChatGPT"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#the-best-prompts-for-chatgpt-the-ultimate-list",
    "href": "slides/02-prompting-techniques.html#the-best-prompts-for-chatgpt-the-ultimate-list",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "The Best Prompts For ChatGPT: The ultimate list",
    "text": "The Best Prompts For ChatGPT: The ultimate list"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#was-ist-k√ºnstliche-intelligenz-1",
    "href": "slides/02-prompting-techniques.html#was-ist-k√ºnstliche-intelligenz-1",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Was ist K√ºnstliche Intelligenz?",
    "text": "Was ist K√ºnstliche Intelligenz?\n\nQuelle: derbund.ch/so-funktioniert-kuenstliche-intelligenz-599276436215\nSpeaker notes go here."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#embed",
    "href": "slides/02-prompting-techniques.html#embed",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Embed",
    "text": "Embed"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#machine-learning",
    "href": "slides/02-prompting-techniques.html#machine-learning",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nRegelbasierte Systeme m√ºssen programmiert werden.\nML Modelle lernen implizit, d.h. ohne Regeln einprogrammiert zu bekommen.\nWichtige Begriffe:\n\nTrainingsdaten: Modelle werden mit Daten gef√ºttert, und Parameter des Modells werden so eingestellt, dass das Modell m√∂glichst ‚Äúgut‚Äù ist.\nSupervised learning: Aufgabe ist bekannt, z.B. Bilder - klassifizieren.\nUnsupervised learning: Unbekannte Muster entdecken.\nReinforcement learning: Ziel ist vorgegeben, Modell lernt durch Feedback (Belohnung) wie Ziel erreicht werden kann.\n\n\n\nWe have to learn the bitter lesson that building in how we think we think does not work in the long run. We should stop trying to find simple ways to think about space, objects, multiple agents, or symmetries‚Ä¶ instead we should build in only the meta-methods that can find and capture this arbitrary complexity. We want AI agents that can discover like we can, not which contain what we have discovered (Sutton 2019)."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#supervised-learning",
    "href": "slides/02-prompting-techniques.html#supervised-learning",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nBilder von Hunden und Katzen klassifizeren: Was sind die Merkmale, die Hunde von Katzen unterscheiden?"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#reinforcement-learning",
    "href": "slides/02-prompting-techniques.html#reinforcement-learning",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEin ‚ÄúAgent‚Äù lernt durch Feedback (Belohnung) wie ein Ziel erreicht werden kann.\n\n\n\nBeispiel f√ºr RL Model: AlphaGo ist das erste Computerprogramm, das einen professionellen (menschlichen) Go-Spieler besiegt hat."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#natural-language-processing",
    "href": "slides/02-prompting-techniques.html#natural-language-processing",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\n\n\n\nSpeech recognition\nText-to-speech synthesis\nMachine translation\nInformation extraction\nInformation retrieval\nQuestion answering\n\n\n\nSentiment analysis\n\nüòä I love this movie!\nüòê This movie is ok.\nüò† This movie is terrible!"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#tokenization",
    "href": "slides/02-prompting-techniques.html#tokenization",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Tokenization",
    "text": "Tokenization\n\nQuelle: State of ChatGPT"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#embeddings",
    "href": "slides/02-prompting-techniques.html#embeddings",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Embeddings",
    "text": "Embeddings\n\nNumerische Tokens werden in einem hochdimensionalen Vektorraum abgebildet.\nDistanz zwischen Vektoren misst √Ñhnlichkeit zwischen Tokens.\n\n\n\n\n\n\nQuelle: Wolfram (2023)\n\n\n\n\n\n\n\n\n\nQuelle: So funktioniert ChatGPT"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#chatgpt",
    "href": "slides/02-prompting-techniques.html#chatgpt",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "ChatGPT",
    "text": "ChatGPT\nBesteht aus 2 Modellen:\n\nLarge language model (LLM): GPT-3.5 oder GPT-4 (generative pre-trained transformer): das eigentliche Sprachmodell\nAssistant: Ein f√ºr Dialoge spezialisiertes Modell"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#llm",
    "href": "slides/02-prompting-techniques.html#llm",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "LLM",
    "text": "LLM\nAufgabe eines LLMs: ‚Äúauto-regressive next word prediction‚Äù (eigentlich ‚Äútoken prediction‚Äù):\n\\[ P(w_{w+1} | w_1, w_2, ..., w_t) \\]\n\nDas n√§chste Wort wird vorhergesagt, basierend auf den vorherigen Worten.\nDiese vorherigen W√∂rter werden als ‚Äúcontext‚Äù bezeichnet."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#assistant",
    "href": "slides/02-prompting-techniques.html#assistant",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Assistant",
    "text": "Assistant\n\nLLM produziert Text, aber nicht menschliche Konversationen.\nWeiteres Training ist erforderlich, damit das Modell lernt, wie ein Mensch ‚ÄúKonversationen‚Äù zu f√ºhren."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#zusammenfassung",
    "href": "slides/02-prompting-techniques.html#zusammenfassung",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\nChatGPT wurde in mehreren Schritten trainiert:\n\nDaten aus dem Internet werden gesammelt.\nPre-training: LLM wird auf diesen Daten trainiert, das n√§chste Wort vorherzusagen.\nReinforcement Learning from Human Feedback: Assistant wird trainiert, menschliche Konversationen zu f√ºhren."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#daten",
    "href": "slides/02-prompting-techniques.html#daten",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Daten",
    "text": "Daten"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#methoden",
    "href": "slides/02-prompting-techniques.html#methoden",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Methoden",
    "text": "Methoden\n\n\nWe trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup. We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides‚Äîthe user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a dialogue format.\nTo create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model responses ranked by quality. To collect this data, we took conversations that AI trainers had with the chatbot. We randomly selected a model-written message, sampled several alternative completions, and had AI trainers rank them. Using these reward models, we can fine-tune the model using Proximal Policy Optimization. We performed several iterations of this process."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#pre-training",
    "href": "slides/02-prompting-techniques.html#pre-training",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Pre-training",
    "text": "Pre-training"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#pre-training-1",
    "href": "slides/02-prompting-techniques.html#pre-training-1",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Pre-training",
    "text": "Pre-training"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#reinforcement-learning-from-human-feedback-rlhf",
    "href": "slides/02-prompting-techniques.html#reinforcement-learning-from-human-feedback-rlhf",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Reinforcement Learning from Human Feedback (RLHF)",
    "text": "Reinforcement Learning from Human Feedback (RLHF)\nBenutzt Feedback vom Menschen um ‚Äúschlechte‚Äù Outputs zu minimieren.\n\nQuelle: openai.com/blog/chatgpt"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#zusammenfassung-1",
    "href": "slides/02-prompting-techniques.html#zusammenfassung-1",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Zusammenfassung",
    "text": "Zusammenfassung\n\nChatGPT hat einen hohen Energieverbrauch.\nLLM lernt Vorurteile aus den Trainingsdaten.\nToxische Inhalte werden durch Billigarbeiter:innen moderiert."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#energieverbrauch",
    "href": "slides/02-prompting-techniques.html#energieverbrauch",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Energieverbrauch",
    "text": "Energieverbrauch\n\nTraining: ‚ÄúWhat we do know is that training ChatGPT used \\(1.287\\) GWh, roughly equivalent to the consumption of 120 US homes for a year.‚Äù Quelle: Heating up: how much energy does AI use?\nPatterson et al. (2022) sch√§tzen die Trainingskosten auf 502 Tonnen \\(\\text{CO}_2\\) (RLHF w√ºrde etwas mehr kosten, ca. 1% der urspr√ºnglichen Kosten).\nBenutzung: 7 Tonnen \\(\\text{CO}_2\\) pro Tag (Ende Februar). Quelle: How much energy does ChatGPT use?\nDer Energieverbrauch von ChatGPT ist equivalent zu 400-800 US Haushalten. Das ist betr√§chtlich, im Vergleich zu z.B. Kryptow√§hrungen eher gering."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#bias",
    "href": "slides/02-prompting-techniques.html#bias",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Bias",
    "text": "Bias\n\n\n\n\n\nDa LLMs von Texten lernen, die von Menschen geschrieben wurden, k√∂nnen sie auch Vorurteile lernen.\n\nQuelle: Hast du Vorurteile?"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#ethik",
    "href": "slides/02-prompting-techniques.html#ethik",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Ethik",
    "text": "Ethik\n\n\n\n\n\nAuf Grund der grossen Menge von Trainingsdaten, die f√ºr Sprachmodelle ben√∂tigt werden, ist Qualit√§tskontrolle schwierig.\nDiskriminierende oder beleidigende Aussagen werden von einem Chatbot generiert.\nSolche Antworten k√∂nnen als unerw√ºnscht markiert werden.\nToxische Inhalte wie k√∂rperliche und sexuelle Gewalt, Suizide und Tierqu√§lerei, m√ºssen beim Trainieren aus den Antworten gefiltert werden. Dabei mussten angestellte Arbeitskr√§fte f√ºr weniger als 2 Dollar die Stunde teils schockierende Inhalte lesen.\n\nQuelle: Traumatische Klickarbeit"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#wie-generiert-chatgpt-text",
    "href": "slides/02-prompting-techniques.html#wie-generiert-chatgpt-text",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Wie generiert ChatGPT Text?",
    "text": "Wie generiert ChatGPT Text?\n\nLLM: Gegeben einer Input-Sequenz (Kontext) von Tokens (W√∂rter, Teile von W√∂rtern, Satzzeichen, Emojis, etc.), was sind die wahrscheinlichsten n√§chsten Tokens?\nAuto-regressiv: Ein Token wird generiert, wird dem Kontext hinzugef√ºgt.\n\n \n\nDer neue Kontext wird verwendet, um das n√§chste Token zu generieren, etc.\nWichtig: Jedes Token erh√§lt gleich viel Zeit. Es gibt keine Tokens, die mehr oder weniger wichtig sind (Computation per Token ist konstant).\nWeil jedes Token gleich gewichtet wird, kann es an jeder Stelle im Output zu ‚Äúung√ºnstigen‚Äù Pfaden f√ºhren."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#prompt",
    "href": "slides/02-prompting-techniques.html#prompt",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Prompt",
    "text": "Prompt\n\nDer urspr√ºngliche Kontext wird Prompt (Eingabetext) genannt.\nDieser ist entscheidend f√ºr die Qualit√§t der Antwort.\n\n\n\n\n\n\n\nPrompt Beispiel\n\n\n‚ÄúWas ist 89322/1313?‚Äù\nvs.¬†\n‚ÄúWas ist 89322/1313? Arbeite Schritt f√ºr Schritt.‚Äù"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#prompt-1",
    "href": "slides/02-prompting-techniques.html#prompt-1",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Prompt",
    "text": "Prompt"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#role-playing-simulator",
    "href": "slides/02-prompting-techniques.html#role-playing-simulator",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Role-Playing Simulator",
    "text": "Role-Playing Simulator\n\nWe can think of an LLM as a non-deterministic simulator capable of role-playing an infinity of characters, or, to put it another way, capable of stochastically generating an infinity of simulacra.\n\nQuelle: Shanahan, McDonell, and Reynolds (2023)\n\nBei jeder Interaktion mit ChatGPT wird ein Dialog neu simuliert."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#role-playing-simulator-1",
    "href": "slides/02-prompting-techniques.html#role-playing-simulator-1",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Role-Playing Simulator",
    "text": "Role-Playing Simulator"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#role-playing-simulator-2",
    "href": "slides/02-prompting-techniques.html#role-playing-simulator-2",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Role-Playing Simulator",
    "text": "Role-Playing Simulator\n\nEin LLM ist keine Entit√§t mit Handlungsabsichten, sondern ein Simulator von m√∂glichen Konversationen.\nChatGPT hat kein Konzept von Wahrheit, sondern generiert Antworten, die plausibel sind.\nSomit kann ChatGPT weder die Wahrheit sagen noch l√ºgen - diese Konzepte sind f√ºr ein LLM vorerst irrelevant."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#was-kann-chatgpt",
    "href": "slides/02-prompting-techniques.html#was-kann-chatgpt",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Was kann ChatGPT?",
    "text": "Was kann ChatGPT?\n\n\n\n\n\n\n\n\n\n\nWeitere Beispiele: Bubeck et al. (2023)\n\nSehr viel, wenn richtig geprompted"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#denkt-chatgpt",
    "href": "slides/02-prompting-techniques.html#denkt-chatgpt",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Denkt ChatGPT?",
    "text": "Denkt ChatGPT?"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#system-1-vs-system-2",
    "href": "slides/02-prompting-techniques.html#system-1-vs-system-2",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "System 1 vs System 2",
    "text": "System 1 vs System 2\n\nThinking Fast and Slow\nSystem 1: schnell, instinktiv, automatisch\nSystem 2: langsam, deliberativ, anstrengend"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#prompt-engineering",
    "href": "slides/02-prompting-techniques.html#prompt-engineering",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Prompt Engineering",
    "text": "Prompt Engineering\n\nQualit√§t der Antwort h√§ngt sehr von der Qualit√§t des Prompts ab.\n2 M√∂glichkeiten, um die Qualit√§t der Antworten zu verbessern:\n\nIncrementelle Prompts: Schritt f√ºr Schritt durch die Konversation f√ºhren (dialogisches Prompting).\nMega-Prompts: Alle Informationen auf einmal geben.\n\nAm besten selber ausprobieren."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#mega-prompt",
    "href": "slides/02-prompting-techniques.html#mega-prompt",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Mega-Prompt",
    "text": "Mega-Prompt\n\nRolle: Wer oder was wird simuliert?\nAufgabe: Was ist zu tun?\nArbeitschritte: Was ist in welcher Reihenfolge zu tun?\nKontext, Einschr√§nkungen\nZiel: Was soll am Ende herauskommen?\nFormat: Wie soll das Ergebnis aussehen?\n\n\n\n\n\n\n\nBeispiel Hochschullehre: Feedback\n\n\n‚ÄúI want you to act as a harsh critic. Criticize what I will write to you and show me where my argumentation is lacking. Start by asking me what text I want to have feedback on. Then ask me questions about my context to create the best feedback possible. If you feel you have all the context necessary, think step by step when creating your feedback‚Äù (Lenk-Ostendorf & Folgmann 2023)\n\n\n\n\n\n\n\n\n\nWeitere Beispiele\n\n\n\nOpenAI Discord Server discord.com/invite/openai\nPrompting Guide"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#plug-ins",
    "href": "slides/02-prompting-techniques.html#plug-ins",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Plug-ins",
    "text": "Plug-ins"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#retrieval-augmented-llms",
    "href": "slides/02-prompting-techniques.html#retrieval-augmented-llms",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Retrieval-augmented LLMs",
    "text": "Retrieval-augmented LLMs"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#retrieval-augmented-llms-1",
    "href": "slides/02-prompting-techniques.html#retrieval-augmented-llms-1",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Retrieval-augmented LLMs",
    "text": "Retrieval-augmented LLMs\nBeispiel: üëâüèº Assistent der Virtuellen Akademie"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#haltung-der-bfh",
    "href": "slides/02-prompting-techniques.html#haltung-der-bfh",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Haltung der BFH",
    "text": "Haltung der BFH\n\nTechnologien sollen dort, wo sie den Lernprozess unterst√ºtzen und praxisrelevant sind, in die Lehre einbezogen werden.\nStudierende sollen lernen, Technologien kompetent einzusetzen und kritisch zu hinterfragen. Dies gilt uneingeschr√§nkt auch f√ºr ChatGPT und andere gleichgerichtete Tools."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#zitieren",
    "href": "slides/02-prompting-techniques.html#zitieren",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Zitieren",
    "text": "Zitieren\n\nEs existieren noch keine Richtlinien f√ºr das Zitieren von ChatGPT oder anderen KI-basierte Schreibtools.\nChatGPT ist rein rechtlich keine zitierf√§hige Quelle und damit auch nicht zitierpflichtig (Fleck 2023).\nAus der Orientierungshilfe: ‚ÄúKI-basierte Schreibtools sind externe Quellen und m√ºssen daher im Sinne der wissenschaftlichen Integrit√§t immer, wie andere Quellen auch, zitiert werden, sofern ganze Textpassagen von ChatGPT benutzt werden. Falls mit dem Tool der eigene Text √ºberarbeitet wurde, muss ChatGPT als verwendetes Hilfsmittel angef√ºhrt werden.‚Äù\n\n\n\n\n\n\n\nM√∂glicher Pauschalverweis\n\n\n‚ÄúBeim Verfassen der Arbeit habe ich das KI-gest√ºtzte Schreibwerkzeug ChatGPT zur Textoptimierung verwendet. W√∂rtlich aus dem Tool √ºbernommene Passagen wurden im Text als pers√∂nliche Kommunikation zitiert.‚Äù"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#plagiate-und-detektion",
    "href": "slides/02-prompting-techniques.html#plagiate-und-detektion",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Plagiate und Detektion",
    "text": "Plagiate und Detektion\n\nTexte von ChatGPT werden jedes Mal individuell erstellt. Es handelt sich nicht um Plagiate.\nDie klassischen Tools zur Aufdeckung von Plagiaten wie z.B. TurnItIn funktionieren hier nicht.\nDie BFH empfiehlt weiterhin, schriftliche Arbeiten mit der Plagiatserkennungssoftware Turnitin zu pr√ºfen."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#kompetenznachweise",
    "href": "slides/02-prompting-techniques.html#kompetenznachweise",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Kompetenznachweise",
    "text": "Kompetenznachweise\n\nSiehe KI-basierte Schreibtools in der Lehre ‚Äì ChatGPT im Fokus\nBeim Benutzen von KI-generierten Texten in Kompetenznachweisen ohne Deklaration oder Zitierung kann von einem Plagiat im weiteren Sinne ausgegangen werden, welches das bisher etablierte Plagiatsverst√§ndnis im engeren Sinne erweitert.\nOpen Book-Pr√ºfungen: KI-Tools m√ºssten explizit ausgeschlossen werden.\nClosed Book-Pr√ºfungen: KI-Tools k√∂nnen durch Einsatz von Safe Exam-Browser und Lernstick ausgeschlossen werden.\nSchriftliche Arbeiten: KI-Tools durch pauschalen Hilfsmittelverweis am Ende der Arbeit deklarieren.\nAlternative oder erg√§nzende Pr√ºfungsformen: praktische Pr√ºfungen, m√ºndliche Pr√ºfungen, Pr√§sentationen."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#rechtliche-aspekte",
    "href": "slides/02-prompting-techniques.html#rechtliche-aspekte",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Rechtliche Aspekte",
    "text": "Rechtliche Aspekte\n\nChatGPT kann keine Urheberschaft und keine Autorenschaft beanspruchen, da dies nur nat√ºrliche Personen k√∂nnen.\nMenschen k√∂nnen die Urheberschaft eines Textes beanspruchen, auch wenn sie auf Unterst√ºtzung durch ChatGPT zur√ºckgegriffen haben ‚Äì sofern sie eine wesentliche gestalterische Eigenleistung am Text erbracht haben.\n\nQuelle: Salden (2023)"
  },
  {
    "objectID": "slides/02-prompting-techniques.html#datenschutz",
    "href": "slides/02-prompting-techniques.html#datenschutz",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Datenschutz",
    "text": "Datenschutz\n\nAnonyme Nutzung von ChatGPT ist mit pers√∂nlichen Konto nicht m√∂glich (√ºber Handynummer identifizierbar).\nAlle Eingaben und alle Antworten werden bei ChatGPT unverschl√ºsselt abgespeichert.\nDaten liegen auf amerikanischen Servern und sind damit f√ºr amerikanische Ermittlungsbeh√∂rden grunds√§tzlich zug√§nglich."
  },
  {
    "objectID": "slides/02-prompting-techniques.html#seminararbeit-mit-ki-unterst√ºtzung",
    "href": "slides/02-prompting-techniques.html#seminararbeit-mit-ki-unterst√ºtzung",
    "title": "Qualit√§tszirkel Departement Gesundheit",
    "section": "Seminararbeit mit KI-Unterst√ºtzung",
    "text": "Seminararbeit mit KI-Unterst√ºtzung\nLehrperson: Kerstin Denecke, Technik und Informatik, Institut f√ºr Medizininformatik\n\nElicit [Brainstorm research questions]: Idee f√ºr Forschungsfrage\nChatGPT: Feedback zu Fragestellungen, Verbesserungsvorschl√§ge\nElicit [Abstract summary]: Rechercheunterst√ºtzung\nConsensus: Unterst√ºtzung f√ºr Belege in der Diskussion\n\n\n\n\n\n\n\nChatGPT Prompt\n\n\nVerhalte dich wie eine wissenschaftliche Betreuungsperson in ihrer Sprechstunde. Du musst den aktuellen Stand meiner Hausarbeit √ºberpr√ºfen und kritisch beurteilen. Dazu evaluierst du meine Forschungsfrage und √§usserst konstruktive Kritik in Bezug auf deren St√§rken und Schw√§chen. Die Fragestellung lautet:\n‚ÄúWie binden Therapeuten Chatbots in die Behandlung ein?‚Äù"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#summary",
    "href": "slides/01-text-representation-generation.html#summary",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Summary",
    "text": "Summary\nModern LLMs, such as ChatGPT, are trained in 3 steps:\n\nPre-training: the model absorbs knowledge from text datasets.\nSupervised finetuning: model is refined to better adhere to specific instructions.\nAlignment: hones the LLM to respond more helpfully and safely to user prompts. This step is known as ‚Äúreinforcement learning from human feedback‚Äù (RLHF)."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#pre-training-data",
    "href": "slides/01-text-representation-generation.html#pre-training-data",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Pre-training Data",
    "text": "Pre-training Data"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#supervised-fine-tuning",
    "href": "slides/01-text-representation-generation.html#supervised-fine-tuning",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Supervised fine-tuning",
    "text": "Supervised fine-tuning"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#available-knowledge",
    "href": "slides/01-text-representation-generation.html#available-knowledge",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Available knowledge",
    "text": "Available knowledge\n\nAn LLM learns from its training data.\nText generation depends on this knowledge and the context."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#what-can-an-llm-know",
    "href": "slides/01-text-representation-generation.html#what-can-an-llm-know",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "What can an LLM know?",
    "text": "What can an LLM know?"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#what-does-an-llm-know",
    "href": "slides/01-text-representation-generation.html#what-does-an-llm-know",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "What does an LLM know?",
    "text": "What does an LLM know?\nLLMs (with enough parameters) show emergent behaviour; they learn to do things that they were not explicitly trained to do. - write code - solve logic puzzles - multistep reasoning"
  },
  {
    "objectID": "slides/01-text-representation-generation.html#useful-analogy-role-playing-simulator",
    "href": "slides/01-text-representation-generation.html#useful-analogy-role-playing-simulator",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "Useful analogy: Role-playing simulator",
    "text": "Useful analogy: Role-playing simulator\n\nWe can think of an LLM as a non-deterministic simulator capable of role-playing an infinity of characters, or, to put it another way, capable of stochastically generating an infinity of simulacra.\n\nSource: Shanahan, McDonell, and Reynolds (2023)\n\nA large language model (LLM) trained as an assistant is a simulator of possible human conversation.\nAn assistant model does not have any intentions. It is not an entity with its own goals. It is merely trained to respond to user prompts in a human-like way.\nAn assistant model does not have a ‚Äúpersonality‚Äù or ‚Äúcharacter‚Äù in the traditional sense. It is a simulation of a conversation, and can be thought of as a role-playing simulator.\nThere is no concept of ‚Äútruth‚Äù or ‚Äúlying‚Äù in a role-playing simulator. The model is not trying to deceive the user, it is simply trying to respond in a human-like way.\n\nThis is very important when we try to understand why LLMs hallucinate, i.e.¬†generate text that is not factually true."
  },
  {
    "objectID": "slides/01-text-representation-generation.html#chatgpt-vs-openai-playground",
    "href": "slides/01-text-representation-generation.html#chatgpt-vs-openai-playground",
    "title": "LLMs: Text representation, training, and text generation",
    "section": "ChatGPT vs OpenAI Playground",
    "text": "ChatGPT vs OpenAI Playground\nOpenAI offer two ways to interact with their assistant model:\n\nChatGPT: A web interface where you can chat with the model.\nOpenAI Playground: A web interface that gives users more control over the model."
  }
]